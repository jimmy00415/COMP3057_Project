{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "995862c2",
   "metadata": {},
   "source": [
    "# Real-Time Multilingual ASR with Whisper\n",
    "\n",
    "This notebook implements a production-ready, real-time speech recognition system using OpenAI's Whisper models.\n",
    "\n",
    "**Features:**\n",
    "- Data preparation with augmentation\n",
    "- Model fine-tuning with multiple variants\n",
    "- Comprehensive evaluation (WER, CER, latency)\n",
    "- Real-time streaming inference\n",
    "- Full MLOps best practices (versioning, logging, reproducibility)\n",
    "\n",
    "**Author:** COMP3057 Project  \n",
    "**Version:** 1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c25953",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23841875",
   "metadata": {},
   "source": [
    "### √¢¬ö¬ô√Ø¬∏¬è Colab Configuration Guide\n",
    "\n",
    "**Resource Constraints:** A100 GPU (~40GB), 220GB Disk\n",
    "\n",
    "**Quick Start Options:**\n",
    "\n",
    "| Profile | Model | Samples | Epochs | Time | Disk | Quality |\n",
    "|---------|-------|---------|--------|------|------|---------|\n",
    "| **Fast Demo** | tiny | 50/10 | 1 | ~5min | ~2GB | Basic |\n",
    "| **Balanced** | base | 200/40 | 2 | ~20min | ~5GB | Good |\n",
    "| **Best Quality** | small | 500/100 | 3 | ~60min | ~10GB | Better |\n",
    "\n",
    "**Adjustable Parameters (in cells below):**\n",
    "- `TRAIN_SAMPLES` / `VAL_SAMPLES` - Dataset size\n",
    "- `MODEL_VARIANT` - Model quality (tiny/base/small)\n",
    "- `TRAIN_EPOCHS` - Training duration\n",
    "\n",
    "**Tips:**\n",
    "- Start with **Fast Demo** to verify everything works\n",
    "- Increase resources gradually if you have time\n",
    "- Checkpoints auto-save to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f24b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment and clone project\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if running in Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"√∞¬ü¬î¬ß Running in Google Colab\")\n",
    "    \n",
    "    # Mount Google Drive for saving checkpoints and logs\n",
    "    from google.colab import drive\n",
    "    if not os.path.exists('/content/drive'):\n",
    "        print(\"√∞¬ü¬ì¬Å Mounting Google Drive...\")\n",
    "        drive.mount('/content/drive', force_remount=False)\n",
    "        print(\"√¢¬ú?Drive mounted at /content/drive\")\n",
    "    else:\n",
    "        print(\"√¢¬ú?Drive already mounted\")\n",
    "    \n",
    "    # --- Project Setup and Path Configuration ---\n",
    "    PROJECT_DIR = 'COMP3057_Project'\n",
    "    \n",
    "    # Clone repository if not exists\n",
    "    if not os.path.exists(PROJECT_DIR):\n",
    "        print(\"\\n√∞¬ü¬ì¬¶ Cloning repository from GitHub...\")\n",
    "        !git clone https://github.com/jimmy00415/COMP3057_Project.git\n",
    "        print(\"√¢¬ú?Repository cloned\")\n",
    "    else:\n",
    "        print(\"√¢¬ú?Repository already exists\")\n",
    "    \n",
    "    # Change to project directory\n",
    "    os.chdir(PROJECT_DIR)\n",
    "    print(f\"√¢¬ú?Working directory: {os.getcwd()}\")\n",
    "\n",
    "    # Add project root to Python path\n",
    "    # This is the crucial step to solve ModuleNotFoundError\n",
    "    if os.getcwd() not in sys.path:\n",
    "        sys.path.insert(0, os.getcwd())\n",
    "        print(f\"√¢¬ú?Added '{os.getcwd()}' to Python path\")\n",
    "    \n",
    "    # Install dependencies IMMEDIATELY after cloning\n",
    "    print(\"\\n√∞¬ü¬ì¬¶ Installing dependencies...\")\n",
    "    !pip install -q -r requirements.txt\n",
    "    !pip install -q sounddevice\n",
    "    print(\"√¢¬ú?Dependencies installed\")\n",
    "\n",
    "else:\n",
    "    print(\"√∞¬ü¬í¬ª Running locally\")\n",
    "    print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "gpu_available = torch.cuda.is_available()\n",
    "if gpu_available:\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"\\n√∞¬ü¬é¬Æ GPU: {gpu_name}\")\n",
    "    print(f\"   Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"\\n√¢¬ö¬†√Ø¬∏¬è  WARNING: No GPU detected! Training will be very slow.\")\n",
    "    print(\"   Enable GPU: Runtime √¢¬Ü?Change runtime type √¢¬Ü?GPU (T4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40c0652",
   "metadata": {},
   "source": [
    "### üîÑ Quick Restart (Skip Training)\n",
    "\n",
    "**If you've already trained a model and just want to run inference/evaluation:**\n",
    "\n",
    "1. Run the **Setup cell above** (mounts Drive, clones repo, installs deps)\n",
    "2. Run the **Quick Restart cell below** to load your trained model\n",
    "3. Skip to Section 7 (Evaluation) or later sections\n",
    "\n",
    "This avoids re-running the expensive training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28021bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Restart - Load existing model without retraining\n",
    "# Run this cell AFTER the setup cell if you want to skip training\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Ensure project path is available (in case you skipped setup)\n",
    "if 'google.colab' in sys.modules:\n",
    "    project_root = '/content/COMP3057_Project'\n",
    "    if os.path.exists(project_root):\n",
    "        os.chdir(project_root)\n",
    "        if project_root not in sys.path:\n",
    "            sys.path.insert(0, project_root)\n",
    "        print(f\"‚úì Working in {project_root}\")\n",
    "    else:\n",
    "        raise RuntimeError(\"Project not found. Run the Setup cell first!\")\n",
    "else:\n",
    "    # Local environment\n",
    "    project_root = os.getcwd()\n",
    "    if project_root not in sys.path:\n",
    "        sys.path.insert(0, project_root)\n",
    "\n",
    "# Import all necessary modules\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from src.utils.config import load_config\n",
    "from src.models.whisper_model import WhisperModelManager\n",
    "from src.utils.versioning import ModelRegistry\n",
    "\n",
    "# Import evaluation and inference modules\n",
    "from src.evaluation import (\n",
    "    ModelEvaluator,\n",
    "    LatencyBenchmark,\n",
    "    EvaluationVisualizer,\n",
    "    generate_comparison_table\n",
    ")\n",
    "\n",
    "from src.inference import (\n",
    "    StreamingASR,\n",
    "    BatchInference\n",
    ")\n",
    "\n",
    "from src.data import VoiceActivityDetector\n",
    "\n",
    "print(\"‚úì Modules imported\")\n",
    "\n",
    "# Load config\n",
    "config = load_config('config.yaml')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize VAD for streaming\n",
    "vad = VoiceActivityDetector(threshold=config['data']['vad_threshold'])\n",
    "\n",
    "# Specify your trained model variant and checkpoint path\n",
    "MODEL_VARIANT = \"small\"  # Change to match your trained model\n",
    "CHECKPOINT_PATH = \"checkpoints/best_model_hf\"  # Or your specific checkpoint\n",
    "\n",
    "# Load model\n",
    "print(f\"Loading model: {MODEL_VARIANT}\")\n",
    "model_manager = WhisperModelManager(config)\n",
    "\n",
    "# Try to load from checkpoint if it exists\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\"Loading fine-tuned model from {CHECKPOINT_PATH}...\")\n",
    "    from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "    \n",
    "    model = WhisperForConditionalGeneration.from_pretrained(CHECKPOINT_PATH)\n",
    "    processor = WhisperProcessor.from_pretrained(CHECKPOINT_PATH)\n",
    "    model = model.to(device)\n",
    "    print(f\"‚úì Loaded fine-tuned model from checkpoint\")\n",
    "else:\n",
    "    print(f\"‚ö† Checkpoint not found at {CHECKPOINT_PATH}\")\n",
    "    print(f\"  Loading base {MODEL_VARIANT} model instead...\")\n",
    "    model, processor = model_manager.initialize_model(variant=MODEL_VARIANT, device=str(device))\n",
    "\n",
    "# Initialize model registry\n",
    "model_registry = ModelRegistry()\n",
    "\n",
    "# Set model to eval mode\n",
    "model.eval()\n",
    "\n",
    "print(\"\\n‚úì Quick restart complete!\")\n",
    "print(\"  You can now skip to Section 7 (Evaluation) or later\")\n",
    "print(f\"  Model: {MODEL_VARIANT}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Checkpoint: {'‚úì Fine-tuned' if os.path.exists(CHECKPOINT_PATH) else '‚ö† Base model'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cc0f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation dataset for evaluation\n",
    "# This loads a small validation set without needing the full training pipeline\n",
    "\n",
    "from datasets import load_dataset, Audio\n",
    "import soundfile as sf\n",
    "import io\n",
    "\n",
    "VAL_SAMPLES = 400  # Match training configuration\n",
    "\n",
    "print(f\"Loading validation dataset ({VAL_SAMPLES} samples)...\")\n",
    "\n",
    "try:\n",
    "    # Try minds14 first\n",
    "    print(\"Loading minds14 dataset...\")\n",
    "    dataset = load_dataset(\"PolyAI/minds14\", \"en-US\", split=f\"train[:{VAL_SAMPLES}]\")\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(decode=False))\n",
    "    \n",
    "    def decode_audio(example):\n",
    "        audio = example[\"audio\"]\n",
    "        if \"bytes\" in audio and audio[\"bytes\"] is not None:\n",
    "            waveform, sr = sf.read(io.BytesIO(audio[\"bytes\"]))\n",
    "        elif \"path\" in audio and audio[\"path\"] is not None:\n",
    "            waveform, sr = sf.read(audio[\"path\"])\n",
    "        else:\n",
    "            waveform, sr = np.array([], dtype=np.float32), 16000\n",
    "        \n",
    "        if not isinstance(waveform, np.ndarray):\n",
    "            waveform = np.array(waveform, dtype=np.float32)\n",
    "        else:\n",
    "            waveform = waveform.astype(np.float32)\n",
    "        \n",
    "        example[\"audio_decoded\"] = {\"array\": waveform.tolist(), \"sampling_rate\": int(sr)}\n",
    "        return example\n",
    "    \n",
    "    dataset = dataset.map(decode_audio, desc=\"Decoding audio\")\n",
    "    dataset = dataset.remove_columns([\"audio\"])\n",
    "    dataset = dataset.rename_column(\"audio_decoded\", \"audio\")\n",
    "    val_dataset = dataset\n",
    "    dataset_name = \"minds14_en\"\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"minds14 failed: {e}, trying LibriSpeech...\")\n",
    "    dataset = load_dataset(\"openslr/librispeech_asr\", \"clean\", split=f\"test[:{VAL_SAMPLES}]\")\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(decode=False))\n",
    "    dataset = dataset.map(decode_audio, desc=\"Decoding audio\")\n",
    "    dataset = dataset.remove_columns([\"audio\"])\n",
    "    dataset = dataset.rename_column(\"audio_decoded\", \"audio\")\n",
    "    val_dataset = dataset\n",
    "    dataset_name = \"librispeech_clean\"\n",
    "\n",
    "# Detect text column\n",
    "text_column = None\n",
    "for col in [\"text\", \"sentence\", \"transcription\", \"transcript\"]:\n",
    "    if col in val_dataset.column_names:\n",
    "        text_column = col\n",
    "        break\n",
    "\n",
    "print(f\"‚úì Validation dataset loaded: {dataset_name}\")\n",
    "print(f\"  Samples: {len(val_dataset)}\")\n",
    "print(f\"  Text column: '{text_column}'\")\n",
    "\n",
    "# Create validation data loader (optional, for batch evaluation)\n",
    "from src.data import WhisperDataset, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "val_dataset_wrapper = WhisperDataset(\n",
    "    val_dataset,\n",
    "    processor,\n",
    "    audio_column=\"audio\",\n",
    "    text_column=text_column,\n",
    "    max_audio_length_sec=config['data']['audio_max_length_sec'],\n",
    "    augmenter=None\n",
    ")\n",
    "\n",
    "collator = DataCollatorWithPadding(processor)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset_wrapper,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collator,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"‚úì Data loader created ({len(val_loader)} batches)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2762a1",
   "metadata": {},
   "source": [
    "### √∞¬ü¬é¬Ø Professional-Grade Improvements\n",
    "\n",
    "This notebook now includes production-grade enhancements:\n",
    "- **Error Recovery:** Automatic handling of corrupted data, NaN/Inf, OOM errors\n",
    "- **Memory Management:** Intelligent cleanup and optimization\n",
    "- **Audio Validation:** Quality checks for all audio samples\n",
    "- **Structured Logging:** Comprehensive metrics tracking\n",
    "- **Safe Checkpointing:** Atomic writes with validation\n",
    "\n",
    "See `IMPROVEMENTS.md` for full details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dcae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab Resource Optimization & Monitoring\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "# Re-check if in Colab (in case this cell is run independently)\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "def check_disk_space():\n",
    "    \"\"\"Check available disk space.\"\"\"\n",
    "    total, used, free = shutil.disk_usage(\"/\")\n",
    "    print(f\"√∞¬ü¬í¬æ Disk Space:\")\n",
    "    print(f\"   Total: {total // (2**30)} GB\")\n",
    "    print(f\"   Used: {used // (2**30)} GB\")\n",
    "    print(f\"   Free: {free // (2**30)} GB\")\n",
    "    return free // (2**30)\n",
    "\n",
    "def check_gpu_memory():\n",
    "    \"\"\"Check GPU memory usage.\"\"\"\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"\\n√∞¬ü¬é¬Æ GPU Memory:\")\n",
    "        print(f\"   Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"   Reserved: {reserved:.2f} GB\")\n",
    "        print(f\"   Total: {total:.2f} GB\")\n",
    "        print(f\"   Free: {total - reserved:.2f} GB\")\n",
    "\n",
    "def cleanup_cache():\n",
    "    \"\"\"Clear unnecessary cache to free disk space.\"\"\"\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print(\"√¢¬ú?Cache cleared\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Setup Google Drive paths for checkpoints\n",
    "    DRIVE_ROOT = '/content/drive/MyDrive/COMP3057_ASR'\n",
    "    import os\n",
    "    os.makedirs(DRIVE_ROOT, exist_ok=True)\n",
    "    os.makedirs(f'{DRIVE_ROOT}/checkpoints', exist_ok=True)\n",
    "    os.makedirs(f'{DRIVE_ROOT}/logs', exist_ok=True)\n",
    "    print(f\"\\n√∞¬ü¬ì¬Å Google Drive storage: {DRIVE_ROOT}\")\n",
    "    \n",
    "    # Create symlinks to save to Drive instead of local disk\n",
    "    if os.path.exists('checkpoints') and not os.path.islink('checkpoints'):\n",
    "        shutil.rmtree('checkpoints')\n",
    "    if not os.path.exists('checkpoints'):\n",
    "        os.symlink(f'{DRIVE_ROOT}/checkpoints', 'checkpoints')\n",
    "        print(\"√¢¬ú?Checkpoints will be saved to Google Drive\")\n",
    "    \n",
    "    if os.path.exists('logs') and not os.path.islink('logs'):\n",
    "        shutil.rmtree('logs')\n",
    "    if not os.path.exists('logs'):\n",
    "        os.symlink(f'{DRIVE_ROOT}/logs', 'logs')\n",
    "        print(\"√¢¬ú?Logs will be saved to Google Drive\")\n",
    "    \n",
    "    # Check initial resources\n",
    "    print(\"\\n√∞¬ü¬ì¬ä Initial Resource Check:\")\n",
    "    free_disk = check_disk_space()\n",
    "    check_gpu_memory()\n",
    "    \n",
    "    if free_disk < 50:\n",
    "        print(\"\\n√¢¬ö¬†√Ø¬∏¬è  WARNING: Low disk space! Consider:\")\n",
    "        print(\"   1. Use smaller dataset (already optimized)\")\n",
    "        print(\"   2. Use 'tiny' or 'base' model variant\")\n",
    "        print(\"   3. Reduce save_steps to save fewer checkpoints\")\n",
    "else:\n",
    "    # Define cleanup_cache for local use\n",
    "    import torch\n",
    "    def cleanup_cache():\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        import gc\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce2e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Import project modules\n",
    "from src.utils import (\n",
    "    load_config,\n",
    "    set_seed,\n",
    "    setup_logging,\n",
    "    get_device,\n",
    "    ExperimentLogger,\n",
    "    DataVersionManager,\n",
    "    ModelRegistry\n",
    ")\n",
    "\n",
    "from src.data import (\n",
    "    AudioPreprocessor,\n",
    "    VoiceActivityDetector,\n",
    "    AudioAugmenter,\n",
    "    WhisperDataset,\n",
    "    prepare_datasets,\n",
    "    create_dataloaders\n",
    ")\n",
    "\n",
    "from src.models import (\n",
    "    WhisperModelManager,\n",
    "    compare_models\n",
    ")\n",
    "\n",
    "from src.training import WhisperTrainer\n",
    "\n",
    "from src.evaluation import (\n",
    "    ModelEvaluator,\n",
    "    LatencyBenchmark,\n",
    "    TrainingVisualizer,\n",
    "    EvaluationVisualizer,\n",
    "    generate_comparison_table\n",
    ")\n",
    "\n",
    "from src.inference import (\n",
    "    StreamingASR,\n",
    "    BatchInference\n",
    ")\n",
    "\n",
    "print(\"√¢¬ú?All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172c29b4",
   "metadata": {},
   "source": [
    "## 2. Configuration & Reproducibility Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55abd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config('config.yaml')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "set_seed(config['project']['seed'])\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_logging('INFO', 'logs/training.log')\n",
    "\n",
    "# Get device\n",
    "device = get_device(config['project']['device'])\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize experiment tracking (choose: wandb, mlflow, or tensorboard)\n",
    "experiment_logger = ExperimentLogger(\n",
    "    backend=config['mlops']['experiment_tracking']['backend'],\n",
    "    project_name=config['mlops']['experiment_tracking']['project_name'],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Initialize versioning\n",
    "data_version_manager = DataVersionManager(\n",
    "    config['mlops']['versioning']['data_version_file']\n",
    ")\n",
    "model_registry = ModelRegistry(\n",
    "    config['mlops']['versioning']['model_registry']\n",
    ")\n",
    "\n",
    "print(f\"√¢¬ú?Configuration loaded\")\n",
    "print(f\"  - Seed: {config['project']['seed']}\")\n",
    "print(f\"  - Device: {device}\")\n",
    "print(f\"  - Tracking: {config['mlops']['experiment_tracking']['backend']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f62e685",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6286d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing utilities\n",
    "audio_preprocessor = AudioPreprocessor(\n",
    "    target_sr=config['data']['sampling_rate'],\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "vad = VoiceActivityDetector(\n",
    "    threshold=config['data']['vad_threshold']\n",
    ")\n",
    "\n",
    "augmenter = AudioAugmenter(\n",
    "    speed_perturbation=config['data']['augmentation']['speed_perturbation'],\n",
    "    pitch_shift_semitones=config['data']['augmentation']['pitch_shift_semitones'],\n",
    "    background_noise_prob=config['data']['augmentation']['background_noise_prob']\n",
    ") if config['data']['augmentation']['enabled'] else None\n",
    "\n",
    "print(\"√¢¬ú?Preprocessing utilities initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1acff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "# Using small subsets optimized for Colab's disk/memory constraints\n",
    "# For production with more resources, increase the sample counts\n",
    "\n",
    "from datasets import load_dataset, Audio\n",
    "import soundfile as sf\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "# Configuration for Colab\n",
    "TRAIN_SAMPLES = 2000  # Increased for better model quality (80GB GPU can handle this)\n",
    "VAL_SAMPLES = 400     # Proportional validation set for robust evaluation\n",
    "\n",
    "print(f\"Loading dataset with {TRAIN_SAMPLES} train + {VAL_SAMPLES} val samples...\")\n",
    "print(\"(Optimized for Colab - increase samples for production)\")\n",
    "\n",
    "try:\n",
    "    # Try minds14 first - smaller and faster to download\n",
    "    print(\"\\nTrying minds14 dataset (lightweight, ~50MB)...\")\n",
    "    dataset = load_dataset(\n",
    "        \"PolyAI/minds14\",\n",
    "        \"en-US\",\n",
    "        split=f\"train[:{TRAIN_SAMPLES + VAL_SAMPLES}]\"\n",
    "    )\n",
    "    \n",
    "    # Cast audio column to disable automatic decoding\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(decode=False))\n",
    "    \n",
    "    # Pre-decode all audio using soundfile - store in separate column\n",
    "    def decode_audio(example):\n",
    "        audio = example[\"audio\"]\n",
    "        if \"bytes\" in audio and audio[\"bytes\"] is not None:\n",
    "            waveform, sr = sf.read(io.BytesIO(audio[\"bytes\"]))\n",
    "        elif \"path\" in audio and audio[\"path\"] is not None:\n",
    "            waveform, sr = sf.read(audio[\"path\"])\n",
    "        else:\n",
    "            waveform, sr = np.array([], dtype=np.float32), 16000\n",
    "        \n",
    "        # Ensure numpy array\n",
    "        if not isinstance(waveform, np.ndarray):\n",
    "            waveform = np.array(waveform, dtype=np.float32)\n",
    "        else:\n",
    "            waveform = waveform.astype(np.float32)\n",
    "        \n",
    "        # Store decoded audio in NEW column\n",
    "        example[\"audio_decoded\"] = {\"array\": waveform.tolist(), \"sampling_rate\": int(sr)}\n",
    "        return example\n",
    "    \n",
    "    print(\"Pre-decoding audio samples...\")\n",
    "    dataset = dataset.map(decode_audio, desc=\"Decoding audio\")\n",
    "    \n",
    "    # Remove original audio column with Audio feature, rename decoded column\n",
    "    dataset = dataset.remove_columns([\"audio\"])\n",
    "    dataset = dataset.rename_column(\"audio_decoded\", \"audio\")\n",
    "    \n",
    "    # Split into train/val\n",
    "    split_point = TRAIN_SAMPLES\n",
    "    train_dataset = dataset.select(range(split_point))\n",
    "    val_dataset = dataset.select(range(split_point, TRAIN_SAMPLES + VAL_SAMPLES))\n",
    "    \n",
    "    dataset_name = \"minds14_en\"\n",
    "    print(f\"√¢¬ú?minds14 loaded successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"minds14 failed: {e}\")\n",
    "    print(\"\\nTrying LibriSpeech (larger, ~300MB)...\")\n",
    "    \n",
    "    # Fallback to LibriSpeech\n",
    "    dataset = load_dataset(\n",
    "        \"openslr/librispeech_asr\",\n",
    "        \"clean\",\n",
    "        split=f\"test[:{TRAIN_SAMPLES + VAL_SAMPLES}]\"\n",
    "    )\n",
    "    \n",
    "    # Cast audio column to disable automatic decoding\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(decode=False))\n",
    "    \n",
    "    # Pre-decode all audio using soundfile\n",
    "    def decode_audio(example):\n",
    "        audio = example[\"audio\"]\n",
    "        if \"bytes\" in audio and audio[\"bytes\"] is not None:\n",
    "            waveform, sr = sf.read(io.BytesIO(audio[\"bytes\"]))\n",
    "        elif \"path\" in audio and audio[\"path\"] is not None:\n",
    "            waveform, sr = sf.read(audio[\"path\"])\n",
    "        else:\n",
    "            waveform, sr = np.array([], dtype=np.float32), 16000\n",
    "        \n",
    "        if not isinstance(waveform, np.ndarray):\n",
    "            waveform = np.array(waveform, dtype=np.float32)\n",
    "        else:\n",
    "            waveform = waveform.astype(np.float32)\n",
    "        \n",
    "        example[\"audio_decoded\"] = {\"array\": waveform.tolist(), \"sampling_rate\": int(sr)}\n",
    "        return example\n",
    "    \n",
    "    print(\"Pre-decoding audio samples...\")\n",
    "    dataset = dataset.map(decode_audio, desc=\"Decoding audio\")\n",
    "    \n",
    "    # Remove original audio column, rename decoded\n",
    "    dataset = dataset.remove_columns([\"audio\"])\n",
    "    dataset = dataset.rename_column(\"audio_decoded\", \"audio\")\n",
    "    \n",
    "    # Split into train/val\n",
    "    split_point = TRAIN_SAMPLES\n",
    "    train_dataset = dataset.select(range(split_point))\n",
    "    val_dataset = dataset.select(range(split_point, TRAIN_SAMPLES + VAL_SAMPLES))\n",
    "    \n",
    "    dataset_name = \"librispeech_clean\"\n",
    "    print(f\"√¢¬ú?LibriSpeech loaded\")\n",
    "\n",
    "print(f\"\\n√¢¬ú?Dataset loaded: {dataset_name}\")\n",
    "print(f\"  - Training samples: {len(train_dataset)}\")\n",
    "print(f\"  - Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Inspect dataset structure\n",
    "print(f\"\\n√∞¬ü¬ì¬ä Dataset structure:\")\n",
    "print(f\"  - Columns: {train_dataset.column_names}\")\n",
    "\n",
    "# Log dataset version\n",
    "data_version_manager.log_dataset_version(\n",
    "    dataset_name=dataset_name,\n",
    "    version=f\"colab_demo_{TRAIN_SAMPLES}train_{VAL_SAMPLES}val\",\n",
    "    metadata={'train': len(train_dataset), 'val': len(val_dataset)}\n",
    ")\n",
    "\n",
    "# Check disk space after loading\n",
    "if IN_COLAB:\n",
    "    print(\"\\n√∞¬ü¬ì¬ä Disk space after dataset load:\")\n",
    "    check_disk_space()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc7d1fc",
   "metadata": {},
   "source": [
    "## 4. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e251a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model manager\n",
    "model_manager = WhisperModelManager(config)\n",
    "\n",
    "# Choose model variant based on Colab resources\n",
    "# Recommendations for A100 (40GB):\n",
    "# - tiny: 39M params, ~500MB, fastest (RECOMMENDED for demo)\n",
    "# - base: 74M params, ~1GB, good balance\n",
    "# - small: 244M params, ~2GB, better accuracy\n",
    "# - medium: 769M params, ~6GB, best accuracy (may be slow)\n",
    "\n",
    "MODEL_VARIANT = 'small'  # Change to 'base' or 'small' if you have time/resources\n",
    "\n",
    "print(f\"√∞¬ü¬§¬ñ Loading Whisper model: {MODEL_VARIANT}\")\n",
    "print(f\"   (Optimized for Colab - use tiny/base for best experience)\")\n",
    "\n",
    "# Load model and processor\n",
    "model, processor = model_manager.initialize_model(\n",
    "    variant=MODEL_VARIANT,\n",
    "    device=str(device)\n",
    ")\n",
    "\n",
    "# Get model info\n",
    "model_info = model_manager.get_model_info()\n",
    "print(f\"\\n√¢¬ú?Model initialized: {MODEL_VARIANT}\")\n",
    "print(f\"  - Total parameters: {model_info['total_parameters']:,}\")\n",
    "print(f\"  - Trainable parameters: {model_info['trainable_parameters']:,}\")\n",
    "\n",
    "# Check GPU memory after model load\n",
    "if IN_COLAB:\n",
    "    check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a09c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model variants\n",
    "variants_info = compare_models(config)\n",
    "\n",
    "print(\"\\nWhisper Model Variants Comparison:\")\n",
    "print(\"-\" * 60)\n",
    "for variant, info in variants_info.items():\n",
    "    print(f\"{variant:10s} | Params: {info['params']:8s} | Speed: {info['speed']:10s} | Accuracy: {info['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c80b81f",
   "metadata": {},
   "source": [
    "## 5. Prepare Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04885ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets\n",
    "# Auto-detect column names from the dataset\n",
    "audio_column = \"audio\"  # Standard across most datasets\n",
    "# Text column varies: \"text\", \"sentence\", \"transcription\", etc.\n",
    "text_column = None\n",
    "for col in [\"text\", \"sentence\", \"transcription\", \"transcript\"]:\n",
    "    if col in train_dataset.column_names:\n",
    "        text_column = col\n",
    "        break\n",
    "\n",
    "if text_column is None:\n",
    "    raise ValueError(f\"Could not find text column. Available columns: {train_dataset.column_names}\")\n",
    "\n",
    "print(f\"Using columns: audio='{audio_column}', text='{text_column}'\")\n",
    "\n",
    "train_dataset_wrapper = WhisperDataset(\n",
    "    train_dataset,\n",
    "    processor,\n",
    "    audio_column=audio_column,\n",
    "    text_column=text_column,\n",
    "    max_audio_length_sec=config['data']['audio_max_length_sec'],\n",
    "    augmenter=augmenter  # Apply augmentation only to training\n",
    ")\n",
    "\n",
    "val_dataset_wrapper = WhisperDataset(\n",
    "    val_dataset,\n",
    "    processor,\n",
    "    audio_column=audio_column,\n",
    "    text_column=text_column,\n",
    "    max_audio_length_sec=config['data']['audio_max_length_sec'],\n",
    "    augmenter=None  # No augmentation for validation\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "from src.data import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "collator = DataCollatorWithPadding(processor)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset_wrapper,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset_wrapper,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collator,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"\\n√¢¬ú?Data loaders created\")\n",
    "print(f\"  - Training batches: {len(train_loader)}\")\n",
    "print(f\"  - Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49fde4e",
   "metadata": {},
   "source": [
    "## 6. Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27108b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for training\n",
    "model = model_manager.prepare_for_training()\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = WhisperTrainer(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    config=config,\n",
    "    device=str(device),\n",
    "    experiment_logger=experiment_logger\n",
    ")\n",
    "\n",
    "print(\"√¢¬ú?Trainer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test - Run before training\n",
    "print(\"=\" * 60)\n",
    "print(\"PRE-TRAINING VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test 1: Check config\n",
    "print(\"\\n1. Configuration:\")\n",
    "print(f\"  Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"  Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"  Grad accumulation: {config['training']['gradient_accumulation_steps']}\")\n",
    "print(f\"  Augmentation: {config['data']['augmentation']['enabled']}\")\n",
    "assert config['training']['learning_rate'] <= 1e-4, \"LR too high!\"\n",
    "assert not config['data']['augmentation']['enabled'], \"Augmentation should be disabled!\"\n",
    "print(\"  ‚úì Config safe\")\n",
    "\n",
    "# Test 2: Sample batch\n",
    "print(\"\\n2. Testing sample batch:\")\n",
    "test_batch = next(iter(train_loader))\n",
    "print(f\"  Input shape: {test_batch['input_features'].shape}\")\n",
    "print(f\"  Labels shape: {test_batch['labels'].shape}\")\n",
    "\n",
    "# Check for NaN/Inf\n",
    "has_nan = torch.isnan(test_batch['input_features']).any()\n",
    "has_inf = torch.isinf(test_batch['input_features']).any()\n",
    "assert not has_nan, \"NaN detected in features!\"\n",
    "assert not has_inf, \"Inf detected in features!\"\n",
    "print(\"  ‚úì No NaN/Inf in batch\")\n",
    "\n",
    "# Test 3: Forward pass\n",
    "print(\"\\n3. Testing forward pass:\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_out = model(\n",
    "        input_features=test_batch['input_features'][:2].to(device),\n",
    "        labels=test_batch['labels'][:2].to(device)\n",
    "    )\n",
    "    test_loss = test_out.loss\n",
    "\n",
    "print(f\"  Test loss: {test_loss.item():.4f}\")\n",
    "assert not torch.isnan(test_loss), \"NaN loss detected!\"\n",
    "assert not torch.isinf(test_loss), \"Inf loss detected!\"\n",
    "print(\"  ‚úì Forward pass successful\")\n",
    "\n",
    "model.train()\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úì ALL VALIDATION TESTS PASSED\")\n",
    "print(\"Safe to proceed with training\")\n",
    "print(\"=\" * 60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e52984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "# Optimized for Colab: fewer epochs, memory-efficient settings\n",
    "\n",
    "TRAIN_EPOCHS = 5  # Increased for better convergence with larger dataset, 2-3 for Colab session, 5+ for production\n",
    "\n",
    "print(f\"√∞¬ü¬ö¬Ä Starting training for {TRAIN_EPOCHS} epoch(s)...\")\n",
    "print(f\"   Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"   Gradient accumulation: {config['training']['gradient_accumulation_steps']}\")\n",
    "print(f\"   Effective batch size: {config['training']['batch_size'] * config['training']['gradient_accumulation_steps']}\")\n",
    "print(f\"   Mixed precision (FP16): {config['training']['fp16']}\")\n",
    "\n",
    "# Check resources before training\n",
    "if IN_COLAB:\n",
    "    print(\"\\n√∞¬ü¬ì¬ä Pre-training resources:\")\n",
    "    check_disk_space()\n",
    "    check_gpu_memory()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Training\n",
    "best_val_loss = trainer.train(num_epochs=TRAIN_EPOCHS)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n√¢¬ú?Training completed!\")\n",
    "print(f\"  - Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Cleanup after training\n",
    "if IN_COLAB:\n",
    "    print(\"\\n√∞¬ü¬ß¬π Cleaning up...\")\n",
    "    cleanup_cache()\n",
    "    print(\"\\n√∞¬ü¬ì¬ä Post-training resources:\")\n",
    "    check_disk_space()\n",
    "    check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729c9491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register trained model\n",
    "# NOTE: If you get \"ModuleNotFoundError: No module named 'src'\":\n",
    "#   1. Make sure you ran the Setup cell (Section 1) first\n",
    "#   2. Or run the Quick Restart cell to restore the environment\n",
    "\n",
    "from src.utils import get_git_revision\n",
    "\n",
    "model_id = model_registry.register_model(\n",
    "    model_id=f\"{MODEL_VARIANT}_finetuned_{TRAIN_EPOCHS}ep\",\n",
    "    model_path=\"checkpoints/best_model_hf\",\n",
    "    metrics={'val_loss': best_val_loss},\n",
    "    config=config,\n",
    "    git_revision=get_git_revision(),\n",
    "    dataset_version=\"common_voice_11_0_en_subset\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì Model registered: {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a168c3e1",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3607b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    device=str(device)\n",
    ")\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"Evaluating model...\\n\")\n",
    "results = evaluator.evaluate_with_samples(val_loader, num_samples=5)\n",
    "\n",
    "print(f\"\\n√¢¬ú?Evaluation Results:\")\n",
    "print(f\"  - WER: {results['metrics']['wer']:.3f}\")\n",
    "print(f\"  - CER: {results['metrics']['cer']:.3f}\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\n√∞¬ü¬ì¬ù Sample Predictions:\")\n",
    "print(\"-\" * 80)\n",
    "for i, sample in enumerate(results['samples'][:3], 1):\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(f\"  Reference:  {sample['reference']}\")\n",
    "    print(f\"  Prediction: {sample['prediction']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b35df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark latency\n",
    "print(\"Benchmarking inference latency...\\n\")\n",
    "\n",
    "latency_bench = LatencyBenchmark(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    device=str(device)\n",
    ")\n",
    "\n",
    "# Generate test audio clips\n",
    "test_audios = []\n",
    "for i in range(10):  # Test on 10 samples\n",
    "    sample = val_dataset[i]\n",
    "    audio = torch.tensor(sample['audio']['array'])\n",
    "    test_audios.append(audio)\n",
    "\n",
    "latency_results = latency_bench.benchmark_batch(test_audios, sr=16000)\n",
    "\n",
    "print(f\"√¢¬ú?Latency Benchmark Results:\")\n",
    "print(f\"  - Mean latency: {latency_results['mean_latency']:.3f}s\")\n",
    "print(f\"  - Std latency: {latency_results['std_latency']:.3f}s\")\n",
    "print(f\"  - Mean RTF: {latency_results['mean_rtf']:.3f}x\")\n",
    "print(f\"\\n  RTF < 1.0 = Real-time capable √¢¬ú¬ì\" if latency_results['mean_rtf'] < 1.0 else \"  RTF >= 1.0 = Not real-time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47284343",
   "metadata": {},
   "source": [
    "## 8. Real-Time Streaming Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fdd7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize streaming ASR\n",
    "streaming_asr = StreamingASR(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    vad=vad,\n",
    "    chunk_length_sec=config['inference']['streaming']['buffer_size_sec'],\n",
    "    overlap_sec=config['inference']['streaming']['overlap_sec'],\n",
    "    device=str(device)\n",
    ")\n",
    "\n",
    "print(\"√¢¬ú?Streaming ASR initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a820abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test streaming on audio file\n",
    "# Use a sample from validation set\n",
    "\n",
    "# Get a test audio file\n",
    "test_sample = val_dataset[0]\n",
    "test_audio = test_sample['audio']['array']\n",
    "test_text = test_sample[text_column]  # Use the detected text column\n",
    "\n",
    "# Check audio duration\n",
    "audio_duration = len(test_audio) / 16000\n",
    "print(f\"Audio duration: {audio_duration:.2f} seconds\")\n",
    "print(f\"Reference text: {test_text}\\n\")\n",
    "\n",
    "# If audio is very short, try a longer sample\n",
    "if audio_duration < 2.0:\n",
    "    print(\"‚ö† Audio too short for streaming demo, trying longer sample...\")\n",
    "    for i in range(1, min(10, len(val_dataset))):\n",
    "        test_sample = val_dataset[i]\n",
    "        test_audio = test_sample['audio']['array']\n",
    "        test_text = test_sample[text_column]\n",
    "        audio_duration = len(test_audio) / 16000\n",
    "        if audio_duration >= 2.0:\n",
    "            print(f\"‚úì Found longer sample (index {i}): {audio_duration:.2f}s\")\n",
    "            print(f\"  Text: {test_text}\\n\")\n",
    "            break\n",
    "\n",
    "# Save to temporary file\n",
    "import torchaudio\n",
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp:\n",
    "    tmp_path = tmp.name\n",
    "    torchaudio.save(tmp_path, torch.tensor(test_audio).unsqueeze(0), 16000)\n",
    "\n",
    "print(f\"Test audio saved to: {tmp_path}\")\n",
    "\n",
    "# For short audio, disable VAD and use smaller chunks\n",
    "# Create a temporary streaming ASR without VAD\n",
    "streaming_asr_test = StreamingASR(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    vad=None,  # Disable VAD for short audio\n",
    "    chunk_length_sec=config['inference']['streaming']['buffer_size_sec'],\n",
    "    overlap_sec=config['inference']['streaming']['overlap_sec'],\n",
    "    device=str(device)\n",
    ")\n",
    "\n",
    "# Stream from file\n",
    "print(\"\\nStreaming transcription:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "transcriptions = []\n",
    "def callback(text):\n",
    "    if text.strip():  # Only print non-empty chunks\n",
    "        print(f\"[CHUNK] {text}\")\n",
    "        transcriptions.append(text)\n",
    "\n",
    "streaming_asr_test.reset()\n",
    "streaming_asr_test.stream_from_file(tmp_path, chunk_duration_sec=1.0, callback=callback)\n",
    "\n",
    "# Get full transcription\n",
    "full_transcription = streaming_asr_test.get_full_transcription(merge=True)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(f\"\\nüìù Final Transcription: {full_transcription}\")\n",
    "print(f\"üìñ Reference:          {test_text}\")\n",
    "\n",
    "# Calculate WER for this sample\n",
    "from jiwer import wer\n",
    "if full_transcription.strip():\n",
    "    sample_wer = wer(test_text.lower(), full_transcription.lower())\n",
    "    print(f\"üìä Sample WER: {sample_wer:.3f}\")\n",
    "else:\n",
    "    print(\"‚ö† No transcription generated!\")\n",
    "\n",
    "# Cleanup\n",
    "os.unlink(tmp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5321c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Live microphone streaming (requires microphone access)\n",
    "# Uncomment to use:\n",
    "\n",
    "# print(\"Starting live microphone transcription...\")\n",
    "# print(\"Speak into your microphone. Press Ctrl+C to stop.\\n\")\n",
    "\n",
    "# streaming_asr.reset()\n",
    "\n",
    "# def mic_callback(text):\n",
    "#     print(f\"√∞¬ü¬é¬§ {text}\")\n",
    "\n",
    "# streaming_asr.stream_from_microphone(\n",
    "#     duration_sec=30,  # Record for 30 seconds\n",
    "#     callback=mic_callback\n",
    "# )\n",
    "\n",
    "# full_transcription = streaming_asr.get_full_transcription(merge=True)\n",
    "# print(f\"\\n√∞¬ü¬ì¬ù Full Transcription: {full_transcription}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fe6f0b",
   "metadata": {},
   "source": [
    "## 9. Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb8e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch inference for multiple files\n",
    "batch_inference = BatchInference(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    device=str(device),\n",
    "    batch_size=config['inference']['batch_size']\n",
    ")\n",
    "\n",
    "# Create temporary test files\n",
    "import tempfile\n",
    "import torchaudio\n",
    "\n",
    "test_files = []\n",
    "for i in range(5):\n",
    "    sample = val_dataset[i]\n",
    "    audio = torch.tensor(sample['audio']['array'])\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp:\n",
    "        torchaudio.save(tmp.name, audio.unsqueeze(0), 16000)\n",
    "        test_files.append(tmp.name)\n",
    "\n",
    "# Batch transcribe\n",
    "print(\"Batch transcribing 5 files...\\n\")\n",
    "batch_transcriptions = batch_inference.transcribe_batch(test_files)\n",
    "\n",
    "# Display results\n",
    "for i, transcription in enumerate(batch_transcriptions):\n",
    "    reference = val_dataset[i][text_column]  # Use the detected text column\n",
    "    print(f\"File {i+1}:\")\n",
    "    print(f\"  Prediction: {transcription}\")\n",
    "    print(f\"  Reference:  {reference}\")\n",
    "    print()\n",
    "\n",
    "# Cleanup\n",
    "for f in test_files:\n",
    "    os.unlink(f)\n",
    "\n",
    "print(\"√¢¬ú?Batch inference completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85b07c4",
   "metadata": {},
   "source": [
    "## 10. Visualization & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837705fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize evaluation results\n",
    "eval_viz = EvaluationVisualizer(save_dir='plots')\n",
    "\n",
    "# Model comparison data (example)\n",
    "comparison_data = {\n",
    "    'whisper-tiny': {\n",
    "        'params': '39M',\n",
    "        'wer_clean': 0.15,\n",
    "        'wer_accented': 0.20,\n",
    "        'latency': 0.1,\n",
    "        'rtf': 0.1\n",
    "    },\n",
    "    'whisper-base': {\n",
    "        'params': '74M',\n",
    "        'wer_clean': results['metrics']['wer'],\n",
    "        'wer_accented': 0.14,\n",
    "        'latency': latency_results['mean_latency'],\n",
    "        'rtf': latency_results['mean_rtf']\n",
    "    },\n",
    "    'whisper-small': {\n",
    "        'params': '244M',\n",
    "        'wer_clean': 0.08,\n",
    "        'wer_accented': 0.12,\n",
    "        'latency': 0.5,\n",
    "        'rtf': 0.5\n",
    "    }\n",
    "}\n",
    "\n",
    "# Plot comparison\n",
    "comparison_path = eval_viz.plot_model_comparison(comparison_data)\n",
    "print(f\"√¢¬ú?Model comparison plot saved: {comparison_path}\")\n",
    "\n",
    "# Display\n",
    "from IPython.display import Image\n",
    "display(Image(comparison_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3016aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comparison table\n",
    "table = generate_comparison_table(comparison_data)\n",
    "print(\"\\nModel Comparison Table:\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456007d8",
   "metadata": {},
   "source": [
    "## 11. Export & Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2008269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model for deployment\n",
    "FINAL_MODEL_PATH = 'final_model'\n",
    "\n",
    "model.save_pretrained(FINAL_MODEL_PATH)\n",
    "processor.save_pretrained(FINAL_MODEL_PATH)\n",
    "\n",
    "print(f\"√¢¬ú?Final model saved to: {FINAL_MODEL_PATH}\")\n",
    "print(f\"\\nTo load model later:\")\n",
    "print(f\"  from transformers import WhisperForConditionalGeneration, WhisperProcessor\")\n",
    "print(f\"  model = WhisperForConditionalGeneration.from_pretrained('{FINAL_MODEL_PATH}')\")\n",
    "print(f\"  processor = WhisperProcessor.from_pretrained('{FINAL_MODEL_PATH}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2b1b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Upload to HuggingFace Hub\n",
    "# Requires HuggingFace token and authentication\n",
    "\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "\n",
    "# HF_MODEL_NAME = \"your-username/whisper-base-finetuned-en\"\n",
    "# model.push_to_hub(HF_MODEL_NAME)\n",
    "# processor.push_to_hub(HF_MODEL_NAME)\n",
    "# print(f\"√¢¬ú?Model uploaded to HuggingFace: {HF_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a91e659",
   "metadata": {},
   "source": [
    "## 12. Cleanup & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3275866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish experiment tracking\n",
    "experiment_logger.finish()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"√∞¬ü¬ì¬ä PROJECT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n√∞¬ü¬§¬ñ Model Configuration:\")\n",
    "print(f\"  - Variant: {MODEL_VARIANT}\")\n",
    "print(f\"  - Training epochs: {TRAIN_EPOCHS}\")\n",
    "print(f\"  - Dataset: {dataset_name}\")\n",
    "print(f\"  - Training samples: {len(train_dataset)}\")\n",
    "print(f\"  - Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "print(f\"\\n√∞¬ü¬ì¬à Training Results:\")\n",
    "print(f\"  - Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"  - WER: {results['metrics']['wer']:.3f}\")\n",
    "print(f\"  - CER: {results['metrics']['cer']:.3f}\")\n",
    "\n",
    "print(f\"\\n√¢¬ö?Performance Metrics:\")\n",
    "print(f\"  - Mean latency: {latency_results['mean_latency']:.3f}s\")\n",
    "print(f\"  - RTF: {latency_results['mean_rtf']:.3f}x\")\n",
    "print(f\"  - Real-time capable: {'√¢¬ú?Yes' if latency_results['mean_rtf'] < 1.0 else '√¢¬ú?No'}\")\n",
    "\n",
    "print(f\"\\n√∞¬ü¬í¬æ Saved Artifacts:\")\n",
    "if IN_COLAB:\n",
    "    print(f\"  - Model: {FINAL_MODEL_PATH}\")\n",
    "    print(f\"  - Checkpoints: {DRIVE_ROOT}/checkpoints/\")\n",
    "    print(f\"  - Logs: {DRIVE_ROOT}/logs/\")\n",
    "    print(f\"  - Plots: plots/\")\n",
    "else:\n",
    "    print(f\"  - Model: {FINAL_MODEL_PATH}\")\n",
    "    print(f\"  - Checkpoints: checkpoints/\")\n",
    "    print(f\"  - Logs: logs/\")\n",
    "    print(f\"  - Plots: plots/\")\n",
    "\n",
    "# Final resource check\n",
    "if IN_COLAB:\n",
    "    print(\"\\n√∞¬ü¬ì¬ä Final Resource Usage:\")\n",
    "    check_disk_space()\n",
    "    check_gpu_memory()\n",
    "    \n",
    "    print(\"\\n√∞¬ü¬í¬° Tips for Colab:\")\n",
    "    print(\"  - Checkpoints saved to Google Drive persist across sessions\")\n",
    "    print(\"  - Increase TRAIN_SAMPLES/VAL_SAMPLES for better accuracy\")\n",
    "    print(\"  - Try 'base' or 'small' model for better quality\")\n",
    "    print(\"  - Use Runtime √¢¬Ü?Factory reset runtime to free all resources\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"√∞¬ü¬é¬â Real-Time ASR System Ready for Deployment!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
