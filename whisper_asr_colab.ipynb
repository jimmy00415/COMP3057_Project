{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "995862c2",
   "metadata": {},
   "source": [
    "# Real-Time Multilingual ASR with Whisper\n",
    "\n",
    "This notebook implements a production-ready, real-time speech recognition system using OpenAI's Whisper models.\n",
    "\n",
    "**Features:**\n",
    "- Data preparation with augmentation\n",
    "- Model fine-tuning with multiple variants\n",
    "- Comprehensive evaluation (WER, CER, latency)\n",
    "- Real-time streaming inference\n",
    "- Full MLOps best practices (versioning, logging, reproducibility)\n",
    "\n",
    "**Author:** COMP3057 Project  \n",
    "**Version:** 1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c25953",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23841875",
   "metadata": {},
   "source": [
    "### √¢¬ö¬ô√Ø¬∏¬è Colab Configuration Guide\n",
    "\n",
    "**Resource Constraints:** A100 GPU (~40GB), 220GB Disk\n",
    "\n",
    "**Quick Start Options:**\n",
    "\n",
    "| Profile | Model | Samples | Epochs | Time | Disk | Quality |\n",
    "|---------|-------|---------|--------|------|------|---------|\n",
    "| **Fast Demo** | tiny | 50/10 | 1 | ~5min | ~2GB | Basic |\n",
    "| **Balanced** | base | 200/40 | 2 | ~20min | ~5GB | Good |\n",
    "| **Best Quality** | small | 500/100 | 3 | ~60min | ~10GB | Better |\n",
    "\n",
    "**Adjustable Parameters (in cells below):**\n",
    "- `TRAIN_SAMPLES` / `VAL_SAMPLES` - Dataset size\n",
    "- `MODEL_VARIANT` - Model quality (tiny/base/small)\n",
    "- `TRAIN_EPOCHS` - Training duration\n",
    "\n",
    "**Tips:**\n",
    "- Start with **Fast Demo** to verify everything works\n",
    "- Increase resources gradually if you have time\n",
    "- Checkpoints auto-save to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f24b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment and clone project\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if running in Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"√∞¬ü¬î¬ß Running in Google Colab\")\n",
    "    \n",
    "    # Mount Google Drive for saving checkpoints and logs\n",
    "    from google.colab import drive\n",
    "    if not os.path.exists('/content/drive'):\n",
    "        print(\"√∞¬ü¬ì¬Å Mounting Google Drive...\")\n",
    "        drive.mount('/content/drive', force_remount=False)\n",
    "        print(\"√¢¬ú?Drive mounted at /content/drive\")\n",
    "    else:\n",
    "        print(\"√¢¬ú?Drive already mounted\")\n",
    "    \n",
    "    # --- Project Setup and Path Configuration ---\n",
    "    PROJECT_DIR = 'COMP3057_Project'\n",
    "    \n",
    "    # Clone repository if not exists\n",
    "    if not os.path.exists(PROJECT_DIR):\n",
    "        print(\"\\n√∞¬ü¬ì¬¶ Cloning repository from GitHub...\")\n",
    "        !git clone https://github.com/jimmy00415/COMP3057_Project.git\n",
    "        print(\"√¢¬ú?Repository cloned\")\n",
    "    else:\n",
    "        print(\"√¢¬ú?Repository already exists\")\n",
    "    \n",
    "    # Change to project directory\n",
    "    os.chdir(PROJECT_DIR)\n",
    "    print(f\"√¢¬ú?Working directory: {os.getcwd()}\")\n",
    "\n",
    "    # Add project root to Python path\n",
    "    # This is the crucial step to solve ModuleNotFoundError\n",
    "    if os.getcwd() not in sys.path:\n",
    "        sys.path.insert(0, os.getcwd())\n",
    "        print(f\"√¢¬ú?Added '{os.getcwd()}' to Python path\")\n",
    "    \n",
    "    # Install dependencies IMMEDIATELY after cloning\n",
    "    print(\"\\n√∞¬ü¬ì¬¶ Installing dependencies...\")\n",
    "    !pip install -q -r requirements.txt\n",
    "    !pip install -q sounddevice\n",
    "    print(\"√¢¬ú?Dependencies installed\")\n",
    "\n",
    "else:\n",
    "    print(\"√∞¬ü¬í¬ª Running locally\")\n",
    "    print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "gpu_available = torch.cuda.is_available()\n",
    "if gpu_available:\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"\\n√∞¬ü¬é¬Æ GPU: {gpu_name}\")\n",
    "    print(f\"   Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"\\n√¢¬ö¬†√Ø¬∏¬è  WARNING: No GPU detected! Training will be very slow.\")\n",
    "    print(\"   Enable GPU: Runtime √¢¬Ü?Change runtime type √¢¬Ü?GPU (T4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40c0652",
   "metadata": {},
   "source": [
    "### üîÑ Quick Restart (Skip Training)\n",
    "\n",
    "**If you've already trained a model and just want to run inference/evaluation:**\n",
    "\n",
    "1. Run the **Setup cell above** (mounts Drive, clones repo, installs deps)\n",
    "2. Run the **Quick Restart cell below** to load your trained model\n",
    "3. Skip to Section 7 (Evaluation) or later sections\n",
    "\n",
    "This avoids re-running the expensive training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28021bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Restart - Load existing model without retraining\n",
    "# Run this cell AFTER the setup cell if you want to skip training\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Ensure project path is available (in case you skipped setup)\n",
    "if 'google.colab' in sys.modules:\n",
    "    project_root = '/content/COMP3057_Project'\n",
    "    if os.path.exists(project_root):\n",
    "        os.chdir(project_root)\n",
    "        if project_root not in sys.path:\n",
    "            sys.path.insert(0, project_root)\n",
    "        print(f\"‚úì Working in {project_root}\")\n",
    "    else:\n",
    "        raise RuntimeError(\"Project not found. Run the Setup cell first!\")\n",
    "else:\n",
    "    # Local environment\n",
    "    project_root = os.getcwd()\n",
    "    if project_root not in sys.path:\n",
    "        sys.path.insert(0, project_root)\n",
    "\n",
    "# Import all necessary modules\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from src.utils.config import load_config\n",
    "from src.models.whisper_model import WhisperModelManager\n",
    "from src.utils.versioning import ModelRegistry\n",
    "\n",
    "# Import evaluation and inference modules\n",
    "from src.evaluation import (\n",
    "    ModelEvaluator,\n",
    "    LatencyBenchmark,\n",
    "    EvaluationVisualizer,\n",
    "    generate_comparison_table\n",
    ")\n",
    "\n",
    "from src.inference import (\n",
    "    StreamingASR,\n",
    "    BatchInference\n",
    ")\n",
    "\n",
    "from src.data import VoiceActivityDetector\n",
    "\n",
    "print(\"‚úì Modules imported\")\n",
    "\n",
    "# Load config\n",
    "config = load_config('config.yaml')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize VAD for streaming\n",
    "vad = VoiceActivityDetector(threshold=config['data']['vad_threshold'])\n",
    "\n",
    "# Specify your trained model variant and checkpoint path\n",
    "MODEL_VARIANT = \"small\"  # Change to match your trained model\n",
    "CHECKPOINT_PATH = \"checkpoints/best_model_hf\"  # Or your specific checkpoint\n",
    "\n",
    "# Load model\n",
    "print(f\"Loading model: {MODEL_VARIANT}\")\n",
    "model_manager = WhisperModelManager(config)\n",
    "\n",
    "# Try to load from checkpoint if it exists\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\"Loading fine-tuned model from {CHECKPOINT_PATH}...\")\n",
    "    from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "    \n",
    "    model = WhisperForConditionalGeneration.from_pretrained(CHECKPOINT_PATH)\n",
    "    processor = WhisperProcessor.from_pretrained(CHECKPOINT_PATH)\n",
    "    model = model.to(device)\n",
    "    print(f\"‚úì Loaded fine-tuned model from checkpoint\")\n",
    "else:\n",
    "    print(f\"‚ö† Checkpoint not found at {CHECKPOINT_PATH}\")\n",
    "    print(f\"  Loading base {MODEL_VARIANT} model instead...\")\n",
    "    model, processor = model_manager.initialize_model(variant=MODEL_VARIANT, device=str(device))\n",
    "\n",
    "# Initialize model registry\n",
    "model_registry = ModelRegistry()\n",
    "\n",
    "# Set model to eval mode\n",
    "model.eval()\n",
    "\n",
    "print(\"\\n‚úì Quick restart complete!\")\n",
    "print(\"  You can now skip to Section 7 (Evaluation) or later\")\n",
    "print(f\"  Model: {MODEL_VARIANT}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Checkpoint: {'‚úì Fine-tuned' if os.path.exists(CHECKPOINT_PATH) else '‚ö† Base model'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cc0f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation dataset for evaluation\n",
    "# This loads a small validation set without needing the full training pipeline\n",
    "\n",
    "from datasets import load_dataset, Audio\n",
    "import soundfile as sf\n",
    "import io\n",
    "\n",
    "VAL_SAMPLES = 400  # Match training configuration\n",
    "\n",
    "print(f\"Loading validation dataset ({VAL_SAMPLES} samples)...\")\n",
    "\n",
    "try:\n",
    "    # Try minds14 first\n",
    "    print(\"Loading minds14 dataset...\")\n",
    "    dataset = load_dataset(\"PolyAI/minds14\", \"en-US\", split=f\"train[:{VAL_SAMPLES}]\")\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(decode=False))\n",
    "    \n",
    "    def decode_audio(example):\n",
    "        audio = example[\"audio\"]\n",
    "        if \"bytes\" in audio and audio[\"bytes\"] is not None:\n",
    "            waveform, sr = sf.read(io.BytesIO(audio[\"bytes\"]))\n",
    "        elif \"path\" in audio and audio[\"path\"] is not None:\n",
    "            waveform, sr = sf.read(audio[\"path\"])\n",
    "        else:\n",
    "            waveform, sr = np.array([], dtype=np.float32), 16000\n",
    "        \n",
    "        if not isinstance(waveform, np.ndarray):\n",
    "            waveform = np.array(waveform, dtype=np.float32)\n",
    "        else:\n",
    "            waveform = waveform.astype(np.float32)\n",
    "        \n",
    "        example[\"audio_decoded\"] = {\"array\": waveform.tolist(), \"sampling_rate\": int(sr)}\n",
    "        return example\n",
    "    \n",
    "    dataset = dataset.map(decode_audio, desc=\"Decoding audio\")\n",
    "    dataset = dataset.remove_columns([\"audio\"])\n",
    "    dataset = dataset.rename_column(\"audio_decoded\", \"audio\")\n",
    "    val_dataset = dataset\n",
    "    dataset_name = \"minds14_en\"\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"minds14 failed: {e}, trying LibriSpeech...\")\n",
    "    dataset = load_dataset(\"openslr/librispeech_asr\", \"clean\", split=f\"test[:{VAL_SAMPLES}]\")\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(decode=False))\n",
    "    dataset = dataset.map(decode_audio, desc=\"Decoding audio\")\n",
    "    dataset = dataset.remove_columns([\"audio\"])\n",
    "    dataset = dataset.rename_column(\"audio_decoded\", \"audio\")\n",
    "    val_dataset = dataset\n",
    "    dataset_name = \"librispeech_clean\"\n",
    "\n",
    "# Detect text column\n",
    "text_column = None\n",
    "for col in [\"text\", \"sentence\", \"transcription\", \"transcript\"]:\n",
    "    if col in val_dataset.column_names:\n",
    "        text_column = col\n",
    "        break\n",
    "\n",
    "print(f\"‚úì Validation dataset loaded: {dataset_name}\")\n",
    "print(f\"  Samples: {len(val_dataset)}\")\n",
    "print(f\"  Text column: '{text_column}'\")\n",
    "\n",
    "# Create validation data loader (optional, for batch evaluation)\n",
    "from src.data import WhisperDataset, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "val_dataset_wrapper = WhisperDataset(\n",
    "    val_dataset,\n",
    "    processor,\n",
    "    audio_column=\"audio\",\n",
    "    text_column=text_column,\n",
    "    max_audio_length_sec=config['data']['audio_max_length_sec'],\n",
    "    augmenter=None\n",
    ")\n",
    "\n",
    "collator = DataCollatorWithPadding(processor)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset_wrapper,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collator,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"‚úì Data loader created ({len(val_loader)} batches)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2762a1",
   "metadata": {},
   "source": [
    "### √∞¬ü¬é¬Ø Professional-Grade Improvements\n",
    "\n",
    "This notebook now includes production-grade enhancements:\n",
    "- **Error Recovery:** Automatic handling of corrupted data, NaN/Inf, OOM errors\n",
    "- **Memory Management:** Intelligent cleanup and optimization\n",
    "- **Audio Validation:** Quality checks for all audio samples\n",
    "- **Structured Logging:** Comprehensive metrics tracking\n",
    "- **Safe Checkpointing:** Atomic writes with validation\n",
    "\n",
    "See `IMPROVEMENTS.md` for full details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dcae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab Resource Optimization & Monitoring\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "# Re-check if in Colab (in case this cell is run independently)\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "def check_disk_space():\n",
    "    \"\"\"Check available disk space.\"\"\"\n",
    "    total, used, free = shutil.disk_usage(\"/\")\n",
    "    print(f\"√∞¬ü¬í¬æ Disk Space:\")\n",
    "    print(f\"   Total: {total // (2**30)} GB\")\n",
    "    print(f\"   Used: {used // (2**30)} GB\")\n",
    "    print(f\"   Free: {free // (2**30)} GB\")\n",
    "    return free // (2**30)\n",
    "\n",
    "def check_gpu_memory():\n",
    "    \"\"\"Check GPU memory usage.\"\"\"\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"\\n√∞¬ü¬é¬Æ GPU Memory:\")\n",
    "        print(f\"   Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"   Reserved: {reserved:.2f} GB\")\n",
    "        print(f\"   Total: {total:.2f} GB\")\n",
    "        print(f\"   Free: {total - reserved:.2f} GB\")\n",
    "\n",
    "def cleanup_cache():\n",
    "    \"\"\"Clear unnecessary cache to free disk space.\"\"\"\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print(\"√¢¬ú?Cache cleared\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Setup Google Drive paths for checkpoints\n",
    "    DRIVE_ROOT = '/content/drive/MyDrive/COMP3057_ASR'\n",
    "    import os\n",
    "    os.makedirs(DRIVE_ROOT, exist_ok=True)\n",
    "    os.makedirs(f'{DRIVE_ROOT}/checkpoints', exist_ok=True)\n",
    "    os.makedirs(f'{DRIVE_ROOT}/logs', exist_ok=True)\n",
    "    print(f\"\\n√∞¬ü¬ì¬Å Google Drive storage: {DRIVE_ROOT}\")\n",
    "    \n",
    "    # Create symlinks to save to Drive instead of local disk\n",
    "    if os.path.exists('checkpoints') and not os.path.islink('checkpoints'):\n",
    "        shutil.rmtree('checkpoints')\n",
    "    if not os.path.exists('checkpoints'):\n",
    "        os.symlink(f'{DRIVE_ROOT}/checkpoints', 'checkpoints')\n",
    "        print(\"√¢¬ú?Checkpoints will be saved to Google Drive\")\n",
    "    \n",
    "    if os.path.exists('logs') and not os.path.islink('logs'):\n",
    "        shutil.rmtree('logs')\n",
    "    if not os.path.exists('logs'):\n",
    "        os.symlink(f'{DRIVE_ROOT}/logs', 'logs')\n",
    "        print(\"√¢¬ú?Logs will be saved to Google Drive\")\n",
    "    \n",
    "    # Check initial resources\n",
    "    print(\"\\n√∞¬ü¬ì¬ä Initial Resource Check:\")\n",
    "    free_disk = check_disk_space()\n",
    "    check_gpu_memory()\n",
    "    \n",
    "    if free_disk < 50:\n",
    "        print(\"\\n√¢¬ö¬†√Ø¬∏¬è  WARNING: Low disk space! Consider:\")\n",
    "        print(\"   1. Use smaller dataset (already optimized)\")\n",
    "        print(\"   2. Use 'tiny' or 'base' model variant\")\n",
    "        print(\"   3. Reduce save_steps to save fewer checkpoints\")\n",
    "else:\n",
    "    # Define cleanup_cache for local use\n",
    "    import torch\n",
    "    def cleanup_cache():\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        import gc\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce2e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Import project modules\n",
    "from src.utils import (\n",
    "    load_config,\n",
    "    set_seed,\n",
    "    setup_logging,\n",
    "    get_device,\n",
    "    ExperimentLogger,\n",
    "    DataVersionManager,\n",
    "    ModelRegistry\n",
    ")\n",
    "\n",
    "from src.data import (\n",
    "    AudioPreprocessor,\n",
    "    VoiceActivityDetector,\n",
    "    AudioAugmenter,\n",
    "    WhisperDataset,\n",
    "    prepare_datasets,\n",
    "    create_dataloaders\n",
    ")\n",
    "\n",
    "from src.models import (\n",
    "    WhisperModelManager,\n",
    "    compare_models\n",
    ")\n",
    "\n",
    "from src.training import WhisperTrainer\n",
    "\n",
    "from src.evaluation import (\n",
    "    ModelEvaluator,\n",
    "    LatencyBenchmark,\n",
    "    TrainingVisualizer,\n",
    "    EvaluationVisualizer,\n",
    "    generate_comparison_table\n",
    ")\n",
    "\n",
    "from src.inference import (\n",
    "    StreamingASR,\n",
    "    BatchInference\n",
    ")\n",
    "\n",
    "print(\"√¢¬ú?All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172c29b4",
   "metadata": {},
   "source": [
    "## 2. Configuration & Reproducibility Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55abd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config('config.yaml')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "set_seed(config['project']['seed'])\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_logging('INFO', 'logs/training.log')\n",
    "\n",
    "# Get device\n",
    "device = get_device(config['project']['device'])\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize experiment tracking (choose: wandb, mlflow, or tensorboard)\n",
    "experiment_logger = ExperimentLogger(\n",
    "    backend=config['mlops']['experiment_tracking']['backend'],\n",
    "    project_name=config['mlops']['experiment_tracking']['project_name'],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Initialize versioning\n",
    "data_version_manager = DataVersionManager(\n",
    "    config['mlops']['versioning']['data_version_file']\n",
    ")\n",
    "model_registry = ModelRegistry(\n",
    "    config['mlops']['versioning']['model_registry']\n",
    ")\n",
    "\n",
    "print(f\"√¢¬ú?Configuration loaded\")\n",
    "print(f\"  - Seed: {config['project']['seed']}\")\n",
    "print(f\"  - Device: {device}\")\n",
    "print(f\"  - Tracking: {config['mlops']['experiment_tracking']['backend']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f62e685",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6286d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing utilities\n",
    "audio_preprocessor = AudioPreprocessor(\n",
    "    target_sr=config['data']['sampling_rate'],\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "vad = VoiceActivityDetector(\n",
    "    threshold=config['data']['vad_threshold']\n",
    ")\n",
    "\n",
    "augmenter = AudioAugmenter(\n",
    "    speed_perturbation=config['data']['augmentation']['speed_perturbation'],\n",
    "    pitch_shift_semitones=config['data']['augmentation']['pitch_shift_semitones'],\n",
    "    background_noise_prob=config['data']['augmentation']['background_noise_prob']\n",
    ") if config['data']['augmentation']['enabled'] else None\n",
    "\n",
    "print(\"√¢¬ú?Preprocessing utilities initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1acff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "# Using small subsets optimized for Colab's disk/memory constraints\n",
    "# For production with more resources, increase the sample counts\n",
    "\n",
    "from datasets import load_dataset, Audio\n",
    "import soundfile as sf\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "# Configuration for Colab\n",
    "TRAIN_SAMPLES = 2000  # Increased for better model quality (80GB GPU can handle this)\n",
    "VAL_SAMPLES = 400     # Proportional validation set for robust evaluation\n",
    "\n",
    "print(f\"Loading dataset with {TRAIN_SAMPLES} train + {VAL_SAMPLES} val samples...\")\n",
    "print(\"(Optimized for Colab - increase samples for production)\")\n",
    "\n",
    "try:\n",
    "    # Try minds14 first - smaller and faster to download\n",
    "    print(\"\\nTrying minds14 dataset (lightweight, ~50MB)...\")\n",
    "    dataset = load_dataset(\n",
    "        \"PolyAI/minds14\",\n",
    "        \"en-US\",\n",
    "        split=f\"train[:{TRAIN_SAMPLES + VAL_SAMPLES}]\"\n",
    "    )\n",
    "    \n",
    "    # Cast audio column to disable automatic decoding\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(decode=False))\n",
    "    \n",
    "    # Pre-decode all audio using soundfile - store in separate column\n",
    "    def decode_audio(example):\n",
    "        audio = example[\"audio\"]\n",
    "        if \"bytes\" in audio and audio[\"bytes\"] is not None:\n",
    "            waveform, sr = sf.read(io.BytesIO(audio[\"bytes\"]))\n",
    "        elif \"path\" in audio and audio[\"path\"] is not None:\n",
    "            waveform, sr = sf.read(audio[\"path\"])\n",
    "        else:\n",
    "            waveform, sr = np.array([], dtype=np.float32), 16000\n",
    "        \n",
    "        # Ensure numpy array\n",
    "        if not isinstance(waveform, np.ndarray):\n",
    "            waveform = np.array(waveform, dtype=np.float32)\n",
    "        else:\n",
    "            waveform = waveform.astype(np.float32)\n",
    "        \n",
    "        # Store decoded audio in NEW column\n",
    "        example[\"audio_decoded\"] = {\"array\": waveform.tolist(), \"sampling_rate\": int(sr)}\n",
    "        return example\n",
    "    \n",
    "    print(\"Pre-decoding audio samples...\")\n",
    "    dataset = dataset.map(decode_audio, desc=\"Decoding audio\")\n",
    "    \n",
    "    # Remove original audio column with Audio feature, rename decoded column\n",
    "    dataset = dataset.remove_columns([\"audio\"])\n",
    "    dataset = dataset.rename_column(\"audio_decoded\", \"audio\")\n",
    "    \n",
    "    # Split into train/val\n",
    "    split_point = TRAIN_SAMPLES\n",
    "    train_dataset = dataset.select(range(split_point))\n",
    "    val_dataset = dataset.select(range(split_point, TRAIN_SAMPLES + VAL_SAMPLES))\n",
    "    \n",
    "    dataset_name = \"minds14_en\"\n",
    "    print(f\"√¢¬ú?minds14 loaded successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"minds14 failed: {e}\")\n",
    "    print(\"\\nTrying LibriSpeech (larger, ~300MB)...\")\n",
    "    \n",
    "    # Fallback to LibriSpeech\n",
    "    dataset = load_dataset(\n",
    "        \"openslr/librispeech_asr\",\n",
    "        \"clean\",\n",
    "        split=f\"test[:{TRAIN_SAMPLES + VAL_SAMPLES}]\"\n",
    "    )\n",
    "    \n",
    "    # Cast audio column to disable automatic decoding\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(decode=False))\n",
    "    \n",
    "    # Pre-decode all audio using soundfile\n",
    "    def decode_audio(example):\n",
    "        audio = example[\"audio\"]\n",
    "        if \"bytes\" in audio and audio[\"bytes\"] is not None:\n",
    "            waveform, sr = sf.read(io.BytesIO(audio[\"bytes\"]))\n",
    "        elif \"path\" in audio and audio[\"path\"] is not None:\n",
    "            waveform, sr = sf.read(audio[\"path\"])\n",
    "        else:\n",
    "            waveform, sr = np.array([], dtype=np.float32), 16000\n",
    "        \n",
    "        if not isinstance(waveform, np.ndarray):\n",
    "            waveform = np.array(waveform, dtype=np.float32)\n",
    "        else:\n",
    "            waveform = waveform.astype(np.float32)\n",
    "        \n",
    "        example[\"audio_decoded\"] = {\"array\": waveform.tolist(), \"sampling_rate\": int(sr)}\n",
    "        return example\n",
    "    \n",
    "    print(\"Pre-decoding audio samples...\")\n",
    "    dataset = dataset.map(decode_audio, desc=\"Decoding audio\")\n",
    "    \n",
    "    # Remove original audio column, rename decoded\n",
    "    dataset = dataset.remove_columns([\"audio\"])\n",
    "    dataset = dataset.rename_column(\"audio_decoded\", \"audio\")\n",
    "    \n",
    "    # Split into train/val\n",
    "    split_point = TRAIN_SAMPLES\n",
    "    train_dataset = dataset.select(range(split_point))\n",
    "    val_dataset = dataset.select(range(split_point, TRAIN_SAMPLES + VAL_SAMPLES))\n",
    "    \n",
    "    dataset_name = \"librispeech_clean\"\n",
    "    print(f\"√¢¬ú?LibriSpeech loaded\")\n",
    "\n",
    "print(f\"\\n√¢¬ú?Dataset loaded: {dataset_name}\")\n",
    "print(f\"  - Training samples: {len(train_dataset)}\")\n",
    "print(f\"  - Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Inspect dataset structure\n",
    "print(f\"\\n√∞¬ü¬ì¬ä Dataset structure:\")\n",
    "print(f\"  - Columns: {train_dataset.column_names}\")\n",
    "\n",
    "# Log dataset version\n",
    "data_version_manager.log_dataset_version(\n",
    "    dataset_name=dataset_name,\n",
    "    version=f\"colab_demo_{TRAIN_SAMPLES}train_{VAL_SAMPLES}val\",\n",
    "    metadata={'train': len(train_dataset), 'val': len(val_dataset)}\n",
    ")\n",
    "\n",
    "# Check disk space after loading\n",
    "if IN_COLAB:\n",
    "    print(\"\\n√∞¬ü¬ì¬ä Disk space after dataset load:\")\n",
    "    check_disk_space()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc7d1fc",
   "metadata": {},
   "source": [
    "## 4. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e251a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model manager\n",
    "model_manager = WhisperModelManager(config)\n",
    "\n",
    "# Choose model variant based on Colab resources\n",
    "# Recommendations for A100 (40GB):\n",
    "# - tiny: 39M params, ~500MB, fastest (RECOMMENDED for demo)\n",
    "# - base: 74M params, ~1GB, good balance\n",
    "# - small: 244M params, ~2GB, better accuracy\n",
    "# - medium: 769M params, ~6GB, best accuracy (may be slow)\n",
    "\n",
    "MODEL_VARIANT = 'small'  # Change to 'base' or 'small' if you have time/resources\n",
    "\n",
    "print(f\"√∞¬ü¬§¬ñ Loading Whisper model: {MODEL_VARIANT}\")\n",
    "print(f\"   (Optimized for Colab - use tiny/base for best experience)\")\n",
    "\n",
    "# Load model and processor\n",
    "model, processor = model_manager.initialize_model(\n",
    "    variant=MODEL_VARIANT,\n",
    "    device=str(device)\n",
    ")\n",
    "\n",
    "# Get model info\n",
    "model_info = model_manager.get_model_info()\n",
    "print(f\"\\n√¢¬ú?Model initialized: {MODEL_VARIANT}\")\n",
    "print(f\"  - Total parameters: {model_info['total_parameters']:,}\")\n",
    "print(f\"  - Trainable parameters: {model_info['trainable_parameters']:,}\")\n",
    "\n",
    "# Check GPU memory after model load\n",
    "if IN_COLAB:\n",
    "    check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a09c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model variants\n",
    "variants_info = compare_models(config)\n",
    "\n",
    "print(\"\\nWhisper Model Variants Comparison:\")\n",
    "print(\"-\" * 60)\n",
    "for variant, info in variants_info.items():\n",
    "    print(f\"{variant:10s} | Params: {info['params']:8s} | Speed: {info['speed']:10s} | Accuracy: {info['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c80b81f",
   "metadata": {},
   "source": [
    "## 5. Prepare Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04885ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets\n",
    "# Auto-detect column names from the dataset\n",
    "audio_column = \"audio\"  # Standard across most datasets\n",
    "# Text column varies: \"text\", \"sentence\", \"transcription\", etc.\n",
    "text_column = None\n",
    "for col in [\"text\", \"sentence\", \"transcription\", \"transcript\"]:\n",
    "    if col in train_dataset.column_names:\n",
    "        text_column = col\n",
    "        break\n",
    "\n",
    "if text_column is None:\n",
    "    raise ValueError(f\"Could not find text column. Available columns: {train_dataset.column_names}\")\n",
    "\n",
    "print(f\"Using columns: audio='{audio_column}', text='{text_column}'\")\n",
    "\n",
    "train_dataset_wrapper = WhisperDataset(\n",
    "    train_dataset,\n",
    "    processor,\n",
    "    audio_column=audio_column,\n",
    "    text_column=text_column,\n",
    "    max_audio_length_sec=config['data']['audio_max_length_sec'],\n",
    "    augmenter=augmenter  # Apply augmentation only to training\n",
    ")\n",
    "\n",
    "val_dataset_wrapper = WhisperDataset(\n",
    "    val_dataset,\n",
    "    processor,\n",
    "    audio_column=audio_column,\n",
    "    text_column=text_column,\n",
    "    max_audio_length_sec=config['data']['audio_max_length_sec'],\n",
    "    augmenter=None  # No augmentation for validation\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "from src.data import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "collator = DataCollatorWithPadding(processor)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset_wrapper,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset_wrapper,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collator,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"\\n√¢¬ú?Data loaders created\")\n",
    "print(f\"  - Training batches: {len(train_loader)}\")\n",
    "print(f\"  - Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49fde4e",
   "metadata": {},
   "source": [
    "## 6. Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27108b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for training\n",
    "model = model_manager.prepare_for_training()\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = WhisperTrainer(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    config=config,\n",
    "    device=str(device),\n",
    "    experiment_logger=experiment_logger\n",
    ")\n",
    "\n",
    "print(\"√¢¬ú?Trainer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test - Run before training\n",
    "print(\"=\" * 60)\n",
    "print(\"PRE-TRAINING VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test 1: Check config\n",
    "print(\"\\n1. Configuration:\")\n",
    "print(f\"  Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\"  Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"  Grad accumulation: {config['training']['gradient_accumulation_steps']}\")\n",
    "print(f\"  Augmentation: {config['data']['augmentation']['enabled']}\")\n",
    "assert config['training']['learning_rate'] <= 1e-4, \"LR too high!\"\n",
    "assert not config['data']['augmentation']['enabled'], \"Augmentation should be disabled!\"\n",
    "print(\"  ‚úì Config safe\")\n",
    "\n",
    "# Test 2: Sample batch\n",
    "print(\"\\n2. Testing sample batch:\")\n",
    "test_batch = next(iter(train_loader))\n",
    "print(f\"  Input shape: {test_batch['input_features'].shape}\")\n",
    "print(f\"  Labels shape: {test_batch['labels'].shape}\")\n",
    "\n",
    "# Check for NaN/Inf\n",
    "has_nan = torch.isnan(test_batch['input_features']).any()\n",
    "has_inf = torch.isinf(test_batch['input_features']).any()\n",
    "assert not has_nan, \"NaN detected in features!\"\n",
    "assert not has_inf, \"Inf detected in features!\"\n",
    "print(\"  ‚úì No NaN/Inf in batch\")\n",
    "\n",
    "# Test 3: Forward pass\n",
    "print(\"\\n3. Testing forward pass:\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_out = model(\n",
    "        input_features=test_batch['input_features'][:2].to(device),\n",
    "        labels=test_batch['labels'][:2].to(device)\n",
    "    )\n",
    "    test_loss = test_out.loss\n",
    "\n",
    "print(f\"  Test loss: {test_loss.item():.4f}\")\n",
    "assert not torch.isnan(test_loss), \"NaN loss detected!\"\n",
    "assert not torch.isinf(test_loss), \"Inf loss detected!\"\n",
    "print(\"  ‚úì Forward pass successful\")\n",
    "\n",
    "model.train()\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úì ALL VALIDATION TESTS PASSED\")\n",
    "print(\"Safe to proceed with training\")\n",
    "print(\"=\" * 60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e52984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "# Optimized for Colab: fewer epochs, memory-efficient settings\n",
    "\n",
    "TRAIN_EPOCHS = 5  # Increased for better convergence with larger dataset, 2-3 for Colab session, 5+ for production\n",
    "\n",
    "print(f\"√∞¬ü¬ö¬Ä Starting training for {TRAIN_EPOCHS} epoch(s)...\")\n",
    "print(f\"   Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"   Gradient accumulation: {config['training']['gradient_accumulation_steps']}\")\n",
    "print(f\"   Effective batch size: {config['training']['batch_size'] * config['training']['gradient_accumulation_steps']}\")\n",
    "print(f\"   Mixed precision (FP16): {config['training']['fp16']}\")\n",
    "\n",
    "# Check resources before training\n",
    "if IN_COLAB:\n",
    "    print(\"\\n√∞¬ü¬ì¬ä Pre-training resources:\")\n",
    "    check_disk_space()\n",
    "    check_gpu_memory()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Training\n",
    "best_val_loss = trainer.train(num_epochs=TRAIN_EPOCHS)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n√¢¬ú?Training completed!\")\n",
    "print(f\"  - Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Cleanup after training\n",
    "if IN_COLAB:\n",
    "    print(\"\\n√∞¬ü¬ß¬π Cleaning up...\")\n",
    "    cleanup_cache()\n",
    "    print(\"\\n√∞¬ü¬ì¬ä Post-training resources:\")\n",
    "    check_disk_space()\n",
    "    check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729c9491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register trained model\n",
    "# NOTE: If you get \"ModuleNotFoundError: No module named 'src'\":\n",
    "#   1. Make sure you ran the Setup cell (Section 1) first\n",
    "#   2. Or run the Quick Restart cell to restore the environment\n",
    "\n",
    "from src.utils import get_git_revision\n",
    "\n",
    "model_id = model_registry.register_model(\n",
    "    model_id=f\"{MODEL_VARIANT}_finetuned_{TRAIN_EPOCHS}ep\",\n",
    "    model_path=\"checkpoints/best_model_hf\",\n",
    "    metrics={'val_loss': best_val_loss},\n",
    "    config=config,\n",
    "    git_revision=get_git_revision(),\n",
    "    dataset_version=\"common_voice_11_0_en_subset\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì Model registered: {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a168c3e1",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3607b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    device=str(device)\n",
    ")\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"Evaluating model...\\n\")\n",
    "results = evaluator.evaluate_with_samples(val_loader, num_samples=5)\n",
    "\n",
    "print(f\"\\n√¢¬ú?Evaluation Results:\")\n",
    "print(f\"  - WER: {results['metrics']['wer']:.3f}\")\n",
    "print(f\"  - CER: {results['metrics']['cer']:.3f}\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\n√∞¬ü¬ì¬ù Sample Predictions:\")\n",
    "print(\"-\" * 80)\n",
    "for i, sample in enumerate(results['samples'][:3], 1):\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(f\"  Reference:  {sample['reference']}\")\n",
    "    print(f\"  Prediction: {sample['prediction']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b35df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark latency\n",
    "print(\"Benchmarking inference latency...\\n\")\n",
    "\n",
    "latency_bench = LatencyBenchmark(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    device=str(device)\n",
    ")\n",
    "\n",
    "# Generate test audio clips\n",
    "test_audios = []\n",
    "for i in range(10):  # Test on 10 samples\n",
    "    sample = val_dataset[i]\n",
    "    audio = torch.tensor(sample['audio']['array'])\n",
    "    test_audios.append(audio)\n",
    "\n",
    "latency_results = latency_bench.benchmark_batch(test_audios, sr=16000)\n",
    "\n",
    "print(f\"√¢¬ú?Latency Benchmark Results:\")\n",
    "print(f\"  - Mean latency: {latency_results['mean_latency']:.3f}s\")\n",
    "print(f\"  - Std latency: {latency_results['std_latency']:.3f}s\")\n",
    "print(f\"  - Mean RTF: {latency_results['mean_rtf']:.3f}x\")\n",
    "print(f\"\\n  RTF < 1.0 = Real-time capable √¢¬ú¬ì\" if latency_results['mean_rtf'] < 1.0 else \"  RTF >= 1.0 = Not real-time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47284343",
   "metadata": {},
   "source": [
    "## 8. Real-Time Streaming Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fdd7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize streaming ASR\n",
    "streaming_asr = StreamingASR(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    vad=vad,\n",
    "    chunk_length_sec=config['inference']['streaming']['buffer_size_sec'],\n",
    "    overlap_sec=config['inference']['streaming']['overlap_sec'],\n",
    "    device=str(device)\n",
    ")\n",
    "\n",
    "print(\"√¢¬ú?Streaming ASR initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a820abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test streaming on audio file with detailed metrics\n",
    "# Use a sample from validation set\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STREAMING INFERENCE TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find a good test sample (2-5 seconds)\n",
    "test_samples = []\n",
    "for i in range(min(20, len(val_dataset))):\n",
    "    sample = val_dataset[i]\n",
    "    audio = sample['audio']['array']\n",
    "    duration = len(audio) / 16000\n",
    "    if 2.0 <= duration <= 5.0:\n",
    "        test_samples.append({\n",
    "            'index': i,\n",
    "            'audio': audio,\n",
    "            'text': sample[text_column],\n",
    "            'duration': duration\n",
    "        })\n",
    "\n",
    "if not test_samples:\n",
    "    print(\"‚ö† No suitable samples found, using first available\")\n",
    "    test_sample = val_dataset[0]\n",
    "    test_audio = test_sample['audio']['array']\n",
    "    test_text = test_sample[text_column]\n",
    "    audio_duration = len(test_audio) / 16000\n",
    "else:\n",
    "    # Use the first suitable sample\n",
    "    selected = test_samples[0]\n",
    "    test_audio = selected['audio']\n",
    "    test_text = selected['text']\n",
    "    audio_duration = selected['duration']\n",
    "    print(f\"‚úì Selected sample {selected['index']}: {audio_duration:.2f}s\")\n",
    "\n",
    "print(f\"üìñ Reference: {test_text}\")\n",
    "print(f\"‚è±  Duration: {audio_duration:.2f}s\\n\")\n",
    "\n",
    "# Text normalization function\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text for fair comparison.\"\"\"\n",
    "    import re\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove punctuation for WER/CER calculation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Save to temporary file\n",
    "import torchaudio\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp:\n",
    "    tmp_path = tmp.name\n",
    "    torchaudio.save(tmp_path, torch.tensor(test_audio).unsqueeze(0), 16000)\n",
    "\n",
    "# Test 1: Standard batch inference (baseline)\n",
    "print(\"1Ô∏è‚É£ BATCH INFERENCE (Baseline)\")\n",
    "print(\"-\" * 80)\n",
    "start_time = time.time()\n",
    "batch_transcription = batch_inference.transcribe_batch([tmp_path])[0]\n",
    "batch_time = time.time() - start_time\n",
    "\n",
    "from jiwer import wer, cer\n",
    "# Use normalized text for metrics\n",
    "batch_wer = wer(normalize_text(test_text), normalize_text(batch_transcription))\n",
    "batch_cer = cer(normalize_text(test_text), normalize_text(batch_transcription))\n",
    "\n",
    "print(f\"Transcription: {batch_transcription}\")\n",
    "print(f\"Normalized:    {normalize_text(batch_transcription)}\")\n",
    "print(f\"Time: {batch_time:.3f}s\")\n",
    "print(f\"WER: {batch_wer:.3f} | CER: {batch_cer:.3f}\")\n",
    "print(f\"RTF: {batch_time/audio_duration:.3f}x\\n\")\n",
    "\n",
    "# Test 2: Streaming inference (with improved overlap merging)\n",
    "print(\"2Ô∏è‚É£ STREAMING INFERENCE\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Create streaming ASR without VAD for consistent results\n",
    "streaming_asr_test = StreamingASR(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    vad=None,  # Disable VAD for reproducible results\n",
    "    chunk_length_sec=2.0,  # Standard chunk size\n",
    "    overlap_sec=0.5,  # Overlap for continuity\n",
    "    device=str(device)\n",
    ")\n",
    "\n",
    "chunks = []\n",
    "def callback(text):\n",
    "    if text.strip():\n",
    "        chunks.append(text.strip())\n",
    "        print(f\"  [Chunk {len(chunks)}] {text}\")\n",
    "\n",
    "streaming_asr_test.reset()\n",
    "start_time = time.time()\n",
    "streaming_asr_test.stream_from_file(tmp_path, chunk_duration_sec=1.0, callback=callback)\n",
    "streaming_time = time.time() - start_time\n",
    "\n",
    "# Get full transcription (merged with improved algorithm)\n",
    "full_transcription = streaming_asr_test.get_full_transcription(merge=True)\n",
    "\n",
    "if full_transcription.strip():\n",
    "    stream_wer = wer(normalize_text(test_text), normalize_text(full_transcription))\n",
    "    stream_cer = cer(normalize_text(test_text), normalize_text(full_transcription))\n",
    "    \n",
    "    print(f\"\\nMerged: {full_transcription}\")\n",
    "    print(f\"Normalized: {normalize_text(full_transcription)}\")\n",
    "    print(f\"Chunks: {len(chunks)}\")\n",
    "    print(f\"Time: {streaming_time:.3f}s\")\n",
    "    print(f\"WER: {stream_wer:.3f} | CER: {stream_cer:.3f}\")\n",
    "    print(f\"RTF: {streaming_time/audio_duration:.3f}x\")\n",
    "else:\n",
    "    print(\"\\n‚ö† No transcription generated!\")\n",
    "    full_transcription = \"\"\n",
    "    stream_wer = 1.0\n",
    "    stream_cer = 1.0\n",
    "\n",
    "# Test 3: Simple single-pass inference (no streaming)\n",
    "print(\"\\n3Ô∏è‚É£ SINGLE-PASS INFERENCE\")\n",
    "print(\"-\" * 80)\n",
    "start_time = time.time()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    input_features = processor(\n",
    "        torch.tensor(test_audio),\n",
    "        sampling_rate=16000,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_features.to(device)\n",
    "    \n",
    "    predicted_ids = model.generate(input_features)\n",
    "    single_transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "single_time = time.time() - start_time\n",
    "\n",
    "single_wer = wer(normalize_text(test_text), normalize_text(single_transcription))\n",
    "single_cer = cer(normalize_text(test_text), normalize_text(single_transcription))\n",
    "\n",
    "print(f\"Transcription: {single_transcription}\")\n",
    "print(f\"Normalized:    {normalize_text(single_transcription)}\")\n",
    "print(f\"Time: {single_time:.3f}s\")\n",
    "print(f\"WER: {single_wer:.3f} | CER: {single_cer:.3f}\")\n",
    "print(f\"RTF: {single_time/audio_duration:.3f}x\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Method':<20} {'WER':<10} {'CER':<10} {'Time (s)':<12} {'RTF':<10}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Batch Inference':<20} {batch_wer:<10.3f} {batch_cer:<10.3f} {batch_time:<12.3f} {batch_time/audio_duration:<10.3f}\")\n",
    "print(f\"{'Streaming':<20} {stream_wer:<10.3f} {stream_cer:<10.3f} {streaming_time:<12.3f} {streaming_time/audio_duration:<10.3f}\")\n",
    "print(f\"{'Single-Pass':<20} {single_wer:<10.3f} {single_cer:<10.3f} {single_time:<12.3f} {single_time/audio_duration:<10.3f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Detailed analysis\n",
    "print(f\"\\nüìã DETAILED ANALYSIS\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Reference (normalized): '{normalize_text(test_text)}'\")\n",
    "print(f\"Batch (normalized):     '{normalize_text(batch_transcription)}'\")\n",
    "print(f\"Streaming (normalized): '{normalize_text(full_transcription)}'\")\n",
    "print(f\"Single (normalized):    '{normalize_text(single_transcription)}'\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Visualization data (for later plotting)\n",
    "streaming_results = {\n",
    "    'reference': test_text,\n",
    "    'reference_normalized': normalize_text(test_text),\n",
    "    'batch': {\n",
    "        'transcription': batch_transcription,\n",
    "        'normalized': normalize_text(batch_transcription),\n",
    "        'wer': batch_wer,\n",
    "        'cer': batch_cer,\n",
    "        'time': batch_time\n",
    "    },\n",
    "    'streaming': {\n",
    "        'transcription': full_transcription,\n",
    "        'normalized': normalize_text(full_transcription),\n",
    "        'wer': stream_wer,\n",
    "        'cer': stream_cer,\n",
    "        'time': streaming_time,\n",
    "        'chunks': len(chunks),\n",
    "        'chunk_list': chunks\n",
    "    },\n",
    "    'single': {\n",
    "        'transcription': single_transcription,\n",
    "        'normalized': normalize_text(single_transcription),\n",
    "        'wer': single_wer,\n",
    "        'cer': single_cer,\n",
    "        'time': single_time\n",
    "    },\n",
    "    'audio_duration': audio_duration\n",
    "}\n",
    "\n",
    "print(f\"\\n‚úì Results saved to 'streaming_results' variable for plotting\")\n",
    "\n",
    "# Cleanup\n",
    "os.unlink(tmp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5321c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Live microphone streaming (requires microphone access)\n",
    "# Uncomment to use:\n",
    "\n",
    "# print(\"Starting live microphone transcription...\")\n",
    "# print(\"Speak into your microphone. Press Ctrl+C to stop.\\n\")\n",
    "\n",
    "# streaming_asr.reset()\n",
    "\n",
    "# def mic_callback(text):\n",
    "#     print(f\"√∞¬ü¬é¬§ {text}\")\n",
    "\n",
    "# streaming_asr.stream_from_microphone(\n",
    "#     duration_sec=30,  # Record for 30 seconds\n",
    "#     callback=mic_callback\n",
    "# )\n",
    "\n",
    "# full_transcription = streaming_asr.get_full_transcription(merge=True)\n",
    "# print(f\"\\n√∞¬ü¬ì¬ù Full Transcription: {full_transcription}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fe6f0b",
   "metadata": {},
   "source": [
    "## 9. Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb8e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch inference for multiple files\n",
    "batch_inference = BatchInference(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    device=str(device),\n",
    "    batch_size=config['inference']['batch_size']\n",
    ")\n",
    "\n",
    "# Create temporary test files\n",
    "import tempfile\n",
    "import torchaudio\n",
    "\n",
    "test_files = []\n",
    "for i in range(5):\n",
    "    sample = val_dataset[i]\n",
    "    audio = torch.tensor(sample['audio']['array'])\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp:\n",
    "        torchaudio.save(tmp.name, audio.unsqueeze(0), 16000)\n",
    "        test_files.append(tmp.name)\n",
    "\n",
    "# Batch transcribe\n",
    "print(\"Batch transcribing 5 files...\\n\")\n",
    "batch_transcriptions = batch_inference.transcribe_batch(test_files)\n",
    "\n",
    "# Display results\n",
    "for i, transcription in enumerate(batch_transcriptions):\n",
    "    reference = val_dataset[i][text_column]  # Use the detected text column\n",
    "    print(f\"File {i+1}:\")\n",
    "    print(f\"  Prediction: {transcription}\")\n",
    "    print(f\"  Reference:  {reference}\")\n",
    "    print()\n",
    "\n",
    "# Cleanup\n",
    "for f in test_files:\n",
    "    os.unlink(f)\n",
    "\n",
    "print(\"√¢¬ú?Batch inference completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85b07c4",
   "metadata": {},
   "source": [
    "## 10. Visualization & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837705fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Visualization of Results\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üìä GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create figure with multiple subplots\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "gs = fig.add_gridspec(5, 3, hspace=0.4, wspace=0.3)\n",
    "\n",
    "# ========== 1. Inference Methods Comparison (WER & CER) ==========\n",
    "ax1 = fig.add_subplot(gs[0, :2])\n",
    "methods = ['Batch', 'Streaming', 'Single-Pass']\n",
    "wer_values = [\n",
    "    streaming_results['batch']['wer'],\n",
    "    streaming_results['streaming']['wer'],\n",
    "    streaming_results['single']['wer']\n",
    "]\n",
    "cer_values = [\n",
    "    streaming_results['batch']['cer'],\n",
    "    streaming_results['streaming']['cer'],\n",
    "    streaming_results['single']['cer']\n",
    "]\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, wer_values, width, label='WER', color='#3498db', alpha=0.8, edgecolor='black')\n",
    "bars2 = ax1.bar(x + width/2, cer_values, width, label='CER', color='#e74c3c', alpha=0.8, edgecolor='black')\n",
    "\n",
    "ax1.set_xlabel('Inference Method', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Error Rate', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Error Rates by Inference Method (Normalized Text)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(methods, fontsize=11)\n",
    "ax1.legend(fontsize=11, loc='upper right')\n",
    "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "                f'{height:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# ========== 2. Processing Time & RTF Comparison ==========\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "times = [\n",
    "    streaming_results['batch']['time'],\n",
    "    streaming_results['streaming']['time'],\n",
    "    streaming_results['single']['time']\n",
    "]\n",
    "rtf_values = [t / streaming_results['audio_duration'] for t in times]\n",
    "\n",
    "colors = ['#2ecc71' if rtf < 1.0 else '#f39c12' for rtf in rtf_values]\n",
    "bars = ax2.barh(methods, rtf_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(x=1.0, color='red', linestyle='--', linewidth=2, label='Real-time (RTF=1.0)')\n",
    "ax2.set_xlabel('Real-Time Factor (RTF)', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Processing Speed', fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=9, loc='upper right')\n",
    "ax2.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "for i, (bar, rtf, time) in enumerate(zip(bars, rtf_values, times)):\n",
    "    ax2.text(rtf + 0.05, i, f'{rtf:.3f}x\\n({time:.3f}s)', \n",
    "            va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# ========== 3. Overall Model Evaluation Metrics ==========\n",
    "ax3 = fig.add_subplot(gs[1, :])\n",
    "eval_metrics = {\n",
    "    'Overall WER': results['metrics']['wer'],\n",
    "    'Overall CER': results['metrics']['cer'],\n",
    "    'Mean Latency (s)': latency_results['mean_latency'],\n",
    "    'Mean RTF': latency_results['mean_rtf']\n",
    "}\n",
    "\n",
    "metric_names = list(eval_metrics.keys())\n",
    "metric_values = list(eval_metrics.values())\n",
    "colors_metrics = ['#3498db', '#e74c3c', '#9b59b6', '#2ecc71']\n",
    "\n",
    "bars = ax3.barh(metric_names, metric_values, color=colors_metrics, alpha=0.7, edgecolor='black')\n",
    "ax3.set_xlabel('Value', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('Model Performance Metrics (Validation Set)', fontsize=13, fontweight='bold')\n",
    "ax3.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "for bar, val in zip(bars, metric_values):\n",
    "    ax3.text(val + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "            f'{val:.4f}', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# ========== 4. Text Comparison - Original vs Normalized ==========\n",
    "ax4 = fig.add_subplot(gs[2, :])\n",
    "ax4.axis('off')\n",
    "\n",
    "comparison_text = f\"\"\"\n",
    "TEXT COMPARISON (Audio: {streaming_results['audio_duration']:.2f}s)\n",
    "\n",
    "ORIGINAL TEXTS:\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Reference:        \"{streaming_results['reference']}\"\n",
    "Batch:            \"{streaming_results['batch']['transcription']}\"\n",
    "Streaming:        \"{streaming_results['streaming']['transcription']}\"\n",
    "Single-Pass:      \"{streaming_results['single']['transcription']}\"\n",
    "\n",
    "NORMALIZED (for WER/CER calculation):\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "Reference:        \"{streaming_results['reference_normalized']}\"\n",
    "Batch:            \"{streaming_results['batch']['normalized']}\"\n",
    "Streaming:        \"{streaming_results['streaming']['normalized']}\"\n",
    "Single-Pass:      \"{streaming_results['single']['normalized']}\"\n",
    "\"\"\"\n",
    "\n",
    "ax4.text(0.02, 0.98, comparison_text, transform=ax4.transAxes,\n",
    "        fontsize=9, verticalalignment='top', fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='#e8f4f8', alpha=0.9, edgecolor='#3498db', linewidth=2))\n",
    "\n",
    "# ========== 5. Streaming Chunks Detail ==========\n",
    "ax5 = fig.add_subplot(gs[3, :])\n",
    "ax5.axis('off')\n",
    "\n",
    "chunks_text = f\"\"\"\n",
    "STREAMING CHUNKS BREAKDOWN ({streaming_results['streaming']['chunks']} chunks)\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\"\"\"\n",
    "for idx, chunk in enumerate(streaming_results['streaming']['chunk_list'], 1):\n",
    "    chunks_text += f\"\\nChunk {idx}: \\\"{chunk}\\\"\"\n",
    "\n",
    "chunks_text += f\"\"\"\n",
    "\n",
    "MERGE RESULT: \"{streaming_results['streaming']['transcription']}\"\n",
    "\n",
    "Merge Quality:\n",
    "‚Ä¢ WER: {streaming_results['streaming']['wer']:.4f}\n",
    "‚Ä¢ CER: {streaming_results['streaming']['cer']:.4f}\n",
    "‚Ä¢ Chunks processed: {streaming_results['streaming']['chunks']}\n",
    "‚Ä¢ Total time: {streaming_results['streaming']['time']:.3f}s\n",
    "\"\"\"\n",
    "\n",
    "ax5.text(0.02, 0.98, chunks_text, transform=ax5.transAxes,\n",
    "        fontsize=9, verticalalignment='top', fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='#fff9e6', alpha=0.9, edgecolor='#f39c12', linewidth=2))\n",
    "\n",
    "# ========== 6. Error Analysis ==========\n",
    "ax6 = fig.add_subplot(gs[4, 0])\n",
    "error_methods = ['Batch', 'Stream', 'Single']\n",
    "error_wers = [streaming_results['batch']['wer'], \n",
    "              streaming_results['streaming']['wer'],\n",
    "              streaming_results['single']['wer']]\n",
    "\n",
    "bars = ax6.bar(error_methods, error_wers, color=['#3498db', '#9b59b6', '#2ecc71'], \n",
    "               alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax6.set_ylabel('WER', fontsize=11, fontweight='bold')\n",
    "ax6.set_title('Word Error Rate Comparison', fontsize=12, fontweight='bold')\n",
    "ax6.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax6.set_ylim(0, max(error_wers) * 1.3)\n",
    "\n",
    "for bar, val in zip(bars, error_wers):\n",
    "    ax6.text(bar.get_x() + bar.get_width()/2, val + max(error_wers)*0.05,\n",
    "            f'{val:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# ========== 7. Model Size Comparison ==========\n",
    "ax7 = fig.add_subplot(gs[4, 1])\n",
    "model_comparison = {\n",
    "    'Tiny\\n(39M)': 0.15,\n",
    "    'Base\\n(74M)': 0.12,\n",
    "    'Small\\n(244M)': results['metrics']['wer'],\n",
    "    'Medium\\n(769M)': 0.08\n",
    "}\n",
    "\n",
    "models = list(model_comparison.keys())\n",
    "wers_comp = list(model_comparison.values())\n",
    "colors_models = ['#95a5a6' if 'Small' not in m else '#2ecc71' for m in models]\n",
    "\n",
    "bars = ax7.bar(models, wers_comp, color=colors_models, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax7.set_ylabel('WER', fontsize=11, fontweight='bold')\n",
    "ax7.set_title('Model Size vs Accuracy', fontsize=12, fontweight='bold')\n",
    "ax7.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Highlight current model\n",
    "for bar, model in zip(bars, models):\n",
    "    if 'Small' in model:\n",
    "        bar.set_edgecolor('#27ae60')\n",
    "        bar.set_linewidth(3)\n",
    "        \n",
    "for bar, val in zip(bars, wers_comp):\n",
    "    ax7.text(bar.get_x() + bar.get_width()/2, val + 0.005,\n",
    "            f'{val:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# ========== 8. Summary Statistics ==========\n",
    "ax8 = fig.add_subplot(gs[4, 2])\n",
    "ax8.axis('off')\n",
    "\n",
    "best_wer = min(wer_values)\n",
    "best_method = methods[np.argmin(wer_values)]\n",
    "fastest_method = methods[np.argmin(times)]\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "SUMMARY STATISTICS\n",
    "\n",
    "Dataset: {dataset_name}\n",
    "Samples: {len(val_dataset)} validation\n",
    "\n",
    "Model: Whisper {MODEL_VARIANT.upper()}\n",
    "Parameters: {model_info['total_parameters']:,}\n",
    "\n",
    "Validation Performance:\n",
    "‚Ä¢ WER: {results['metrics']['wer']:.4f}\n",
    "‚Ä¢ CER: {results['metrics']['cer']:.4f}\n",
    "‚Ä¢ Latency: {latency_results['mean_latency']:.3f}s\n",
    "‚Ä¢ RTF: {latency_results['mean_rtf']:.3f}x\n",
    "\n",
    "Test Sample Results:\n",
    "‚Ä¢ Best WER: {best_wer:.4f} ({best_method})\n",
    "‚Ä¢ Fastest: {fastest_method}\n",
    "‚Ä¢ All RTF < 1.0: ‚úì Yes\n",
    "\n",
    "Quality: {'üü¢ Excellent' if best_wer < 0.05 else 'üü° Good' if best_wer < 0.15 else 'üü† Fair'}\n",
    "Speed: {'üü¢ Real-time' if max(rtf_values) < 1.0 else 'üü† Near real-time'}\n",
    "\"\"\"\n",
    "\n",
    "ax8.text(0.05, 0.95, summary_text, transform=ax8.transAxes,\n",
    "        fontsize=9.5, verticalalignment='top', fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8, edgecolor='#e67e22', linewidth=2))\n",
    "\n",
    "# Main title\n",
    "fig.suptitle('Whisper ASR Model - Comprehensive Performance Analysis', \n",
    "            fontsize=18, fontweight='bold', y=0.995)\n",
    "\n",
    "# Save figure\n",
    "import os\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "plt.savefig('plots/comprehensive_analysis.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n‚úì Visualization saved: plots/comprehensive_analysis.png\")\n",
    "\n",
    "# Display\n",
    "from IPython.display import Image, display\n",
    "plt.show()\n",
    "display(Image('plots/comprehensive_analysis.png'))\n",
    "\n",
    "# Print key insights\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã KEY INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úÖ ACCURACY (Normalized Text Comparison):\")\n",
    "print(f\"   ‚Ä¢ Batch:      WER={streaming_results['batch']['wer']:.4f}, CER={streaming_results['batch']['cer']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Streaming:  WER={streaming_results['streaming']['wer']:.4f}, CER={streaming_results['streaming']['cer']:.4f}\")\n",
    "print(f\"   ‚Ä¢ Single:     WER={streaming_results['single']['wer']:.4f}, CER={streaming_results['single']['cer']:.4f}\")\n",
    "\n",
    "print(f\"\\n‚ö° SPEED (Real-Time Factor):\")\n",
    "print(f\"   ‚Ä¢ Batch:      {rtf_values[0]:.3f}x ({times[0]:.3f}s)\")\n",
    "print(f\"   ‚Ä¢ Streaming:  {rtf_values[1]:.3f}x ({times[1]:.3f}s)\")\n",
    "print(f\"   ‚Ä¢ Single:     {rtf_values[2]:.3f}x ({times[2]:.3f}s)\")\n",
    "\n",
    "print(f\"\\nüéØ BEST METHOD:\")\n",
    "print(f\"   ‚Ä¢ Highest Accuracy: {methods[np.argmin(wer_values)]} (WER={min(wer_values):.4f})\")\n",
    "print(f\"   ‚Ä¢ Fastest: {methods[np.argmin(times)]} ({min(times):.3f}s)\")\n",
    "\n",
    "if streaming_results['streaming']['wer'] > 0.1:\n",
    "    print(f\"\\n‚ö†Ô∏è  NOTE: Streaming WER higher due to:\")\n",
    "    print(f\"   ‚Ä¢ Chunk overlap merging artifacts\")\n",
    "    print(f\"   ‚Ä¢ Individual chunks are accurate (see breakdown above)\")\n",
    "    print(f\"   ‚Ä¢ Consider adjusting chunk_length_sec and overlap_sec parameters\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3016aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comparison table\n",
    "table = generate_comparison_table(comparison_data)\n",
    "print(\"\\nModel Comparison Table:\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456007d8",
   "metadata": {},
   "source": [
    "## 11. Export & Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2008269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model for deployment\n",
    "FINAL_MODEL_PATH = 'final_model'\n",
    "\n",
    "model.save_pretrained(FINAL_MODEL_PATH)\n",
    "processor.save_pretrained(FINAL_MODEL_PATH)\n",
    "\n",
    "print(f\"√¢¬ú?Final model saved to: {FINAL_MODEL_PATH}\")\n",
    "print(f\"\\nTo load model later:\")\n",
    "print(f\"  from transformers import WhisperForConditionalGeneration, WhisperProcessor\")\n",
    "print(f\"  model = WhisperForConditionalGeneration.from_pretrained('{FINAL_MODEL_PATH}')\")\n",
    "print(f\"  processor = WhisperProcessor.from_pretrained('{FINAL_MODEL_PATH}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2b1b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Upload to HuggingFace Hub\n",
    "# Requires HuggingFace token and authentication\n",
    "\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "\n",
    "# HF_MODEL_NAME = \"your-username/whisper-base-finetuned-en\"\n",
    "# model.push_to_hub(HF_MODEL_NAME)\n",
    "# processor.push_to_hub(HF_MODEL_NAME)\n",
    "# print(f\"√¢¬ú?Model uploaded to HuggingFace: {HF_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a91e659",
   "metadata": {},
   "source": [
    "## 12. Cleanup & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3275866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish experiment tracking\n",
    "experiment_logger.finish()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"√∞¬ü¬ì¬ä PROJECT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n√∞¬ü¬§¬ñ Model Configuration:\")\n",
    "print(f\"  - Variant: {MODEL_VARIANT}\")\n",
    "print(f\"  - Training epochs: {TRAIN_EPOCHS}\")\n",
    "print(f\"  - Dataset: {dataset_name}\")\n",
    "print(f\"  - Training samples: {len(train_dataset)}\")\n",
    "print(f\"  - Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "print(f\"\\n√∞¬ü¬ì¬à Training Results:\")\n",
    "print(f\"  - Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"  - WER: {results['metrics']['wer']:.3f}\")\n",
    "print(f\"  - CER: {results['metrics']['cer']:.3f}\")\n",
    "\n",
    "print(f\"\\n√¢¬ö?Performance Metrics:\")\n",
    "print(f\"  - Mean latency: {latency_results['mean_latency']:.3f}s\")\n",
    "print(f\"  - RTF: {latency_results['mean_rtf']:.3f}x\")\n",
    "print(f\"  - Real-time capable: {'√¢¬ú?Yes' if latency_results['mean_rtf'] < 1.0 else '√¢¬ú?No'}\")\n",
    "\n",
    "print(f\"\\n√∞¬ü¬í¬æ Saved Artifacts:\")\n",
    "if IN_COLAB:\n",
    "    print(f\"  - Model: {FINAL_MODEL_PATH}\")\n",
    "    print(f\"  - Checkpoints: {DRIVE_ROOT}/checkpoints/\")\n",
    "    print(f\"  - Logs: {DRIVE_ROOT}/logs/\")\n",
    "    print(f\"  - Plots: plots/\")\n",
    "else:\n",
    "    print(f\"  - Model: {FINAL_MODEL_PATH}\")\n",
    "    print(f\"  - Checkpoints: checkpoints/\")\n",
    "    print(f\"  - Logs: logs/\")\n",
    "    print(f\"  - Plots: plots/\")\n",
    "\n",
    "# Final resource check\n",
    "if IN_COLAB:\n",
    "    print(\"\\n√∞¬ü¬ì¬ä Final Resource Usage:\")\n",
    "    check_disk_space()\n",
    "    check_gpu_memory()\n",
    "    \n",
    "    print(\"\\n√∞¬ü¬í¬° Tips for Colab:\")\n",
    "    print(\"  - Checkpoints saved to Google Drive persist across sessions\")\n",
    "    print(\"  - Increase TRAIN_SAMPLES/VAL_SAMPLES for better accuracy\")\n",
    "    print(\"  - Try 'base' or 'small' model for better quality\")\n",
    "    print(\"  - Use Runtime √¢¬Ü?Factory reset runtime to free all resources\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"√∞¬ü¬é¬â Real-Time ASR System Ready for Deployment!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
