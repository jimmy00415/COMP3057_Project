{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "995862c2",
   "metadata": {},
   "source": [
    "# Real-Time Multilingual ASR with Whisper\n",
    "\n",
    "This notebook implements a production-ready, real-time speech recognition system using OpenAI's Whisper models.\n",
    "\n",
    "**Features:**\n",
    "- Data preparation with augmentation\n",
    "- Model fine-tuning with multiple variants\n",
    "- Comprehensive evaluation (WER, CER, latency)\n",
    "- Real-time streaming inference\n",
    "- Full MLOps best practices (versioning, logging, reproducibility)\n",
    "\n",
    "**Author:** COMP3057 Project  \n",
    "**Version:** 1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c25953",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f24b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment and clone project\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if running in Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üîß Running in Google Colab\")\n",
    "    \n",
    "    # Mount Google Drive for saving checkpoints and logs\n",
    "    from google.colab import drive\n",
    "    if not os.path.exists('/content/drive'):\n",
    "        print(\"üìÅ Mounting Google Drive...\")\n",
    "        drive.mount('/content/drive', force_remount=False)\n",
    "        print(\"‚úì Drive mounted at /content/drive\")\n",
    "    else:\n",
    "        print(\"‚úì Drive already mounted\")\n",
    "    \n",
    "    # Clone repository if not exists\n",
    "    if not os.path.exists('COMP3057_Project'):\n",
    "        print(\"\\nüì¶ Cloning repository from GitHub...\")\n",
    "        !git clone https://github.com/jimmy00415/COMP3057_Project.git\n",
    "        print(\"‚úì Repository cloned\")\n",
    "    else:\n",
    "        print(\"‚úì Repository already exists\")\n",
    "    \n",
    "    # Change to project directory\n",
    "    os.chdir('COMP3057_Project')\n",
    "    print(f\"‚úì Working directory: {os.getcwd()}\")\n",
    "else:\n",
    "    print(\"üíª Running locally\")\n",
    "    print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "gpu_available = torch.cuda.is_available()\n",
    "if gpu_available:\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"\\nüéÆ GPU: {gpu_name}\")\n",
    "    print(f\"   Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: No GPU detected! Training will be very slow.\")\n",
    "    print(\"   Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU (T4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56250e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# Install additional Colab-specific packages\n",
    "if IN_COLAB:\n",
    "    !pip install -q sounddevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce2e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to Python path\n",
    "project_root = os.getcwd()\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    print(f\"Added '{project_root}' to Python path\")\n",
    "    # print(\"\\nUpdated sys.path:\")\n",
    "    # for p in sys.path[:5]:\n",
    "    #     print(f\"  - {p}\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Import project modules\n",
    "from src.utils import (\n",
    "    load_config,\n",
    "    set_seed,\n",
    "    setup_logging,\n",
    "    get_device,\n",
    "    ExperimentLogger,\n",
    "    DataVersionManager,\n",
    "    ModelRegistry\n",
    ")\n",
    "\n",
    "from src.data import (\n",
    "    AudioPreprocessor,\n",
    "    VoiceActivityDetector,\n",
    "    AudioAugmenter,\n",
    "    WhisperDataset,\n",
    "    prepare_datasets,\n",
    "    create_dataloaders\n",
    ")\n",
    "\n",
    "from src.models import (\n",
    "    WhisperModelManager,\n",
    "    compare_models\n",
    ")\n",
    "\n",
    "from src.training import WhisperTrainer\n",
    "\n",
    "from src.evaluation import (\n",
    "    ModelEvaluator,\n",
    "    LatencyBenchmark,\n",
    "    TrainingVisualizer,\n",
    "    EvaluationVisualizer,\n",
    "    generate_comparison_table\n",
    ")\n",
    "\n",
    "from src.inference import (\n",
    "    StreamingASR,\n",
    "    BatchInference\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172c29b4",
   "metadata": {},
   "source": [
    "## 2. Configuration & Reproducibility Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55abd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config('config.yaml')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "set_seed(config['project']['seed'])\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_logging('INFO', 'logs/training.log')\n",
    "\n",
    "# Get device\n",
    "device = get_device(config['project']['device'])\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize experiment tracking (choose: wandb, mlflow, or tensorboard)\n",
    "experiment_logger = ExperimentLogger(\n",
    "    backend=config['mlops']['experiment_tracking']['backend'],\n",
    "    project_name=config['mlops']['experiment_tracking']['project_name'],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Initialize versioning\n",
    "data_version_manager = DataVersionManager(\n",
    "    config['mlops']['versioning']['data_version_file']\n",
    ")\n",
    "model_registry = ModelRegistry(\n",
    "    config['mlops']['versioning']['model_registry']\n",
    ")\n",
    "\n",
    "print(f\"‚úì Configuration loaded\")\n",
    "print(f\"  - Seed: {config['project']['seed']}\")\n",
    "print(f\"  - Device: {device}\")\n",
    "print(f\"  - Tracking: {config['mlops']['experiment_tracking']['backend']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f62e685",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6286d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing utilities\n",
    "audio_preprocessor = AudioPreprocessor(\n",
    "    target_sr=config['data']['sampling_rate'],\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "vad = VoiceActivityDetector(\n",
    "    threshold=config['data']['vad_threshold']\n",
    ")\n",
    "\n",
    "augmenter = AudioAugmenter(\n",
    "    speed_perturbation=config['data']['augmentation']['speed_perturbation'],\n",
    "    pitch_shift_semitones=config['data']['augmentation']['pitch_shift_semitones'],\n",
    "    background_noise_prob=config['data']['augmentation']['background_noise_prob']\n",
    ") if config['data']['augmentation']['enabled'] else None\n",
    "\n",
    "print(\"‚úì Preprocessing utilities initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1acff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "# Note: This uses a simplified version with Common Voice\n",
    "# For full implementation, add People's Speech and SpeechOcean762\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load Common Voice (small subset for demo)\n",
    "print(\"Loading Common Voice dataset...\")\n",
    "dataset = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_11_0\",\n",
    "    \"en\",\n",
    "    split=\"train[:1000]+validation[:200]\",  # Small subset for demo\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Split into train/val\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=config['project']['seed'])\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['test']\n",
    "\n",
    "print(f\"‚úì Dataset loaded\")\n",
    "print(f\"  - Training samples: {len(train_dataset)}\")\n",
    "print(f\"  - Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Log dataset version\n",
    "data_version_manager.log_dataset_version(\n",
    "    dataset_name=\"common_voice_11_0\",\n",
    "    version=\"en_subset_1200\",\n",
    "    metadata={'train': len(train_dataset), 'val': len(val_dataset)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc7d1fc",
   "metadata": {},
   "source": [
    "## 4. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e251a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model manager\n",
    "model_manager = WhisperModelManager(config)\n",
    "\n",
    "# Choose model variant: tiny, base, small, medium, distil\n",
    "MODEL_VARIANT = 'base'  # Change to 'tiny' for faster training, 'small' for better accuracy\n",
    "\n",
    "# Load model and processor\n",
    "model, processor = model_manager.initialize_model(\n",
    "    variant=MODEL_VARIANT,\n",
    "    device=str(device)\n",
    ")\n",
    "\n",
    "# Get model info\n",
    "model_info = model_manager.get_model_info()\n",
    "print(f\"\\n‚úì Model initialized: {MODEL_VARIANT}\")\n",
    "print(f\"  - Total parameters: {model_info['total_parameters']:,}\")\n",
    "print(f\"  - Trainable parameters: {model_info['trainable_parameters']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a09c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model variants\n",
    "variants_info = compare_models(config)\n",
    "\n",
    "print(\"\\nWhisper Model Variants Comparison:\")\n",
    "print(\"-\" * 60)\n",
    "for variant, info in variants_info.items():\n",
    "    print(f\"{variant:10s} | Params: {info['params']:8s} | Speed: {info['speed']:10s} | Accuracy: {info['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c80b81f",
   "metadata": {},
   "source": [
    "## 5. Prepare Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04885ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets\n",
    "train_dataset_wrapper = WhisperDataset(\n",
    "    train_dataset,\n",
    "    processor,\n",
    "    audio_column=\"audio\",\n",
    "    text_column=\"sentence\",\n",
    "    max_audio_length_sec=config['data']['audio_max_length_sec'],\n",
    "    augmenter=augmenter  # Apply augmentation only to training\n",
    ")\n",
    "\n",
    "val_dataset_wrapper = WhisperDataset(\n",
    "    val_dataset,\n",
    "    processor,\n",
    "    audio_column=\"audio\",\n",
    "    text_column=\"sentence\",\n",
    "    max_audio_length_sec=config['data']['audio_max_length_sec'],\n",
    "    augmenter=None  # No augmentation for validation\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "from src.data import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "collator = DataCollatorWithPadding(processor)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset_wrapper,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset_wrapper,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collator,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"‚úì Data loaders created\")\n",
    "print(f\"  - Training batches: {len(train_loader)}\")\n",
    "print(f\"  - Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49fde4e",
   "metadata": {},
   "source": [
    "## 6. Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27108b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for training\n",
    "model = model_manager.prepare_for_training()\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = WhisperTrainer(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    config=config,\n",
    "    device=str(device),\n",
    "    experiment_logger=experiment_logger\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e52984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "# Adjust num_epochs for quick testing (e.g., 1-2 epochs)\n",
    "# For production, use config['training']['num_epochs'] (10 epochs)\n",
    "\n",
    "TRAIN_EPOCHS = 2  # Set to 1-2 for quick demo, 10+ for production\n",
    "\n",
    "print(f\"Starting training for {TRAIN_EPOCHS} epochs...\\n\")\n",
    "\n",
    "best_val_loss = trainer.train(num_epochs=TRAIN_EPOCHS)\n",
    "\n",
    "print(f\"\\n‚úì Training completed!\")\n",
    "print(f\"  - Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729c9491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register trained model\n",
    "from src.utils import get_git_revision\n",
    "\n",
    "model_id = model_registry.register_model(\n",
    "    model_id=f\"{MODEL_VARIANT}_finetuned_{TRAIN_EPOCHS}ep\",\n",
    "    model_path=\"checkpoints/best_model_hf\",\n",
    "    metrics={'val_loss': best_val_loss},\n",
    "    config=config,\n",
    "    git_revision=get_git_revision(),\n",
    "    dataset_version=\"common_voice_11_0_en_subset\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì Model registered: {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a168c3e1",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3607b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    device=str(device)\n",
    ")\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"Evaluating model...\\n\")\n",
    "results = evaluator.evaluate_with_samples(val_loader, num_samples=5)\n",
    "\n",
    "print(f\"\\n‚úì Evaluation Results:\")\n",
    "print(f\"  - WER: {results['metrics']['wer']:.3f}\")\n",
    "print(f\"  - CER: {results['metrics']['cer']:.3f}\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\nüìù Sample Predictions:\")\n",
    "print(\"-\" * 80)\n",
    "for i, sample in enumerate(results['samples'][:3], 1):\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(f\"  Reference:  {sample['reference']}\")\n",
    "    print(f\"  Prediction: {sample['prediction']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b35df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark latency\n",
    "print(\"Benchmarking inference latency...\\n\")\n",
    "\n",
    "latency_bench = LatencyBenchmark(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    device=str(device)\n",
    ")\n",
    "\n",
    "# Generate test audio clips\n",
    "test_audios = []\n",
    "for i in range(10):  # Test on 10 samples\n",
    "    sample = val_dataset[i]\n",
    "    audio = torch.tensor(sample['audio']['array'])\n",
    "    test_audios.append(audio)\n",
    "\n",
    "latency_results = latency_bench.benchmark_batch(test_audios, sr=16000)\n",
    "\n",
    "print(f\"‚úì Latency Benchmark Results:\")\n",
    "print(f\"  - Mean latency: {latency_results['mean_latency']:.3f}s\")\n",
    "print(f\"  - Std latency: {latency_results['std_latency']:.3f}s\")\n",
    "print(f\"  - Mean RTF: {latency_results['mean_rtf']:.3f}x\")\n",
    "print(f\"\\n  RTF < 1.0 = Real-time capable ‚úì\" if latency_results['mean_rtf'] < 1.0 else \"  RTF >= 1.0 = Not real-time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47284343",
   "metadata": {},
   "source": [
    "## 8. Real-Time Streaming Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fdd7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize streaming ASR\n",
    "streaming_asr = StreamingASR(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    vad=vad,\n",
    "    chunk_length_sec=config['inference']['streaming']['buffer_size_sec'],\n",
    "    overlap_sec=config['inference']['streaming']['overlap_sec'],\n",
    "    device=str(device)\n",
    ")\n",
    "\n",
    "print(\"‚úì Streaming ASR initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a820abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test streaming on audio file\n",
    "# Use a sample from validation set\n",
    "\n",
    "# Get a test audio file\n",
    "test_sample = val_dataset[0]\n",
    "test_audio = test_sample['audio']['array']\n",
    "test_text = test_sample['sentence']\n",
    "\n",
    "# Save to temporary file\n",
    "import torchaudio\n",
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp:\n",
    "    tmp_path = tmp.name\n",
    "    torchaudio.save(tmp_path, torch.tensor(test_audio).unsqueeze(0), 16000)\n",
    "\n",
    "print(f\"Test audio saved to: {tmp_path}\")\n",
    "print(f\"Reference text: {test_text}\\n\")\n",
    "\n",
    "# Stream from file\n",
    "print(\"Streaming transcription:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "transcriptions = []\n",
    "def callback(text):\n",
    "    print(f\"[CHUNK] {text}\")\n",
    "    transcriptions.append(text)\n",
    "\n",
    "streaming_asr.reset()\n",
    "streaming_asr.stream_from_file(tmp_path, chunk_duration_sec=0.5, callback=callback)\n",
    "\n",
    "# Get full transcription\n",
    "full_transcription = streaming_asr.get_full_transcription(merge=True)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(f\"\\nüìù Final Transcription: {full_transcription}\")\n",
    "print(f\"üìñ Reference:          {test_text}\")\n",
    "\n",
    "# Cleanup\n",
    "os.unlink(tmp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5321c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Live microphone streaming (requires microphone access)\n",
    "# Uncomment to use:\n",
    "\n",
    "# print(\"Starting live microphone transcription...\")\n",
    "# print(\"Speak into your microphone. Press Ctrl+C to stop.\\n\")\n",
    "\n",
    "# streaming_asr.reset()\n",
    "\n",
    "# def mic_callback(text):\n",
    "#     print(f\"üé§ {text}\")\n",
    "\n",
    "# streaming_asr.stream_from_microphone(\n",
    "#     duration_sec=30,  # Record for 30 seconds\n",
    "#     callback=mic_callback\n",
    "# )\n",
    "\n",
    "# full_transcription = streaming_asr.get_full_transcription(merge=True)\n",
    "# print(f\"\\nüìù Full Transcription: {full_transcription}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fe6f0b",
   "metadata": {},
   "source": [
    "## 9. Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb8e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch inference for multiple files\n",
    "batch_inference = BatchInference(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    device=str(device),\n",
    "    batch_size=config['inference']['batch_size']\n",
    ")\n",
    "\n",
    "# Create temporary test files\n",
    "import tempfile\n",
    "import torchaudio\n",
    "\n",
    "test_files = []\n",
    "for i in range(5):\n",
    "    sample = val_dataset[i]\n",
    "    audio = torch.tensor(sample['audio']['array'])\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp:\n",
    "        torchaudio.save(tmp.name, audio.unsqueeze(0), 16000)\n",
    "        test_files.append(tmp.name)\n",
    "\n",
    "# Batch transcribe\n",
    "print(\"Batch transcribing 5 files...\\n\")\n",
    "batch_transcriptions = batch_inference.transcribe_batch(test_files)\n",
    "\n",
    "# Display results\n",
    "for i, transcription in enumerate(batch_transcriptions):\n",
    "    reference = val_dataset[i]['sentence']\n",
    "    print(f\"File {i+1}:\")\n",
    "    print(f\"  Prediction: {transcription}\")\n",
    "    print(f\"  Reference:  {reference}\")\n",
    "    print()\n",
    "\n",
    "# Cleanup\n",
    "for f in test_files:\n",
    "    os.unlink(f)\n",
    "\n",
    "print(\"‚úì Batch inference completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85b07c4",
   "metadata": {},
   "source": [
    "## 10. Visualization & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837705fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize evaluation results\n",
    "eval_viz = EvaluationVisualizer(save_dir='plots')\n",
    "\n",
    "# Model comparison data (example)\n",
    "comparison_data = {\n",
    "    'whisper-tiny': {\n",
    "        'params': '39M',\n",
    "        'wer_clean': 0.15,\n",
    "        'wer_accented': 0.20,\n",
    "        'latency': 0.1,\n",
    "        'rtf': 0.1\n",
    "    },\n",
    "    'whisper-base': {\n",
    "        'params': '74M',\n",
    "        'wer_clean': results['metrics']['wer'],\n",
    "        'wer_accented': 0.14,\n",
    "        'latency': latency_results['mean_latency'],\n",
    "        'rtf': latency_results['mean_rtf']\n",
    "    },\n",
    "    'whisper-small': {\n",
    "        'params': '244M',\n",
    "        'wer_clean': 0.08,\n",
    "        'wer_accented': 0.12,\n",
    "        'latency': 0.5,\n",
    "        'rtf': 0.5\n",
    "    }\n",
    "}\n",
    "\n",
    "# Plot comparison\n",
    "comparison_path = eval_viz.plot_model_comparison(comparison_data)\n",
    "print(f\"‚úì Model comparison plot saved: {comparison_path}\")\n",
    "\n",
    "# Display\n",
    "from IPython.display import Image\n",
    "display(Image(comparison_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3016aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comparison table\n",
    "table = generate_comparison_table(comparison_data)\n",
    "print(\"\\nModel Comparison Table:\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456007d8",
   "metadata": {},
   "source": [
    "## 11. Export & Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2008269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model for deployment\n",
    "FINAL_MODEL_PATH = 'final_model'\n",
    "\n",
    "model.save_pretrained(FINAL_MODEL_PATH)\n",
    "processor.save_pretrained(FINAL_MODEL_PATH)\n",
    "\n",
    "print(f\"‚úì Final model saved to: {FINAL_MODEL_PATH}\")\n",
    "print(f\"\\nTo load model later:\")\n",
    "print(f\"  from transformers import WhisperForConditionalGeneration, WhisperProcessor\")\n",
    "print(f\"  model = WhisperForConditionalGeneration.from_pretrained('{FINAL_MODEL_PATH}')\")\n",
    "print(f\"  processor = WhisperProcessor.from_pretrained('{FINAL_MODEL_PATH}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2b1b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Upload to HuggingFace Hub\n",
    "# Requires HuggingFace token and authentication\n",
    "\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "\n",
    "# HF_MODEL_NAME = \"your-username/whisper-base-finetuned-en\"\n",
    "# model.push_to_hub(HF_MODEL_NAME)\n",
    "# processor.push_to_hub(HF_MODEL_NAME)\n",
    "# print(f\"‚úì Model uploaded to HuggingFace: {HF_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a91e659",
   "metadata": {},
   "source": [
    "## 12. Cleanup & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3275866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish experiment tracking\n",
    "experiment_logger.finish()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä PROJECT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n‚úì Model: {MODEL_VARIANT}\")\n",
    "print(f\"‚úì Training epochs: {TRAIN_EPOCHS}\")\n",
    "print(f\"‚úì Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"‚úì WER: {results['metrics']['wer']:.3f}\")\n",
    "print(f\"‚úì CER: {results['metrics']['cer']:.3f}\")\n",
    "print(f\"‚úì Mean latency: {latency_results['mean_latency']:.3f}s\")\n",
    "print(f\"‚úì RTF: {latency_results['mean_rtf']:.3f}x\")\n",
    "\n",
    "print(f\"\\n‚úì Model saved: {FINAL_MODEL_PATH}\")\n",
    "print(f\"‚úì Checkpoints: checkpoints/\")\n",
    "print(f\"‚úì Logs: logs/\")\n",
    "print(f\"‚úì Plots: plots/\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ Real-Time ASR System Ready for Deployment!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
