{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "995862c2",
   "metadata": {},
   "source": [
    "# Real-Time Multilingual ASR with Whisper\n",
    "\n",
    "This notebook implements a production-ready, real-time speech recognition system using OpenAI's Whisper models.\n",
    "\n",
    "**Features:**\n",
    "- Data preparation with augmentation\n",
    "- Model fine-tuning with multiple variants\n",
    "- Comprehensive evaluation (WER, CER, latency)\n",
    "- Real-time streaming inference\n",
    "- Full MLOps best practices (versioning, logging, reproducibility)\n",
    "\n",
    "**Author:** COMP3057 Project \n",
    "**Version:** 1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c25953",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23841875",
   "metadata": {},
   "source": [
    "### Colab Configuration Guide\n",
    "\n",
    "**Resource Constraints:** A100 GPU (~40GB), 220GB Disk\n",
    "\n",
    "**Quick Start Options:**\n",
    "\n",
    "| Profile | Model | Samples | Epochs | Time | Disk | Quality |\n",
    "|---------|-------|---------|--------|------|------|---------|\n",
    "| **Fast Demo** | tiny | 50/10 | 1 | ~5min | ~2GB | Basic |\n",
    "| **Balanced** | base | 200/40 | 2 | ~20min | ~5GB | Good |\n",
    "| **Best Quality** | small | 500/100 | 3 | ~60min | ~10GB | Better |\n",
    "\n",
    "**Adjustable Parameters:**\n",
    "- `TRAIN_SAMPLES` / `VAL_SAMPLES` - Dataset size\n",
    "- `MODEL_VARIANT` - Model quality (tiny/base/small)\n",
    "- `TRAIN_EPOCHS` - Training duration\n",
    "\n",
    "**Tips:**\n",
    "- Start with Fast Demo to verify everything works\n",
    "- Increase resources gradually if you have time\n",
    "- Checkpoints auto-save to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f24b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment and clone project\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if running in Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"Running in Google Colab\")\n",
    "    \n",
    "    # Mount Google Drive for saving checkpoints and logs\n",
    "    from google.colab import drive\n",
    "    if not os.path.exists('/content/drive'):\n",
    "        print(\"Mounting Google Drive...\")\n",
    "        drive.mount('/content/drive', force_remount=False)\n",
    "        print(\"Drive mounted at /content/drive\")\n",
    "    else:\n",
    "        print(\"Drive already mounted\")\n",
    "    \n",
    "    # --- Project Setup with Absolute Paths (Prevents Nesting) ---\n",
    "    PROJECT_DIR = '/content/COMP3057_Project'  # Use ABSOLUTE path\n",
    " \n",
    "    # Always start from /content to avoid nested cloning\n",
    "    os.chdir('/content')\n",
    "    print(f\"Working from: {os.getcwd()}\")\n",
    "    \n",
    "    # Check for nested structure and fix it\n",
    "    if os.path.exists('COMP3057_Project/COMP3057_Project'):\n",
    "        print(\"\\nNested structure detected. Fixing...\")\n",
    "        import shutil\n",
    "        \n",
    "        # Check if outer is just a wrapper\n",
    "        outer_contents = os.listdir('COMP3057_Project')\n",
    "        if len(outer_contents) == 1 and outer_contents[0] == 'COMP3057_Project':\n",
    "            print(\"   Moving inner project to correct location...\")\n",
    "            shutil.move('COMP3057_Project/COMP3057_Project', 'COMP3057_Project_temp')\n",
    "            shutil.rmtree('COMP3057_Project')\n",
    "            shutil.move('COMP3057_Project_temp', 'COMP3057_Project')\n",
    "            print(\"   Fixed nested structure\")\n",
    "        else:\n",
    "            print(\"   Complex nesting - keeping as is\")\n",
    " \n",
    "    # Clone repository if not exists\n",
    "    if not os.path.exists(PROJECT_DIR):\n",
    "        print(\"\\nCloning repository from GitHub...\")\n",
    "        !git clone https://github.com/jimmy00415/COMP3057_Project.git\n",
    "        print(\"Repository cloned\")\n",
    "    else:\n",
    "        print(\"Repository already exists\")\n",
    "        \n",
    "        # Pull latest changes if already exists\n",
    "        print(\"   Checking for updates...\")\n",
    "        os.chdir(PROJECT_DIR)\n",
    "        !git pull\n",
    "        os.chdir('/content')\n",
    "    \n",
    "    # Change to project directory\n",
    "    os.chdir(PROJECT_DIR)\n",
    "    print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "    # Add project root to Python path\n",
    "    if os.getcwd() not in sys.path:\n",
    "        sys.path.insert(0, os.getcwd())\n",
    "        print(f\"Added '{os.getcwd()}' to Python path\")\n",
    " \n",
    "    # Verify structure\n",
    "    expected_files = ['src', 'config.yaml', 'requirements.txt', 'whisper_asr_colab.ipynb']\n",
    "    found_files = [f for f in expected_files if os.path.exists(f)]\n",
    "    print(f\"Structure check: {len(found_files)}/{len(expected_files)} key items found\")\n",
    "    \n",
    "    if len(found_files) < len(expected_files):\n",
    "        missing = set(expected_files) - set(found_files)\n",
    "        print(f\"   Missing: {missing}\")\n",
    "    \n",
    "    # Install dependencies\n",
    "    print(\"\\nInstalling dependencies...\")\n",
    "    !pip install -q -r requirements.txt\n",
    "    !pip install -q sounddevice\n",
    "    print(\"Dependencies installed\")\n",
    "\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "    print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "gpu_available = torch.cuda.is_available()\n",
    "if gpu_available:\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"\\nGPU: {gpu_name}\")\n",
    "    print(f\"   Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"\\nWARNING: No GPU detected. Training will be very slow.\")\n",
    "    print(\"   Enable GPU: Runtime -> Change runtime type -> GPU (T4)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SETUP COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext steps for post-training evaluation:\")\n",
    "print(\"  1. Run the Quick Restart cell (Section 1) to load your trained model\")\n",
    "print(\"  2. Run the validation dataset loader (Section 1)\")\n",
    "print(\"  3. Skip to Section 7 (Model Evaluation)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40c0652",
   "metadata": {},
   "source": [
    "### Quick Restart (Skip Training)\n",
    "\n",
    "**If you've already trained a model and just want to run inference/evaluation:**\n",
    "\n",
    "1. Run the Setup cell above (mounts Drive, clones repo, installs deps)\n",
    "2. Run the Quick Restart cell below to load your trained model\n",
    "3. Skip to Section 7 (Evaluation) or later sections\n",
    "\n",
    "This avoids re-running the expensive training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28021bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Restart - Load existing model without retraining\n",
    "# Use this to skip training and jump to evaluation\n",
    "# Run after the setup cell if you have a trained model\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"QUICK RESTART - POST-TRAINING EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Ensure project path is available\n",
    "if 'google.colab' in sys.modules:\n",
    "    project_root = '/content/COMP3057_Project'\n",
    "    if os.path.exists(project_root):\n",
    "        os.chdir(project_root)\n",
    "        if project_root not in sys.path:\n",
    "            sys.path.insert(0, project_root)\n",
    "        print(f\"Working in {project_root}\")\n",
    "    else:\n",
    "        raise RuntimeError(\"Project not found. Run the Setup cell first.\")\n",
    "else:\n",
    "    # Local environment\n",
    "    project_root = os.getcwd()\n",
    "    if project_root not in sys.path:\n",
    "        sys.path.insert(0, project_root)\n",
    "    print(f\"Working in {project_root}\")\n",
    "\n",
    "# Import all necessary modules\n",
    "print(\"\\nImporting modules...\")\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from src.utils.config import load_config\n",
    "from src.models.whisper_model import WhisperModelManager\n",
    "from src.utils.versioning import ModelRegistry\n",
    "\n",
    "# Import evaluation and inference modules\n",
    "from src.evaluation import (\n",
    " ModelEvaluator,\n",
    " LatencyBenchmark,\n",
    " EvaluationVisualizer,\n",
    " generate_comparison_table\n",
    ")\n",
    "\n",
    "from src.inference import (\n",
    " StreamingASR,\n",
    " BatchInference\n",
    ")\n",
    "\n",
    "from src.data import VoiceActivityDetector\n",
    "\n",
    "print(\"Modules imported\")\n",
    "\n",
    "# Load config\n",
    "config = load_config('config.yaml')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Initialize VAD for streaming\n",
    "vad = VoiceActivityDetector(threshold=config['data']['vad_threshold'])\n",
    "\n",
    "# Auto-detect checkpoint and model variant\n",
    "print(\"\\nSearching for trained model...\")\n",
    "\n",
    "# Priority order: best_model_hf > final_model > checkpoint-*/\n",
    "checkpoint_candidates = [\n",
    " \"checkpoints/best_model_hf\",\n",
    " \"final_model\",\n",
    " \"checkpoints/checkpoint-final\",\n",
    "]\n",
    "\n",
    "# Also check Google Drive if in Colab\n",
    "if 'google.colab' in sys.modules:\n",
    " drive_checkpoints = [\n",
    " \"/content/drive/MyDrive/COMP3057_ASR/checkpoints/best_model_hf\",\n",
    " \"/content/drive/MyDrive/COMP3057_ASR/final_model\",\n",
    " ]\n",
    " checkpoint_candidates = drive_checkpoints + checkpoint_candidates\n",
    "\n",
    "# Find the first existing checkpoint\n",
    "CHECKPOINT_PATH = None\n",
    "for candidate in checkpoint_candidates:\n",
    " if os.path.exists(candidate):\n",
    " CHECKPOINT_PATH = candidate\n",
    " print(f\"Found checkpoint: {CHECKPOINT_PATH}\")\n",
    " break\n",
    "\n",
    "if CHECKPOINT_PATH is None:\n",
    " print(\"No trained checkpoint found.\")\n",
    " print(\" Checked locations:\")\n",
    " for candidate in checkpoint_candidates:\n",
    " print(f\" - {candidate}\")\n",
    " \n",
    " # Fallback to base model\n",
    " print(\"\\n Loading base model instead (not fine-tuned)...\")\n",
    " MODEL_VARIANT = config.get('model', {}).get('variant', 'small')\n",
    " model_manager = WhisperModelManager(config)\n",
    " model, processor = model_manager.initialize_model(variant=MODEL_VARIANT, device=str(device))\n",
    " print(f\"Loaded base {MODEL_VARIANT} model (NOT fine-tuned)\")\n",
    " \n",
    "else:\n",
    " # Load from checkpoint\n",
    " print(f\"\\nLoading fine-tuned model from: {CHECKPOINT_PATH}\")\n",
    " from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    " \n",
    " model = WhisperForConditionalGeneration.from_pretrained(CHECKPOINT_PATH)\n",
    " processor = WhisperProcessor.from_pretrained(CHECKPOINT_PATH)\n",
    " model = model.to(device)\n",
    " \n",
    " # Detect model variant from config\n",
    " from transformers import WhisperConfig\n",
    " model_config = model.config\n",
    " if hasattr(model_config, 'd_model'):\n",
    " d_model = model_config.d_model\n",
    " variant_map = {256: 'tiny', 512: 'base', 768: 'small', 1024: 'medium', 1280: 'large'}\n",
    " MODEL_VARIANT = variant_map.get(d_model, 'small')\n",
    " else:\n",
    " MODEL_VARIANT = 'small'\n",
    " \n",
    " print(f\"Loaded fine-tuned model (variant: {MODEL_VARIANT})\")\n",
    "\n",
    "# Initialize model registry\n",
    "model_registry = ModelRegistry()\n",
    "\n",
    "# Set model to eval mode\n",
    "model.eval()\n",
    "\n",
    "# Get model info (needed for visualization)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "model_info = {\n",
    " 'total_parameters': total_params,\n",
    " 'trainable_parameters': trainable_params,\n",
    " 'variant': MODEL_VARIANT\n",
    "}\n",
    "\n",
    "# Initialize batch inference (needed for streaming test)\n",
    "batch_inference = BatchInference(\n",
    " model=model,\n",
    " processor=processor,\n",
    " device=str(device),\n",
    " batch_size=config['inference']['batch_size']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUICK RESTART COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model Summary:\")\n",
    "print(f\" - Variant: {MODEL_VARIANT}\")\n",
    "print(f\" - Parameters: {total_params:,}\")\n",
    "print(f\" - Device: {device}\")\n",
    "print(f\" - Status: {'Fine-tuned' if CHECKPOINT_PATH else 'Base model (not trained)'}\")\n",
    "print(f\" - Checkpoint: {CHECKPOINT_PATH if CHECKPOINT_PATH else 'N/A'}\")\n",
    "print(\"\\nReady for evaluation.\")\n",
    "print(\" Next: Run the validation dataset loader cell below\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cc0f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation dataset for evaluation\n",
    "# This loads a small validation set without needing the full training pipeline\n",
    "\n",
    "from datasets import load_dataset, Audio\n",
    "import soundfile as sf\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "VAL_SAMPLES = 400  # Match training configuration\n",
    "\n",
    "print(f\"Loading validation dataset ({VAL_SAMPLES} samples)...\")\n",
    "\n",
    "    # Try minds14 first\n",
    "    print(\"Loading minds14 dataset...\")\n",
    "    dataset = load_dataset(\"PolyAI/minds14\", \"en-US\", split=f\"train[:{VAL_SAMPLES}]\")\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(decode=False))\n",
    "    \n",
    "    def decode_audio(example):\n",
    "        audio = example[\"audio\"]\n",
    "        if \"bytes\" in audio and audio[\"bytes\"] is not None:\n",
    "            waveform, sr = sf.read(io.BytesIO(audio[\"bytes\"]))\n",
    "        elif \"path\" in audio and audio[\"path\"] is not None:\n",
    "            waveform, sr = sf.read(audio[\"path\"])\n",
    "        else:\n",
    "            waveform, sr = np.array([], dtype=np.float32), 16000\n",
    "        \n",
    "        if not isinstance(waveform, np.ndarray):\n",
    "            waveform = np.array(waveform, dtype=np.float32)\n",
    "        else:\n",
    "            waveform = waveform.astype(np.float32)\n",
    "        \n",
    "        example[\"audio_decoded\"] = {\"array\": waveform.tolist(), \"sampling_rate\": int(sr)}\n",
    "        return example\n",
    " return example\n",
    "    dataset = dataset.map(decode_audio, desc=\"Decoding audio\")\n",
    "    dataset = dataset.remove_columns([\"audio\"])\n",
    "    dataset = dataset.rename_column(\"audio_decoded\", \"audio\")\n",
    "    val_dataset = dataset\n",
    "    dataset_name = \"minds14_en\"\n",
    " dataset_name = \"minds14_en\"\n",
    " \n",
    "    print(f\"minds14 failed: {e}, trying LibriSpeech...\")\n",
    "    dataset = load_dataset(\"openslr/librispeech_asr\", \"clean\", split=f\"test[:{VAL_SAMPLES}]\")\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(decode=False))\n",
    "    dataset = dataset.map(decode_audio, desc=\"Decoding audio\")\n",
    "    dataset = dataset.remove_columns([\"audio\"])\n",
    "    dataset = dataset.rename_column(\"audio_decoded\", \"audio\")\n",
    "    val_dataset = dataset\n",
    "    dataset_name = \"librispeech_clean\"\n",
    " dataset_name = \"librispeech_clean\"\n",
    "\n",
    "# Detect text column - Make it global for use in other cells\n",
    "text_column = None\n",
    "    if col in val_dataset.column_names:\n",
    "        text_column = col\n",
    "        break\n",
    " break\n",
    "\n",
    "    raise ValueError(f\"Could not find text column. Available: {val_dataset.column_names}\")\n",
    " raise ValueError(f\"Could not find text column. Available: {val_dataset.column_names}\")\n",
    "\n",
    "print(f\"  Samples: {len(val_dataset)}\")\n",
    "print(f\"  Text column: '{text_column}'\")\n",
    "print(f\" Text column: '{text_column}'\")\n",
    "\n",
    "# Create validation data loader\n",
    "from src.data import WhisperDataset, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "    val_dataset,\n",
    "    processor,\n",
    "    audio_column=\"audio\",\n",
    "    text_column=text_column,\n",
    "    max_audio_length_sec=config['data']['audio_max_length_sec'],\n",
    "    augmenter=None\n",
    " augmenter=None\n",
    ")\n",
    "\n",
    "collator = DataCollatorWithPadding(processor)\n",
    "    val_dataset_wrapper,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collator,\n",
    "    num_workers=0\n",
    " num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Data loader created ({len(val_loader)} batches)\")\n",
    "print(f\"\\nDataset ready for evaluation.\")\n",
    "print(f\" Next: Skip to Section 7 (Model Evaluation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8850e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST-TRAINING VALIDATION - Verify all components are ready\n",
    "print(\"=\" * 80)\n",
    "print(\"POST-TRAINING VALIDATION CHECK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Track validation results\n",
    "validation_passed = True\n",
    "required_vars = {}\n",
    "\n",
    "# Check 1: Model and processor\n",
    "print(\"\\n1. Model & Processor:\")\n",
    "try:\n",
    " assert 'model' in dir(), \"model not found\"\n",
    " assert 'processor' in dir(), \"processor not found\"\n",
    " assert model is not None, \"model is None\"\n",
    " assert processor is not None, \"processor is None\"\n",
    " print(f\" Model: {type(model).__name__}\")\n",
    " print(f\" Processor: {type(processor).__name__}\")\n",
    " required_vars['model'] = True\n",
    " required_vars['processor'] = True\n",
    "except AssertionError as e:\n",
    " print(f\" FAILED: {e}\")\n",
    " validation_passed = False\n",
    " required_vars['model'] = False\n",
    " required_vars['processor'] = False\n",
    "\n",
    "# Check 2: Device\n",
    "print(\"\\n2. Device Configuration:\")\n",
    "try:\n",
    " assert 'device' in dir(), \"device not found\"\n",
    " print(f\" Device: {device}\")\n",
    " print(f\" Model on device: {next(model.parameters()).device}\")\n",
    " required_vars['device'] = True\n",
    "except AssertionError as e:\n",
    " print(f\" FAILED: {e}\")\n",
    " validation_passed = False\n",
    " required_vars['device'] = False\n",
    "\n",
    "# Check 3: Config\n",
    "print(\"\\n3. Configuration:\")\n",
    "try:\n",
    " assert 'config' in dir(), \"config not found\"\n",
    " assert config is not None, \"config is None\"\n",
    " print(f\" Config loaded\")\n",
    " print(f\" Inference batch size: {config['inference']['batch_size']}\")\n",
    " required_vars['config'] = True\n",
    "except AssertionError as e:\n",
    " print(f\" FAILED: {e}\")\n",
    " validation_passed = False\n",
    " required_vars['config'] = False\n",
    "\n",
    "# Check 4: Validation dataset\n",
    "print(\"\\n4. Validation Dataset:\")\n",
    "try:\n",
    " assert 'val_dataset' in dir(), \"val_dataset not found\"\n",
    " assert 'val_loader' in dir(), \"val_loader not found\"\n",
    " assert 'text_column' in dir(), \"text_column not found\"\n",
    " print(f\" Dataset: {len(val_dataset)} samples\")\n",
    " print(f\" Data loader: {len(val_loader)} batches\")\n",
    " print(f\" Text column: '{text_column}'\")\n",
    " required_vars['val_dataset'] = True\n",
    " required_vars['val_loader'] = True\n",
    " required_vars['text_column'] = True\n",
    "except AssertionError as e:\n",
    " print(f\" FAILED: {e}\")\n",
    " print(f\" Action: Run the validation dataset loader cell above\")\n",
    " validation_passed = False\n",
    " required_vars['val_dataset'] = False\n",
    " required_vars['val_loader'] = False\n",
    " required_vars['text_column'] = False\n",
    "\n",
    "# Check 5: Model info\n",
    "print(\"\\n5. Model Information:\")\n",
    "try:\n",
    " assert 'model_info' in dir(), \"model_info not found\"\n",
    " assert 'MODEL_VARIANT' in dir(), \"MODEL_VARIANT not found\"\n",
    " print(f\" Variant: {MODEL_VARIANT}\")\n",
    " print(f\" Parameters: {model_info['total_parameters']:,}\")\n",
    " required_vars['model_info'] = True\n",
    " required_vars['MODEL_VARIANT'] = True\n",
    "except AssertionError as e:\n",
    " print(f\" FAILED: {e}\")\n",
    " print(f\" Action: Run the Quick Restart cell\")\n",
    " validation_passed = False\n",
    " required_vars['model_info'] = False\n",
    " required_vars['MODEL_VARIANT'] = False\n",
    "\n",
    "# Check 6: Inference components\n",
    "print(\"\\n6. Inference Components:\")\n",
    "try:\n",
    " assert 'batch_inference' in dir(), \"batch_inference not found\"\n",
    " assert 'vad' in dir(), \"vad not found\"\n",
    " print(f\" BatchInference initialized\")\n",
    " print(f\" VAD initialized\")\n",
    " required_vars['batch_inference'] = True\n",
    " required_vars['vad'] = True\n",
    "except AssertionError as e:\n",
    " print(f\" FAILED: {e}\")\n",
    " print(f\" Action: Run the Quick Restart cell\")\n",
    " validation_passed = False\n",
    " required_vars['batch_inference'] = False\n",
    " required_vars['vad'] = False\n",
    "\n",
    "# Check 7: Test forward pass\n",
    "print(\"\\n7. Forward Pass Test:\")\n",
    "try:\n",
    " test_sample = val_dataset[0]\n",
    " test_audio = torch.tensor(test_sample['audio']['array'])\n",
    " \n",
    " model.eval()\n",
    " with torch.no_grad():\n",
    " input_features = processor(\n",
    " test_audio,\n",
    " sampling_rate=16000,\n",
    " return_tensors=\"pt\"\n",
    " ).input_features.to(device)\n",
    " \n",
    " predicted_ids = model.generate(input_features, max_length=50)\n",
    " test_transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    " \n",
    " print(f\" Forward pass successful\")\n",
    " print(f\" Test transcription: '{test_transcription[:50]}...'\")\n",
    " required_vars['forward_pass'] = True\n",
    "except Exception as e:\n",
    " print(f\" FAILED: {e}\")\n",
    " validation_passed = False\n",
    " required_vars['forward_pass'] = False\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "if validation_passed:\n",
    " print(\"ALL CHECKS PASSED - READY FOR EVALUATION\")\n",
    " print(\"=\" * 80)\n",
    " print(\"\\nYou can now run:\")\n",
    " print(\" - Section 7: Model Evaluation (WER/CER/Latency)\")\n",
    " print(\" - Section 8: Streaming Inference Test\")\n",
    " print(\" - Section 10: Visualization\")\n",
    "else:\n",
    " print(\"VALIDATION FAILED - MISSING COMPONENTS\")\n",
    " print(\"=\" * 80)\n",
    " print(\"\\nRequired actions:\")\n",
    " if not required_vars.get('model') or not required_vars.get('processor'):\n",
    " print(\" 1. Run the Quick Restart cell (Section 1)\")\n",
    " if not required_vars.get('val_dataset'):\n",
    " print(\" 2. Run the validation dataset loader cell\")\n",
    " if not required_vars.get('batch_inference'):\n",
    " print(\" 3. Re-run the Quick Restart cell\")\n",
    " \n",
    " print(\"\\nMissing variables:\")\n",
    " for var, status in required_vars.items():\n",
    " if not status:\n",
    " print(f\" - {var}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2762a1",
   "metadata": {},
   "source": [
    "### Professional-Grade Improvements\n",
    "\n",
    "This notebook includes production-grade enhancements:\n",
    "- **Error Recovery:** Automatic handling of corrupted data, NaN/Inf, OOM errors\n",
    "- **Memory Management:** Intelligent cleanup and optimization\n",
    "- **Audio Validation:** Quality checks for all audio samples\n",
    "- **Structured Logging:** Comprehensive metrics tracking\n",
    "- **Safe Checkpointing:** Atomic writes with validation\n",
    "\n",
    "See `IMPROVEMENTS.md` for full details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dcae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab Resource Optimization & Monitoring\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "# Re-check if in Colab (in case this cell is run independently)\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "def check_disk_space():\n",
    " \"\"\"Check available disk space.\"\"\"\n",
    " total, used, free = shutil.disk_usage(\"/\")\n",
    " print(f\"ð¾ Disk Space:\")\n",
    " print(f\" Total: {total // (2**30)} GB\")\n",
    " print(f\" Used: {used // (2**30)} GB\")\n",
    " print(f\" Free: {free // (2**30)} GB\")\n",
    " return free // (2**30)\n",
    "\n",
    "def check_gpu_memory():\n",
    " \"\"\"Check GPU memory usage.\"\"\"\n",
    " import torch\n",
    " if torch.cuda.is_available():\n",
    " allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    " reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    " total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    " print(f\"\\nð® GPU Memory:\")\n",
    " print(f\" Allocated: {allocated:.2f} GB\")\n",
    " print(f\" Reserved: {reserved:.2f} GB\")\n",
    " print(f\" Total: {total:.2f} GB\")\n",
    " print(f\" Free: {total - reserved:.2f} GB\")\n",
    "\n",
    "def cleanup_cache():\n",
    " \"\"\"Clear unnecessary cache to free disk space.\"\"\"\n",
    " import torch\n",
    " if torch.cuda.is_available():\n",
    " torch.cuda.empty_cache()\n",
    " import gc\n",
    " gc.collect()\n",
    " print(\"â?Cache cleared\")\n",
    "\n",
    "if IN_COLAB:\n",
    " # Setup Google Drive paths for checkpoints\n",
    " DRIVE_ROOT = '/content/drive/MyDrive/COMP3057_ASR'\n",
    " import os\n",
    " os.makedirs(DRIVE_ROOT, exist_ok=True)\n",
    " os.makedirs(f'{DRIVE_ROOT}/checkpoints', exist_ok=True)\n",
    " os.makedirs(f'{DRIVE_ROOT}/logs', exist_ok=True)\n",
    " print(f\"\\nð Google Drive storage: {DRIVE_ROOT}\")\n",
    " \n",
    " # Create symlinks to save to Drive instead of local disk\n",
    " if os.path.exists('checkpoints') and not os.path.islink('checkpoints'):\n",
    " shutil.rmtree('checkpoints')\n",
    " if not os.path.exists('checkpoints'):\n",
    " os.symlink(f'{DRIVE_ROOT}/checkpoints', 'checkpoints')\n",
    " print(\"â?Checkpoints will be saved to Google Drive\")\n",
    " \n",
    " if os.path.exists('logs') and not os.path.islink('logs'):\n",
    " shutil.rmtree('logs')\n",
    " if not os.path.exists('logs'):\n",
    " os.symlink(f'{DRIVE_ROOT}/logs', 'logs')\n",
    " print(\"â?Logs will be saved to Google Drive\")\n",
    " \n",
    " # Check initial resources\n",
    " print(\"\\nð Initial Resource Check:\")\n",
    " free_disk = check_disk_space()\n",
    " check_gpu_memory()\n",
    " \n",
    " if free_disk < 50:\n",
    " print(\"\\nâ ï¸ WARNING: Low disk space! Consider:\")\n",
    " print(\" 1. Use smaller dataset (already optimized)\")\n",
    " print(\" 2. Use 'tiny' or 'base' model variant\")\n",
    " print(\" 3. Reduce save_steps to save fewer checkpoints\")\n",
    "else:\n",
    " # Define cleanup_cache for local use\n",
    " import torch\n",
    " def cleanup_cache():\n",
    " if torch.cuda.is_available():\n",
    " torch.cuda.empty_cache()\n",
    " import gc\n",
    " gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce2e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Import project modules\n",
    "from src.utils import (\n",
    " load_config,\n",
    " set_seed,\n",
    " setup_logging,\n",
    " get_device,\n",
    " ExperimentLogger,\n",
    " DataVersionManager,\n",
    " ModelRegistry\n",
    ")\n",
    "\n",
    "from src.data import (\n",
    " AudioPreprocessor,\n",
    " VoiceActivityDetector,\n",
    " AudioAugmenter,\n",
    " WhisperDataset,\n",
    " prepare_datasets,\n",
    " create_dataloaders\n",
    ")\n",
    "\n",
    "from src.models import (\n",
    " WhisperModelManager,\n",
    " compare_models\n",
    ")\n",
    "\n",
    "from src.training import WhisperTrainer\n",
    "\n",
    "from src.evaluation import (\n",
    " ModelEvaluator,\n",
    " LatencyBenchmark,\n",
    " TrainingVisualizer,\n",
    " EvaluationVisualizer,\n",
    " generate_comparison_table\n",
    ")\n",
    "\n",
    "from src.inference import (\n",
    " StreamingASR,\n",
    " BatchInference\n",
    ")\n",
    "\n",
    "print(\"â?All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172c29b4",
   "metadata": {},
   "source": [
    "## 2. Configuration & Reproducibility Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55abd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config('config.yaml')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "set_seed(config['project']['seed'])\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_logging('INFO', 'logs/training.log')\n",
    "\n",
    "# Get device\n",
    "device = get_device(config['project']['device'])\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize experiment tracking (choose: wandb, mlflow, or tensorboard)\n",
    "experiment_logger = ExperimentLogger(\n",
    " backend=config['mlops']['experiment_tracking']['backend'],\n",
    " project_name=config['mlops']['experiment_tracking']['project_name'],\n",
    " config=config\n",
    ")\n",
    "\n",
    "# Initialize versioning\n",
    "data_version_manager = DataVersionManager(\n",
    " config['mlops']['versioning']['data_version_file']\n",
    ")\n",
    "model_registry = ModelRegistry(\n",
    " config['mlops']['versioning']['model_registry']\n",
    ")\n",
    "\n",
    "print(f\"â?Configuration loaded\")\n",
    "print(f\" - Seed: {config['project']['seed']}\")\n",
    "print(f\" - Device: {device}\")\n",
    "print(f\" - Tracking: {config['mlops']['experiment_tracking']['backend']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f62e685",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6286d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing utilities\n",
    "audio_preprocessor = AudioPreprocessor(\n",
    " target_sr=config['data']['sampling_rate'],\n",
    " normalize=True\n",
    ")\n",
    "\n",
    "vad = VoiceActivityDetector(\n",
    " threshold=config['data']['vad_threshold']\n",
    ")\n",
    "\n",
    "augmenter = AudioAugmenter(\n",
    " speed_perturbation=config['data']['augmentation']['speed_perturbation'],\n",
    " pitch_shift_semitones=config['data']['augmentation']['pitch_shift_semitones'],\n",
    " background_noise_prob=config['data']['augmentation']['background_noise_prob']\n",
    ") if config['data']['augmentation']['enabled'] else None\n",
    "\n",
    "print(\"â?Preprocessing utilities initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1acff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "# Using small subsets optimized for Colab's disk/memory constraints\n",
    "# For production with more resources, increase the sample counts\n",
    "\n",
    "from datasets import load_dataset, Audio\n",
    "import soundfile as sf\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "# Configuration for Colab\n",
    "TRAIN_SAMPLES = 2000 # Increased for better model quality (80GB GPU can handle this)\n",
    "VAL_SAMPLES = 400 # Proportional validation set for robust evaluation\n",
    "\n",
    "print(f\"Loading dataset with {TRAIN_SAMPLES} train + {VAL_SAMPLES} val samples...\")\n",
    "print(\"(Optimized for Colab - increase samples for production)\")\n",
    "\n",
    "try:\n",
    " # Try minds14 first - smaller and faster to download\n",
    " print(\"\\nTrying minds14 dataset (lightweight, ~50MB)...\")\n",
    " dataset = load_dataset(\n",
    " \"PolyAI/minds14\",\n",
    " \"en-US\",\n",
    " split=f\"train[:{TRAIN_SAMPLES + VAL_SAMPLES}]\"\n",
    " )\n",
    " \n",
    " # Cast audio column to disable automatic decoding\n",
    " dataset = dataset.cast_column(\"audio\", Audio(decode=False))\n",
    " \n",
    " # Pre-decode all audio using soundfile - store in separate column\n",
    " def decode_audio(example):\n",
    " audio = example[\"audio\"]\n",
    " if \"bytes\" in audio and audio[\"bytes\"] is not None:\n",
    " waveform, sr = sf.read(io.BytesIO(audio[\"bytes\"]))\n",
    " elif \"path\" in audio and audio[\"path\"] is not None:\n",
    " waveform, sr = sf.read(audio[\"path\"])\n",
    " else:\n",
    " waveform, sr = np.array([], dtype=np.float32), 16000\n",
    " \n",
    " # Ensure numpy array\n",
    " if not isinstance(waveform, np.ndarray):\n",
    " waveform = np.array(waveform, dtype=np.float32)\n",
    " else:\n",
    " waveform = waveform.astype(np.float32)\n",
    " \n",
    " # Store decoded audio in NEW column\n",
    " example[\"audio_decoded\"] = {\"array\": waveform.tolist(), \"sampling_rate\": int(sr)}\n",
    " return example\n",
    " \n",
    " print(\"Pre-decoding audio samples...\")\n",
    " dataset = dataset.map(decode_audio, desc=\"Decoding audio\")\n",
    " \n",
    " # Remove original audio column with Audio feature, rename decoded column\n",
    " dataset = dataset.remove_columns([\"audio\"])\n",
    " dataset = dataset.rename_column(\"audio_decoded\", \"audio\")\n",
    " \n",
    " # Split into train/val\n",
    " split_point = TRAIN_SAMPLES\n",
    " train_dataset = dataset.select(range(split_point))\n",
    " val_dataset = dataset.select(range(split_point, TRAIN_SAMPLES + VAL_SAMPLES))\n",
    " \n",
    " dataset_name = \"minds14_en\"\n",
    " print(f\"â?minds14 loaded successfully\")\n",
    " \n",
    "except Exception as e:\n",
    " print(f\"minds14 failed: {e}\")\n",
    " print(\"\\nTrying LibriSpeech (larger, ~300MB)...\")\n",
    " \n",
    " # Fallback to LibriSpeech\n",
    " dataset = load_dataset(\n",
    " \"openslr/librispeech_asr\",\n",
    " \"clean\",\n",
    " split=f\"test[:{TRAIN_SAMPLES + VAL_SAMPLES}]\"\n",
    " )\n",
    " \n",
    " # Cast audio column to disable automatic decoding\n",
    " dataset = dataset.cast_column(\"audio\", Audio(decode=False))\n",
    " \n",
    " # Pre-decode all audio using soundfile\n",
    " def decode_audio(example):\n",
    " audio = example[\"audio\"]\n",
    " if \"bytes\" in audio and audio[\"bytes\"] is not None:\n",
    " waveform, sr = sf.read(io.BytesIO(audio[\"bytes\"]))\n",
    " elif \"path\" in audio and audio[\"path\"] is not None:\n",
    " waveform, sr = sf.read(audio[\"path\"])\n",
    " else:\n",
    " waveform, sr = np.array([], dtype=np.float32), 16000\n",
    " \n",
    " if not isinstance(waveform, np.ndarray):\n",
    " waveform = np.array(waveform, dtype=np.float32)\n",
    " else:\n",
    " waveform = waveform.astype(np.float32)\n",
    " \n",
    " example[\"audio_decoded\"] = {\"array\": waveform.tolist(), \"sampling_rate\": int(sr)}\n",
    " return example\n",
    " \n",
    " print(\"Pre-decoding audio samples...\")\n",
    " dataset = dataset.map(decode_audio, desc=\"Decoding audio\")\n",
    " \n",
    " # Remove original audio column, rename decoded\n",
    " dataset = dataset.remove_columns([\"audio\"])\n",
    " dataset = dataset.rename_column(\"audio_decoded\", \"audio\")\n",
    " \n",
    " # Split into train/val\n",
    " split_point = TRAIN_SAMPLES\n",
    " train_dataset = dataset.select(range(split_point))\n",
    " val_dataset = dataset.select(range(split_point, TRAIN_SAMPLES + VAL_SAMPLES))\n",
    " \n",
    " dataset_name = \"librispeech_clean\"\n",
    " print(f\"â?LibriSpeech loaded\")\n",
    "\n",
    "print(f\"\\nâ?Dataset loaded: {dataset_name}\")\n",
    "print(f\" - Training samples: {len(train_dataset)}\")\n",
    "print(f\" - Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Inspect dataset structure\n",
    "print(f\"\\nð Dataset structure:\")\n",
    "print(f\" - Columns: {train_dataset.column_names}\")\n",
    "\n",
    "# Log dataset version\n",
    "data_version_manager.log_dataset_version(\n",
    " dataset_name=dataset_name,\n",
    " version=f\"colab_demo_{TRAIN_SAMPLES}train_{VAL_SAMPLES}val\",\n",
    " metadata={'train': len(train_dataset), 'val': len(val_dataset)}\n",
    ")\n",
    "\n",
    "# Check disk space after loading\n",
    "if IN_COLAB:\n",
    " print(\"\\nð Disk space after dataset load:\")\n",
    " check_disk_space()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc7d1fc",
   "metadata": {},
   "source": [
    "## 4. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e251a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model manager\n",
    "model_manager = WhisperModelManager(config)\n",
    "\n",
    "# Choose model variant based on Colab resources\n",
    "# Recommendations for A100 (40GB):\n",
    "# - tiny: 39M params, ~500MB, fastest (RECOMMENDED for demo)\n",
    "# - base: 74M params, ~1GB, good balance\n",
    "# - small: 244M params, ~2GB, better accuracy\n",
    "# - medium: 769M params, ~6GB, best accuracy (may be slow)\n",
    "\n",
    "MODEL_VARIANT = 'small' # Change to 'base' or 'small' if you have time/resources\n",
    "\n",
    "print(f\"ð¤ Loading Whisper model: {MODEL_VARIANT}\")\n",
    "print(f\" (Optimized for Colab - use tiny/base for best experience)\")\n",
    "\n",
    "# Load model and processor\n",
    "model, processor = model_manager.initialize_model(\n",
    " variant=MODEL_VARIANT,\n",
    " device=str(device)\n",
    ")\n",
    "\n",
    "# Get model info\n",
    "model_info = model_manager.get_model_info()\n",
    "print(f\"\\nâ?Model initialized: {MODEL_VARIANT}\")\n",
    "print(f\" - Total parameters: {model_info['total_parameters']:,}\")\n",
    "print(f\" - Trainable parameters: {model_info['trainable_parameters']:,}\")\n",
    "\n",
    "# Check GPU memory after model load\n",
    "if IN_COLAB:\n",
    " check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a09c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model variants\n",
    "variants_info = compare_models(config)\n",
    "\n",
    "print(\"\\nWhisper Model Variants Comparison:\")\n",
    "print(\"-\" * 60)\n",
    "for variant, info in variants_info.items():\n",
    " print(f\"{variant:10s} | Params: {info['params']:8s} | Speed: {info['speed']:10s} | Accuracy: {info['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c80b81f",
   "metadata": {},
   "source": [
    "## 5. Prepare Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04885ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets\n",
    "# Auto-detect column names from the dataset\n",
    "audio_column = \"audio\" # Standard across most datasets\n",
    "# Text column varies: \"text\", \"sentence\", \"transcription\", etc.\n",
    "text_column = None\n",
    "for col in [\"text\", \"sentence\", \"transcription\", \"transcript\"]:\n",
    " if col in train_dataset.column_names:\n",
    " text_column = col\n",
    " break\n",
    "\n",
    "if text_column is None:\n",
    " raise ValueError(f\"Could not find text column. Available columns: {train_dataset.column_names}\")\n",
    "\n",
    "print(f\"Using columns: audio='{audio_column}', text='{text_column}'\")\n",
    "\n",
    "train_dataset_wrapper = WhisperDataset(\n",
    " train_dataset,\n",
    " processor,\n",
    " audio_column=audio_column,\n",
    " text_column=text_column,\n",
    " max_audio_length_sec=config['data']['audio_max_length_sec'],\n",
    " augmenter=augmenter # Apply augmentation only to training\n",
    ")\n",
    "\n",
    "val_dataset_wrapper = WhisperDataset(\n",
    " val_dataset,\n",
    " processor,\n",
    " audio_column=audio_column,\n",
    " text_column=text_column,\n",
    " max_audio_length_sec=config['data']['audio_max_length_sec'],\n",
    " augmenter=None # No augmentation for validation\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "from src.data import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "collator = DataCollatorWithPadding(processor)\n",
    "\n",
    "train_loader = DataLoader(\n",
    " train_dataset_wrapper,\n",
    " batch_size=config['training']['batch_size'],\n",
    " shuffle=True,\n",
    " collate_fn=collator,\n",
    " num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    " val_dataset_wrapper,\n",
    " batch_size=config['training']['batch_size'],\n",
    " shuffle=False,\n",
    " collate_fn=collator,\n",
    " num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"\\nâ?Data loaders created\")\n",
    "print(f\" - Training batches: {len(train_loader)}\")\n",
    "print(f\" - Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49fde4e",
   "metadata": {},
   "source": [
    "## 6. Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27108b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for training\n",
    "model = model_manager.prepare_for_training()\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = WhisperTrainer(\n",
    " model=model,\n",
    " processor=processor,\n",
    " train_loader=train_loader,\n",
    " val_loader=val_loader,\n",
    " config=config,\n",
    " device=str(device),\n",
    " experiment_logger=experiment_logger\n",
    ")\n",
    "\n",
    "print(\"â?Trainer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Test - Run before training\n",
    "print(\"=\" * 60)\n",
    "print(\"PRE-TRAINING VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test 1: Check config\n",
    "print(\"\\n1. Configuration:\")\n",
    "print(f\" Learning rate: {config['training']['learning_rate']}\")\n",
    "print(f\" Batch size: {config['training']['batch_size']}\")\n",
    "print(f\" Grad accumulation: {config['training']['gradient_accumulation_steps']}\")\n",
    "print(f\" Augmentation: {config['data']['augmentation']['enabled']}\")\n",
    "assert config['training']['learning_rate'] <= 1e-4, \"LR too high!\"\n",
    "assert not config['data']['augmentation']['enabled'], \"Augmentation should be disabled!\"\n",
    "print(\" Config safe\")\n",
    "\n",
    "# Test 2: Sample batch\n",
    "print(\"\\n2. Testing sample batch:\")\n",
    "test_batch = next(iter(train_loader))\n",
    "print(f\" Input shape: {test_batch['input_features'].shape}\")\n",
    "print(f\" Labels shape: {test_batch['labels'].shape}\")\n",
    "\n",
    "# Check for NaN/Inf\n",
    "has_nan = torch.isnan(test_batch['input_features']).any()\n",
    "has_inf = torch.isinf(test_batch['input_features']).any()\n",
    "assert not has_nan, \"NaN detected in features!\"\n",
    "assert not has_inf, \"Inf detected in features!\"\n",
    "print(\" No NaN/Inf in batch\")\n",
    "\n",
    "# Test 3: Forward pass\n",
    "print(\"\\n3. Testing forward pass:\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    " test_out = model(\n",
    " input_features=test_batch['input_features'][:2].to(device),\n",
    " labels=test_batch['labels'][:2].to(device)\n",
    " )\n",
    " test_loss = test_out.loss\n",
    "\n",
    "print(f\" Test loss: {test_loss.item():.4f}\")\n",
    "assert not torch.isnan(test_loss), \"NaN loss detected!\"\n",
    "assert not torch.isinf(test_loss), \"Inf loss detected!\"\n",
    "print(\" Forward pass successful\")\n",
    "\n",
    "model.train()\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" ALL VALIDATION TESTS PASSED\")\n",
    "print(\"Safe to proceed with training\")\n",
    "print(\"=\" * 60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e52984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "# Optimized for Colab: fewer epochs, memory-efficient settings\n",
    "\n",
    "TRAIN_EPOCHS = 5 # Increased for better convergence with larger dataset, 2-3 for Colab session, 5+ for production\n",
    "\n",
    "print(f\"ð Starting training for {TRAIN_EPOCHS} epoch(s)...\")\n",
    "print(f\" Batch size: {config['training']['batch_size']}\")\n",
    "print(f\" Gradient accumulation: {config['training']['gradient_accumulation_steps']}\")\n",
    "print(f\" Effective batch size: {config['training']['batch_size'] * config['training']['gradient_accumulation_steps']}\")\n",
    "print(f\" Mixed precision (FP16): {config['training']['fp16']}\")\n",
    "\n",
    "# Check resources before training\n",
    "if IN_COLAB:\n",
    " print(\"\\nð Pre-training resources:\")\n",
    " check_disk_space()\n",
    " check_gpu_memory()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Training\n",
    "best_val_loss = trainer.train(num_epochs=TRAIN_EPOCHS)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nâ?Training completed!\")\n",
    "print(f\" - Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Cleanup after training\n",
    "if IN_COLAB:\n",
    " print(\"\\nð§¹ Cleaning up...\")\n",
    " cleanup_cache()\n",
    " print(\"\\nð Post-training resources:\")\n",
    " check_disk_space()\n",
    " check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729c9491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register trained model\n",
    "# NOTE: If you get \"ModuleNotFoundError: No module named 'src'\":\n",
    "# 1. Make sure you ran the Setup cell (Section 1) first\n",
    "# 2. Or run the Quick Restart cell to restore the environment\n",
    "\n",
    "from src.utils import get_git_revision\n",
    "\n",
    "model_id = model_registry.register_model(\n",
    " model_id=f\"{MODEL_VARIANT}_finetuned_{TRAIN_EPOCHS}ep\",\n",
    " model_path=\"checkpoints/best_model_hf\",\n",
    " metrics={'val_loss': best_val_loss},\n",
    " config=config,\n",
    " git_revision=get_git_revision(),\n",
    " dataset_version=\"common_voice_11_0_en_subset\"\n",
    ")\n",
    "\n",
    "print(f\" Model registered: {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a168c3e1",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3607b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation - WER/CER Metrics\n",
    "print(\"=\"*80)\n",
    "print(\" MODEL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Validation: Check required variables\n",
    "required = ['model', 'processor', 'device', 'val_loader']\n",
    "missing = [var for var in required if var not in dir()]\n",
    "\n",
    "if missing:\n",
    " print(f\"\\n Missing required variables: {missing}\")\n",
    " print(\"\\n Required steps:\")\n",
    " if 'model' in missing or 'processor' in missing:\n",
    " print(\" 1. Run the Quick Restart cell (Section 1)\")\n",
    " if 'val_loader' in missing:\n",
    " print(\" 2. Run the validation dataset loader cell\")\n",
    " raise RuntimeError(\"Missing required variables for evaluation\")\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator(\n",
    " model=model,\n",
    " processor=processor,\n",
    " device=str(device)\n",
    ")\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"\\nEvaluating model on validation set...\\n\")\n",
    "results = evaluator.evaluate_with_samples(val_loader, num_samples=5)\n",
    "\n",
    "print(f\"\\n Evaluation Results (with text normalization):\")\n",
    "print(f\" - WER: {results['metrics']['wer']:.4f}\")\n",
    "print(f\" - CER: {results['metrics']['cer']:.4f}\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\n Sample Predictions:\")\n",
    "print(\"=\"*80)\n",
    "for i, sample in enumerate(results['samples'][:3], 1):\n",
    " print(f\"\\nSample {i}:\")\n",
    " print(f\" Reference: {sample['reference']}\")\n",
    " print(f\" Prediction: {sample['prediction']}\")\n",
    " print(f\" -----------------------------------------\")\n",
    " print(f\" Normalized Reference: {sample['reference_normalized']}\")\n",
    " print(f\" Normalized Prediction: {sample['prediction_normalized']}\")\n",
    " print(f\" Match: {' Yes' if sample['reference_normalized'] == sample['prediction_normalized'] else 'X No'}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n Evaluation complete!\")\n",
    "print(\" Next: Run the latency benchmark cell below\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b35df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latency Benchmark\n",
    "print(\"=\"*80)\n",
    "print(\" LATENCY BENCHMARK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Validation: Check required variables\n",
    "required = ['model', 'processor', 'device', 'val_dataset']\n",
    "missing = [var for var in required if var not in dir()]\n",
    "\n",
    "if missing:\n",
    " print(f\"\\n Missing required variables: {missing}\")\n",
    " print(\"\\n Run the Quick Restart cell and validation dataset loader\")\n",
    " raise RuntimeError(\"Missing required variables for latency benchmark\")\n",
    "\n",
    "print(\"\\nBenchmarking inference latency...\\n\")\n",
    "\n",
    "latency_bench = LatencyBenchmark(\n",
    " model=model,\n",
    " processor=processor,\n",
    " device=str(device)\n",
    ")\n",
    "\n",
    "# Generate test audio clips\n",
    "test_audios = []\n",
    "num_test_samples = min(10, len(val_dataset))\n",
    "for i in range(num_test_samples):\n",
    " sample = val_dataset[i]\n",
    " audio = torch.tensor(sample['audio']['array'])\n",
    " test_audios.append(audio)\n",
    "\n",
    "latency_results = latency_bench.benchmark_batch(test_audios, sr=16000)\n",
    "\n",
    "print(f\" Latency Benchmark Results:\")\n",
    "print(f\" - Mean latency: {latency_results['mean_latency']:.3f}s\")\n",
    "print(f\" - Std latency: {latency_results['std_latency']:.3f}s\")\n",
    "print(f\" - Mean RTF: {latency_results['mean_rtf']:.3f}x\")\n",
    "print(f\"\\n {' Real-time capable (RTF < 1.0)' if latency_results['mean_rtf'] < 1.0 else ' Not real-time (RTF >= 1.0)'}\")\n",
    "\n",
    "print(\"\\n Latency benchmark complete!\")\n",
    "print(\" Next: Skip to Section 8 (Streaming Inference)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47284343",
   "metadata": {},
   "source": [
    "## 8. Real-Time Streaming Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fdd7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize streaming ASR\n",
    "print(\"=\"*80)\n",
    "print(\" STREAMING ASR INITIALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Validation: Check required variables\n",
    "required = ['model', 'processor', 'vad', 'config', 'device']\n",
    "missing = [var for var in required if var not in dir()]\n",
    "\n",
    "if missing:\n",
    " print(f\"\\n Missing required variables: {missing}\")\n",
    " print(\"\\n Run the Quick Restart cell (Section 1)\")\n",
    " raise RuntimeError(\"Missing required variables for streaming ASR\")\n",
    "\n",
    "streaming_asr = StreamingASR(\n",
    " model=model,\n",
    " processor=processor,\n",
    " vad=vad,\n",
    " chunk_length_sec=config['inference']['streaming']['buffer_size_sec'],\n",
    " overlap_sec=config['inference']['streaming']['overlap_sec'],\n",
    " device=str(device)\n",
    ")\n",
    "\n",
    "print(\" Streaming ASR initialized\")\n",
    "print(f\" - Chunk length: {config['inference']['streaming']['buffer_size_sec']}s\")\n",
    "print(f\" - Overlap: {config['inference']['streaming']['overlap_sec']}s\")\n",
    "print(\"\\n Ready for streaming inference!\")\n",
    "print(\" Next: Run the streaming test cell below\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a820abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test streaming on audio file with detailed metrics\n",
    "# Use a sample from validation set\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STREAMING INFERENCE TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find a good test sample (2-5 seconds)\n",
    "test_samples = []\n",
    "for i in range(min(20, len(val_dataset))):\n",
    " sample = val_dataset[i]\n",
    " audio = sample['audio']['array']\n",
    " duration = len(audio) / 16000\n",
    " if 2.0 <= duration <= 5.0:\n",
    " test_samples.append({\n",
    " 'index': i,\n",
    " 'audio': audio,\n",
    " 'text': sample[text_column],\n",
    " 'duration': duration\n",
    " })\n",
    "\n",
    "if not test_samples:\n",
    " print(\" No suitable samples found, using first available\")\n",
    " test_sample = val_dataset[0]\n",
    " test_audio = test_sample['audio']['array']\n",
    " test_text = test_sample[text_column]\n",
    " audio_duration = len(test_audio) / 16000\n",
    "else:\n",
    " # Use the first suitable sample\n",
    " selected = test_samples[0]\n",
    " test_audio = selected['audio']\n",
    " test_text = selected['text']\n",
    " audio_duration = selected['duration']\n",
    " print(f\" Selected sample {selected['index']}: {audio_duration:.2f}s\")\n",
    "\n",
    "print(f\" Reference: {test_text}\")\n",
    "print(f\" Duration: {audio_duration:.2f}s\\n\")\n",
    "\n",
    "# Text normalization function\n",
    "def normalize_text(text):\n",
    " \"\"\"Normalize text for fair comparison.\"\"\"\n",
    " import re\n",
    " # Convert to lowercase\n",
    " text = text.lower()\n",
    " # Remove extra whitespace\n",
    " text = re.sub(r'\\s+', ' ', text)\n",
    " # Remove punctuation for WER/CER calculation\n",
    " text = re.sub(r'[^\\w\\s]', '', text)\n",
    " return text.strip()\n",
    "\n",
    "# Save to temporary file\n",
    "import torchaudio\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp:\n",
    " tmp_path = tmp.name\n",
    " torchaudio.save(tmp_path, torch.tensor(test_audio).unsqueeze(0), 16000)\n",
    "\n",
    "# Test 1: Standard batch inference (baseline)\n",
    "print(\"1️⃣ BATCH INFERENCE (Baseline)\")\n",
    "print(\"-\" * 80)\n",
    "start_time = time.time()\n",
    "batch_transcription = batch_inference.transcribe_batch([tmp_path])[0]\n",
    "batch_time = time.time() - start_time\n",
    "\n",
    "from jiwer import wer, cer\n",
    "# Use normalized text for metrics\n",
    "batch_wer = wer(normalize_text(test_text), normalize_text(batch_transcription))\n",
    "batch_cer = cer(normalize_text(test_text), normalize_text(batch_transcription))\n",
    "\n",
    "print(f\"Transcription: {batch_transcription}\")\n",
    "print(f\"Normalized: {normalize_text(batch_transcription)}\")\n",
    "print(f\"Time: {batch_time:.3f}s\")\n",
    "print(f\"WER: {batch_wer:.3f} | CER: {batch_cer:.3f}\")\n",
    "print(f\"RTF: {batch_time/audio_duration:.3f}x\\n\")\n",
    "\n",
    "# Test 2: Streaming inference (with improved overlap merging)\n",
    "print(\"2️⃣ STREAMING INFERENCE\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Create streaming ASR without VAD for consistent results\n",
    "streaming_asr_test = StreamingASR(\n",
    " model=model,\n",
    " processor=processor,\n",
    " vad=None, # Disable VAD for reproducible results\n",
    " chunk_length_sec=2.0, # Standard chunk size\n",
    " overlap_sec=0.5, # Overlap for continuity\n",
    " device=str(device)\n",
    ")\n",
    "\n",
    "chunks = []\n",
    "def callback(text):\n",
    " if text.strip():\n",
    " chunks.append(text.strip())\n",
    " print(f\" [Chunk {len(chunks)}] {text}\")\n",
    "\n",
    "streaming_asr_test.reset()\n",
    "start_time = time.time()\n",
    "streaming_asr_test.stream_from_file(tmp_path, chunk_duration_sec=1.0, callback=callback)\n",
    "streaming_time = time.time() - start_time\n",
    "\n",
    "# Get full transcription (merged with improved algorithm)\n",
    "full_transcription = streaming_asr_test.get_full_transcription(merge=True)\n",
    "\n",
    "if full_transcription.strip():\n",
    " stream_wer = wer(normalize_text(test_text), normalize_text(full_transcription))\n",
    " stream_cer = cer(normalize_text(test_text), normalize_text(full_transcription))\n",
    " \n",
    " print(f\"\\nMerged: {full_transcription}\")\n",
    " print(f\"Normalized: {normalize_text(full_transcription)}\")\n",
    " print(f\"Chunks: {len(chunks)}\")\n",
    " print(f\"Time: {streaming_time:.3f}s\")\n",
    " print(f\"WER: {stream_wer:.3f} | CER: {stream_cer:.3f}\")\n",
    " print(f\"RTF: {streaming_time/audio_duration:.3f}x\")\n",
    "else:\n",
    " print(\"\\n No transcription generated!\")\n",
    " full_transcription = \"\"\n",
    " stream_wer = 1.0\n",
    " stream_cer = 1.0\n",
    "\n",
    "# Test 3: Simple single-pass inference (no streaming)\n",
    "print(\"\\n3️⃣ SINGLE-PASS INFERENCE\")\n",
    "print(\"-\" * 80)\n",
    "start_time = time.time()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    " input_features = processor(\n",
    " torch.tensor(test_audio),\n",
    " sampling_rate=16000,\n",
    " return_tensors=\"pt\"\n",
    " ).input_features.to(device)\n",
    " \n",
    " predicted_ids = model.generate(input_features)\n",
    " single_transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "single_time = time.time() - start_time\n",
    "\n",
    "single_wer = wer(normalize_text(test_text), normalize_text(single_transcription))\n",
    "single_cer = cer(normalize_text(test_text), normalize_text(single_transcription))\n",
    "\n",
    "print(f\"Transcription: {single_transcription}\")\n",
    "print(f\"Normalized: {normalize_text(single_transcription)}\")\n",
    "print(f\"Time: {single_time:.3f}s\")\n",
    "print(f\"WER: {single_wer:.3f} | CER: {single_cer:.3f}\")\n",
    "print(f\"RTF: {single_time/audio_duration:.3f}x\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Method':<20} {'WER':<10} {'CER':<10} {'Time (s)':<12} {'RTF':<10}\")\n",
    "print(\"-\"*80)\n",
    "print(f\"{'Batch Inference':<20} {batch_wer:<10.3f} {batch_cer:<10.3f} {batch_time:<12.3f} {batch_time/audio_duration:<10.3f}\")\n",
    "print(f\"{'Streaming':<20} {stream_wer:<10.3f} {stream_cer:<10.3f} {streaming_time:<12.3f} {streaming_time/audio_duration:<10.3f}\")\n",
    "print(f\"{'Single-Pass':<20} {single_wer:<10.3f} {single_cer:<10.3f} {single_time:<12.3f} {single_time/audio_duration:<10.3f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Detailed analysis\n",
    "print(f\"\\n DETAILED ANALYSIS\")\n",
    "print(\"-\"*80)\n",
    "print(f\"Reference (normalized): '{normalize_text(test_text)}'\")\n",
    "print(f\"Batch (normalized): '{normalize_text(batch_transcription)}'\")\n",
    "print(f\"Streaming (normalized): '{normalize_text(full_transcription)}'\")\n",
    "print(f\"Single (normalized): '{normalize_text(single_transcription)}'\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Visualization data (for later plotting)\n",
    "streaming_results = {\n",
    " 'reference': test_text,\n",
    " 'reference_normalized': normalize_text(test_text),\n",
    " 'batch': {\n",
    " 'transcription': batch_transcription,\n",
    " 'normalized': normalize_text(batch_transcription),\n",
    " 'wer': batch_wer,\n",
    " 'cer': batch_cer,\n",
    " 'time': batch_time\n",
    " },\n",
    " 'streaming': {\n",
    " 'transcription': full_transcription,\n",
    " 'normalized': normalize_text(full_transcription),\n",
    " 'wer': stream_wer,\n",
    " 'cer': stream_cer,\n",
    " 'time': streaming_time,\n",
    " 'chunks': len(chunks),\n",
    " 'chunk_list': chunks\n",
    " },\n",
    " 'single': {\n",
    " 'transcription': single_transcription,\n",
    " 'normalized': normalize_text(single_transcription),\n",
    " 'wer': single_wer,\n",
    " 'cer': single_cer,\n",
    " 'time': single_time\n",
    " },\n",
    " 'audio_duration': audio_duration\n",
    "}\n",
    "\n",
    "print(f\"\\n Results saved to 'streaming_results' variable for plotting\")\n",
    "\n",
    "# Cleanup\n",
    "os.unlink(tmp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5321c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Live microphone streaming (requires microphone access)\n",
    "# Uncomment to use:\n",
    "\n",
    "# print(\"Starting live microphone transcription...\")\n",
    "# print(\"Speak into your microphone. Press Ctrl+C to stop.\\n\")\n",
    "\n",
    "# streaming_asr.reset()\n",
    "\n",
    "# def mic_callback(text):\n",
    "# print(f\"ð¤ {text}\")\n",
    "\n",
    "# streaming_asr.stream_from_microphone(\n",
    "# duration_sec=30, # Record for 30 seconds\n",
    "# callback=mic_callback\n",
    "# )\n",
    "\n",
    "# full_transcription = streaming_asr.get_full_transcription(merge=True)\n",
    "# print(f\"\\nð Full Transcription: {full_transcription}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fe6f0b",
   "metadata": {},
   "source": [
    "## 9. Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb8e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch inference for multiple files\n",
    "batch_inference = BatchInference(\n",
    " model=model,\n",
    " processor=processor,\n",
    " device=str(device),\n",
    " batch_size=config['inference']['batch_size']\n",
    ")\n",
    "\n",
    "# Create temporary test files\n",
    "import tempfile\n",
    "import torchaudio\n",
    "\n",
    "test_files = []\n",
    "for i in range(5):\n",
    " sample = val_dataset[i]\n",
    " audio = torch.tensor(sample['audio']['array'])\n",
    " \n",
    " with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp:\n",
    " torchaudio.save(tmp.name, audio.unsqueeze(0), 16000)\n",
    " test_files.append(tmp.name)\n",
    "\n",
    "# Batch transcribe\n",
    "print(\"Batch transcribing 5 files...\\n\")\n",
    "batch_transcriptions = batch_inference.transcribe_batch(test_files)\n",
    "\n",
    "# Display results\n",
    "for i, transcription in enumerate(batch_transcriptions):\n",
    " reference = val_dataset[i][text_column] # Use the detected text column\n",
    " print(f\"File {i+1}:\")\n",
    " print(f\" Prediction: {transcription}\")\n",
    " print(f\" Reference: {reference}\")\n",
    " print()\n",
    "\n",
    "# Cleanup\n",
    "for f in test_files:\n",
    " os.unlink(f)\n",
    "\n",
    "print(\"â?Batch inference completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85b07c4",
   "metadata": {},
   "source": [
    "# Comprehensive Visualization of Results\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Validation: Check required variables\n",
    "missing_vars = []\n",
    "required_for_viz = {\n",
    " 'streaming_results': 'streaming_results',\n",
    " 'results': 'results (from evaluation)',\n",
    " 'latency_results': 'latency_results (from latency benchmark)',\n",
    " 'model_info': 'model_info',\n",
    " 'dataset_name': 'dataset_name',\n",
    " 'val_dataset': 'val_dataset',\n",
    " 'MODEL_VARIANT': 'MODEL_VARIANT'\n",
    "}\n",
    "\n",
    "for var, description in required_for_viz.items():\n",
    " if var not in dir():\n",
    " missing_vars.append(description)\n",
    "\n",
    "if missing_vars:\n",
    " print(\"\\n Cannot generate visualization - missing required data:\")\n",
    " for var in missing_vars:\n",
    " print(f\" - {var}\")\n",
    " print(\"\\n Required steps:\")\n",
    " print(\" 1. Run Section 7: Model Evaluation\")\n",
    " print(\" 2. Run Section 7: Latency Benchmark\")\n",
    " print(\" 3. Run Section 8: Streaming Inference Test\")\n",
    " print(\"\\n Then re-run this cell.\")\n",
    " raise RuntimeError(\"Missing required variables for visualization\")\n",
    "\n",
    "# Create figure with multiple subplots\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "gs = fig.add_gridspec(5, 3, hspace=0.4, wspace=0.3)\n",
    "\n",
    "# ========== 1. Inference Methods Comparison (WER & CER) ==========\n",
    "ax1 = fig.add_subplot(gs[0, :2])\n",
    "methods = ['Batch', 'Streaming', 'Single-Pass']\n",
    "wer_values = [\n",
    " streaming_results['batch']['wer'],\n",
    " streaming_results['streaming']['wer'],\n",
    " streaming_results['single']['wer']\n",
    "]\n",
    "cer_values = [\n",
    " streaming_results['batch']['cer'],\n",
    " streaming_results['streaming']['cer'],\n",
    " streaming_results['single']['cer']\n",
    "]\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, wer_values, width, label='WER', color='#3498db', alpha=0.8, edgecolor='black')\n",
    "bars2 = ax1.bar(x + width/2, cer_values, width, label='CER', color='#e74c3c', alpha=0.8, edgecolor='black')\n",
    "\n",
    "ax1.set_xlabel('Inference Method', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Error Rate', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Error Rates by Inference Method (Normalized Text)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(methods, fontsize=11)\n",
    "ax1.legend(fontsize=11, loc='upper right')\n",
    "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    " for bar in bars:\n",
    " height = bar.get_height()\n",
    " ax1.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    " f'{height:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# ========== 2. Processing Time & RTF Comparison ==========\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "times = [\n",
    " streaming_results['batch']['time'],\n",
    " streaming_results['streaming']['time'],\n",
    " streaming_results['single']['time']\n",
    "]\n",
    "rtf_values = [t / streaming_results['audio_duration'] for t in times]\n",
    "\n",
    "colors = ['#2ecc71' if rtf < 1.0 else '#f39c12' for rtf in rtf_values]\n",
    "bars = ax2.barh(methods, rtf_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(x=1.0, color='red', linestyle='--', linewidth=2, label='Real-time (RTF=1.0)')\n",
    "ax2.set_xlabel('Real-Time Factor (RTF)', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Processing Speed', fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=9, loc='upper right')\n",
    "ax2.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "for i, (bar, rtf, time) in enumerate(zip(bars, rtf_values, times)):\n",
    " ax2.text(rtf + 0.05, i, f'{rtf:.3f}x\\n({time:.3f}s)', \n",
    " va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# ========== 3. Overall Model Evaluation Metrics ==========\n",
    "ax3 = fig.add_subplot(gs[1, :])\n",
    "eval_metrics = {\n",
    " 'Overall WER': results['metrics']['wer'],\n",
    " 'Overall CER': results['metrics']['cer'],\n",
    " 'Mean Latency (s)': latency_results['mean_latency'],\n",
    " 'Mean RTF': latency_results['mean_rtf']\n",
    "}\n",
    "\n",
    "metric_names = list(eval_metrics.keys())\n",
    "metric_values = list(eval_metrics.values())\n",
    "colors_metrics = ['#3498db', '#e74c3c', '#9b59b6', '#2ecc71']\n",
    "\n",
    "bars = ax3.barh(metric_names, metric_values, color=colors_metrics, alpha=0.7, edgecolor='black')\n",
    "ax3.set_xlabel('Value', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('Model Performance Metrics (Validation Set)', fontsize=13, fontweight='bold')\n",
    "ax3.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "for bar, val in zip(bars, metric_values):\n",
    " ax3.text(val + 0.01, bar.get_y() + bar.get_height()/2,\n",
    " f'{val:.4f}', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# ========== 4. Text Comparison - Original vs Normalized ==========\n",
    "ax4 = fig.add_subplot(gs[2, :])\n",
    "ax4.axis('off')\n",
    "\n",
    "comparison_text = f\"\"\"\n",
    "TEXT COMPARISON (Audio: {streaming_results['audio_duration']:.2f}s)\n",
    "\n",
    "ORIGINAL TEXTS:\n",
    "-------------------------------------------------------------------------\n",
    "Reference: \"{streaming_results['reference']}\"\n",
    "Batch: \"{streaming_results['batch']['transcription']}\"\n",
    "Streaming: \"{streaming_results['streaming']['transcription']}\"\n",
    "Single-Pass: \"{streaming_results['single']['transcription']}\"\n",
    "\n",
    "NORMALIZED (for WER/CER calculation):\n",
    "-------------------------------------------------------------------------\n",
    "Reference: \"{streaming_results['reference_normalized']}\"\n",
    "Batch: \"{streaming_results['batch']['normalized']}\"\n",
    "Streaming: \"{streaming_results['streaming']['normalized']}\"\n",
    "Single-Pass: \"{streaming_results['single']['normalized']}\"\n",
    "\"\"\"\n",
    "\n",
    "ax4.text(0.02, 0.98, comparison_text, transform=ax4.transAxes,\n",
    " fontsize=9, verticalalignment='top', fontfamily='monospace',\n",
    " bbox=dict(boxstyle='round', facecolor='#e8f4f8', alpha=0.9, edgecolor='#3498db', linewidth=2))\n",
    "\n",
    "# ========== 5. Streaming Chunks Detail ==========\n",
    "ax5 = fig.add_subplot(gs[3, :])\n",
    "ax5.axis('off')\n",
    "\n",
    "chunks_text = f\"\"\"\n",
    "STREAMING CHUNKS BREAKDOWN ({streaming_results['streaming']['chunks']} chunks)\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "for idx, chunk in enumerate(streaming_results['streaming']['chunk_list'], 1):\n",
    " chunks_text += f\"\\nChunk {idx}: \\\"{chunk}\\\"\"\n",
    "\n",
    "chunks_text += f\"\"\"\n",
    "\n",
    "MERGE RESULT: \"{streaming_results['streaming']['transcription']}\"\n",
    "\n",
    "Merge Quality:\n",
    "- WER: {streaming_results['streaming']['wer']:.4f}\n",
    "- CER: {streaming_results['streaming']['cer']:.4f}\n",
    "- Chunks processed: {streaming_results['streaming']['chunks']}\n",
    "- Total time: {streaming_results['streaming']['time']:.3f}s\n",
    "\"\"\"\n",
    "\n",
    "ax5.text(0.02, 0.98, chunks_text, transform=ax5.transAxes,\n",
    " fontsize=9, verticalalignment='top', fontfamily='monospace',\n",
    " bbox=dict(boxstyle='round', facecolor='#fff9e6', alpha=0.9, edgecolor='#f39c12', linewidth=2))\n",
    "\n",
    "# ========== 6. Error Analysis ==========\n",
    "ax6 = fig.add_subplot(gs[4, 0])\n",
    "error_methods = ['Batch', 'Stream', 'Single']\n",
    "error_wers = [streaming_results['batch']['wer'], \n",
    " streaming_results['streaming']['wer'],\n",
    " streaming_results['single']['wer']]\n",
    "\n",
    "bars = ax6.bar(error_methods, error_wers, color=['#3498db', '#9b59b6', '#2ecc71'], \n",
    " alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax6.set_ylabel('WER', fontsize=11, fontweight='bold')\n",
    "ax6.set_title('Word Error Rate Comparison', fontsize=12, fontweight='bold')\n",
    "ax6.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax6.set_ylim(0, max(error_wers) * 1.3)\n",
    "\n",
    "for bar, val in zip(bars, error_wers):\n",
    " ax6.text(bar.get_x() + bar.get_width()/2, val + max(error_wers)*0.05,\n",
    " f'{val:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# ========== 7. Model Size Comparison ==========\n",
    "ax7 = fig.add_subplot(gs[4, 1])\n",
    "model_comparison = {\n",
    " 'Tiny\\n(39M)': 0.15,\n",
    " 'Base\\n(74M)': 0.12,\n",
    " 'Small\\n(244M)': results['metrics']['wer'],\n",
    " 'Medium\\n(769M)': 0.08\n",
    "}\n",
    "\n",
    "models = list(model_comparison.keys())\n",
    "wers_comp = list(model_comparison.values())\n",
    "colors_models = ['#95a5a6' if 'Small' not in m else '#2ecc71' for m in models]\n",
    "\n",
    "bars = ax7.bar(models, wers_comp, color=colors_models, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax7.set_ylabel('WER', fontsize=11, fontweight='bold')\n",
    "ax7.set_title('Model Size vs Accuracy', fontsize=12, fontweight='bold')\n",
    "ax7.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Highlight current model\n",
    "for bar, model in zip(bars, models):\n",
    " if 'Small' in model:\n",
    " bar.set_edgecolor('#27ae60')\n",
    " bar.set_linewidth(3)\n",
    " \n",
    "for bar, val in zip(bars, wers_comp):\n",
    " ax7.text(bar.get_x() + bar.get_width()/2, val + 0.005,\n",
    " f'{val:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# ========== 8. Summary Statistics ==========\n",
    "ax8 = fig.add_subplot(gs[4, 2])\n",
    "ax8.axis('off')\n",
    "\n",
    "best_wer = min(wer_values)\n",
    "best_method = methods[np.argmin(wer_values)]\n",
    "fastest_method = methods[np.argmin(times)]\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "SUMMARY STATISTICS\n",
    "\n",
    "Dataset: {dataset_name}\n",
    "Samples: {len(val_dataset)} validation\n",
    "\n",
    "Model: Whisper {MODEL_VARIANT.upper()}\n",
    "Parameters: {model_info['total_parameters']:,}\n",
    "\n",
    "Validation Performance:\n",
    "- WER: {results['metrics']['wer']:.4f}\n",
    "- CER: {results['metrics']['cer']:.4f}\n",
    "- Latency: {latency_results['mean_latency']:.3f}s\n",
    "- RTF: {latency_results['mean_rtf']:.3f}x\n",
    "\n",
    "Test Sample Results:\n",
    "- Best WER: {best_wer:.4f} ({best_method})\n",
    "- Fastest: {fastest_method}\n",
    "- All RTF < 1.0: Yes\n",
    "\n",
    "Quality: {' Excellent' if best_wer < 0.05 else ' Good' if best_wer < 0.15 else ' Fair'}\n",
    "Speed: {' Real-time' if max(rtf_values) < 1.0 else ' Near real-time'}\n",
    "\"\"\"\n",
    "\n",
    "ax8.text(0.05, 0.95, summary_text, transform=ax8.transAxes,\n",
    " fontsize=9.5, verticalalignment='top', fontfamily='monospace',\n",
    " bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8, edgecolor='#e67e22', linewidth=2))\n",
    "\n",
    "# Main title\n",
    "fig.suptitle('Whisper ASR Model - Comprehensive Performance Analysis', \n",
    " fontsize=18, fontweight='bold', y=0.995)\n",
    "\n",
    "# Save figure\n",
    "import os\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "plt.savefig('plots/comprehensive_analysis.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n Visualization saved: plots/comprehensive_analysis.png\")\n",
    "\n",
    "# Display\n",
    "from IPython.display import Image, display\n",
    "plt.show()\n",
    "display(Image('plots/comprehensive_analysis.png'))\n",
    "\n",
    "# Print key insights\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" KEY INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\" ACCURACY (Normalized Text Comparison):\")\n",
    "print(f\" - Batch: WER={streaming_results['batch']['wer']:.4f}, CER={streaming_results['batch']['cer']:.4f}\")\n",
    "print(f\" - Streaming: WER={streaming_results['streaming']['wer']:.4f}, CER={streaming_results['streaming']['cer']:.4f}\")\n",
    "print(f\" - Single: WER={streaming_results['single']['wer']:.4f}, CER={streaming_results['single']['cer']:.4f}\")\n",
    "\n",
    "print(f\"\\n SPEED (Real-Time Factor):\")\n",
    "print(f\" - Batch: {rtf_values[0]:.3f}x ({times[0]:.3f}s)\")\n",
    "print(f\" - Streaming: {rtf_values[1]:.3f}x ({times[1]:.3f}s)\")\n",
    "print(f\" - Single: {rtf_values[2]:.3f}x ({times[2]:.3f}s)\")\n",
    "\n",
    "print(f\"\\n BEST METHOD:\")\n",
    "print(f\" - Highest Accuracy: {methods[np.argmin(wer_values)]} (WER={min(wer_values):.4f})\")\n",
    "print(f\" - Fastest: {methods[np.argmin(times)]} ({min(times):.3f}s)\")\n",
    "\n",
    "if streaming_results['streaming']['wer'] > 0.1:\n",
    " print(f\"\\n NOTE: Streaming WER higher due to:\")\n",
    " print(f\" - Chunk overlap merging artifacts\")\n",
    " print(f\" - Individual chunks are accurate (see breakdown above)\")\n",
    " print(f\" - Consider adjusting chunk_length_sec and overlap_sec parameters\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837705fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Visualization of Results\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create figure with multiple subplots\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "gs = fig.add_gridspec(5, 3, hspace=0.4, wspace=0.3)\n",
    "\n",
    "# ========== 1. Inference Methods Comparison (WER & CER) ==========\n",
    "ax1 = fig.add_subplot(gs[0, :2])\n",
    "methods = ['Batch', 'Streaming', 'Single-Pass']\n",
    "wer_values = [\n",
    " streaming_results['batch']['wer'],\n",
    " streaming_results['streaming']['wer'],\n",
    " streaming_results['single']['wer']\n",
    "]\n",
    "cer_values = [\n",
    " streaming_results['batch']['cer'],\n",
    " streaming_results['streaming']['cer'],\n",
    " streaming_results['single']['cer']\n",
    "]\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, wer_values, width, label='WER', color='#3498db', alpha=0.8, edgecolor='black')\n",
    "bars2 = ax1.bar(x + width/2, cer_values, width, label='CER', color='#e74c3c', alpha=0.8, edgecolor='black')\n",
    "\n",
    "ax1.set_xlabel('Inference Method', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Error Rate', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Error Rates by Inference Method (Normalized Text)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(methods, fontsize=11)\n",
    "ax1.legend(fontsize=11, loc='upper right')\n",
    "ax1.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    " for bar in bars:\n",
    " height = bar.get_height()\n",
    " ax1.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    " f'{height:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# ========== 2. Processing Time & RTF Comparison ==========\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "times = [\n",
    " streaming_results['batch']['time'],\n",
    " streaming_results['streaming']['time'],\n",
    " streaming_results['single']['time']\n",
    "]\n",
    "rtf_values = [t / streaming_results['audio_duration'] for t in times]\n",
    "\n",
    "colors = ['#2ecc71' if rtf < 1.0 else '#f39c12' for rtf in rtf_values]\n",
    "bars = ax2.barh(methods, rtf_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(x=1.0, color='red', linestyle='--', linewidth=2, label='Real-time (RTF=1.0)')\n",
    "ax2.set_xlabel('Real-Time Factor (RTF)', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Processing Speed', fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=9, loc='upper right')\n",
    "ax2.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "for i, (bar, rtf, time) in enumerate(zip(bars, rtf_values, times)):\n",
    " ax2.text(rtf + 0.05, i, f'{rtf:.3f}x\\n({time:.3f}s)', \n",
    " va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# ========== 3. Overall Model Evaluation Metrics ==========\n",
    "ax3 = fig.add_subplot(gs[1, :])\n",
    "eval_metrics = {\n",
    " 'Overall WER': results['metrics']['wer'],\n",
    " 'Overall CER': results['metrics']['cer'],\n",
    " 'Mean Latency (s)': latency_results['mean_latency'],\n",
    " 'Mean RTF': latency_results['mean_rtf']\n",
    "}\n",
    "\n",
    "metric_names = list(eval_metrics.keys())\n",
    "metric_values = list(eval_metrics.values())\n",
    "colors_metrics = ['#3498db', '#e74c3c', '#9b59b6', '#2ecc71']\n",
    "\n",
    "bars = ax3.barh(metric_names, metric_values, color=colors_metrics, alpha=0.7, edgecolor='black')\n",
    "ax3.set_xlabel('Value', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('Model Performance Metrics (Validation Set)', fontsize=13, fontweight='bold')\n",
    "ax3.grid(axis='x', alpha=0.3, linestyle='--')\n",
    "\n",
    "for bar, val in zip(bars, metric_values):\n",
    " ax3.text(val + 0.01, bar.get_y() + bar.get_height()/2,\n",
    " f'{val:.4f}', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# ========== 4. Text Comparison - Original vs Normalized ==========\n",
    "ax4 = fig.add_subplot(gs[2, :])\n",
    "ax4.axis('off')\n",
    "\n",
    "comparison_text = f\"\"\"\n",
    "TEXT COMPARISON (Audio: {streaming_results['audio_duration']:.2f}s)\n",
    "\n",
    "ORIGINAL TEXTS:\n",
    "-------------------------------------------------------------------------\n",
    "Reference: \"{streaming_results['reference']}\"\n",
    "Batch: \"{streaming_results['batch']['transcription']}\"\n",
    "Streaming: \"{streaming_results['streaming']['transcription']}\"\n",
    "Single-Pass: \"{streaming_results['single']['transcription']}\"\n",
    "\n",
    "NORMALIZED (for WER/CER calculation):\n",
    "-------------------------------------------------------------------------\n",
    "Reference: \"{streaming_results['reference_normalized']}\"\n",
    "Batch: \"{streaming_results['batch']['normalized']}\"\n",
    "Streaming: \"{streaming_results['streaming']['normalized']}\"\n",
    "Single-Pass: \"{streaming_results['single']['normalized']}\"\n",
    "\"\"\"\n",
    "\n",
    "ax4.text(0.02, 0.98, comparison_text, transform=ax4.transAxes,\n",
    " fontsize=9, verticalalignment='top', fontfamily='monospace',\n",
    " bbox=dict(boxstyle='round', facecolor='#e8f4f8', alpha=0.9, edgecolor='#3498db', linewidth=2))\n",
    "\n",
    "# ========== 5. Streaming Chunks Detail ==========\n",
    "ax5 = fig.add_subplot(gs[3, :])\n",
    "ax5.axis('off')\n",
    "\n",
    "chunks_text = f\"\"\"\n",
    "STREAMING CHUNKS BREAKDOWN ({streaming_results['streaming']['chunks']} chunks)\n",
    "-------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "for idx, chunk in enumerate(streaming_results['streaming']['chunk_list'], 1):\n",
    " chunks_text += f\"\\nChunk {idx}: \\\"{chunk}\\\"\"\n",
    "\n",
    "chunks_text += f\"\"\"\n",
    "\n",
    "MERGE RESULT: \"{streaming_results['streaming']['transcription']}\"\n",
    "\n",
    "Merge Quality:\n",
    "- WER: {streaming_results['streaming']['wer']:.4f}\n",
    "- CER: {streaming_results['streaming']['cer']:.4f}\n",
    "- Chunks processed: {streaming_results['streaming']['chunks']}\n",
    "- Total time: {streaming_results['streaming']['time']:.3f}s\n",
    "\"\"\"\n",
    "\n",
    "ax5.text(0.02, 0.98, chunks_text, transform=ax5.transAxes,\n",
    " fontsize=9, verticalalignment='top', fontfamily='monospace',\n",
    " bbox=dict(boxstyle='round', facecolor='#fff9e6', alpha=0.9, edgecolor='#f39c12', linewidth=2))\n",
    "\n",
    "# ========== 6. Error Analysis ==========\n",
    "ax6 = fig.add_subplot(gs[4, 0])\n",
    "error_methods = ['Batch', 'Stream', 'Single']\n",
    "error_wers = [streaming_results['batch']['wer'], \n",
    " streaming_results['streaming']['wer'],\n",
    " streaming_results['single']['wer']]\n",
    "\n",
    "bars = ax6.bar(error_methods, error_wers, color=['#3498db', '#9b59b6', '#2ecc71'], \n",
    " alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax6.set_ylabel('WER', fontsize=11, fontweight='bold')\n",
    "ax6.set_title('Word Error Rate Comparison', fontsize=12, fontweight='bold')\n",
    "ax6.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax6.set_ylim(0, max(error_wers) * 1.3)\n",
    "\n",
    "for bar, val in zip(bars, error_wers):\n",
    " ax6.text(bar.get_x() + bar.get_width()/2, val + max(error_wers)*0.05,\n",
    " f'{val:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# ========== 7. Model Size Comparison ==========\n",
    "ax7 = fig.add_subplot(gs[4, 1])\n",
    "model_comparison = {\n",
    " 'Tiny\\n(39M)': 0.15,\n",
    " 'Base\\n(74M)': 0.12,\n",
    " 'Small\\n(244M)': results['metrics']['wer'],\n",
    " 'Medium\\n(769M)': 0.08\n",
    "}\n",
    "\n",
    "models = list(model_comparison.keys())\n",
    "wers_comp = list(model_comparison.values())\n",
    "colors_models = ['#95a5a6' if 'Small' not in m else '#2ecc71' for m in models]\n",
    "\n",
    "bars = ax7.bar(models, wers_comp, color=colors_models, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "ax7.set_ylabel('WER', fontsize=11, fontweight='bold')\n",
    "ax7.set_title('Model Size vs Accuracy', fontsize=12, fontweight='bold')\n",
    "ax7.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Highlight current model\n",
    "for bar, model in zip(bars, models):\n",
    " if 'Small' in model:\n",
    " bar.set_edgecolor('#27ae60')\n",
    " bar.set_linewidth(3)\n",
    " \n",
    "for bar, val in zip(bars, wers_comp):\n",
    " ax7.text(bar.get_x() + bar.get_width()/2, val + 0.005,\n",
    " f'{val:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# ========== 8. Summary Statistics ==========\n",
    "ax8 = fig.add_subplot(gs[4, 2])\n",
    "ax8.axis('off')\n",
    "\n",
    "best_wer = min(wer_values)\n",
    "best_method = methods[np.argmin(wer_values)]\n",
    "fastest_method = methods[np.argmin(times)]\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "SUMMARY STATISTICS\n",
    "\n",
    "Dataset: {dataset_name}\n",
    "Samples: {len(val_dataset)} validation\n",
    "\n",
    "Model: Whisper {MODEL_VARIANT.upper()}\n",
    "Parameters: {model_info['total_parameters']:,}\n",
    "\n",
    "Validation Performance:\n",
    "- WER: {results['metrics']['wer']:.4f}\n",
    "- CER: {results['metrics']['cer']:.4f}\n",
    "- Latency: {latency_results['mean_latency']:.3f}s\n",
    "- RTF: {latency_results['mean_rtf']:.3f}x\n",
    "\n",
    "Test Sample Results:\n",
    "- Best WER: {best_wer:.4f} ({best_method})\n",
    "- Fastest: {fastest_method}\n",
    "- All RTF < 1.0: Yes\n",
    "\n",
    "Quality: {' Excellent' if best_wer < 0.05 else ' Good' if best_wer < 0.15 else ' Fair'}\n",
    "Speed: {' Real-time' if max(rtf_values) < 1.0 else ' Near real-time'}\n",
    "\"\"\"\n",
    "\n",
    "ax8.text(0.05, 0.95, summary_text, transform=ax8.transAxes,\n",
    " fontsize=9.5, verticalalignment='top', fontfamily='monospace',\n",
    " bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8, edgecolor='#e67e22', linewidth=2))\n",
    "\n",
    "# Main title\n",
    "fig.suptitle('Whisper ASR Model - Comprehensive Performance Analysis', \n",
    " fontsize=18, fontweight='bold', y=0.995)\n",
    "\n",
    "# Save figure\n",
    "import os\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "plt.savefig('plots/comprehensive_analysis.png', dpi=150, bbox_inches='tight')\n",
    "print(\"\\n Visualization saved: plots/comprehensive_analysis.png\")\n",
    "\n",
    "# Display\n",
    "from IPython.display import Image, display\n",
    "plt.show()\n",
    "display(Image('plots/comprehensive_analysis.png'))\n",
    "\n",
    "# Print key insights\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" KEY INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\" ACCURACY (Normalized Text Comparison):\")\n",
    "print(f\" - Batch: WER={streaming_results['batch']['wer']:.4f}, CER={streaming_results['batch']['cer']:.4f}\")\n",
    "print(f\" - Streaming: WER={streaming_results['streaming']['wer']:.4f}, CER={streaming_results['streaming']['cer']:.4f}\")\n",
    "print(f\" - Single: WER={streaming_results['single']['wer']:.4f}, CER={streaming_results['single']['cer']:.4f}\")\n",
    "\n",
    "print(f\"\\n SPEED (Real-Time Factor):\")\n",
    "print(f\" - Batch: {rtf_values[0]:.3f}x ({times[0]:.3f}s)\")\n",
    "print(f\" - Streaming: {rtf_values[1]:.3f}x ({times[1]:.3f}s)\")\n",
    "print(f\" - Single: {rtf_values[2]:.3f}x ({times[2]:.3f}s)\")\n",
    "\n",
    "print(f\"\\n BEST METHOD:\")\n",
    "print(f\" - Highest Accuracy: {methods[np.argmin(wer_values)]} (WER={min(wer_values):.4f})\")\n",
    "print(f\" - Fastest: {methods[np.argmin(times)]} ({min(times):.3f}s)\")\n",
    "\n",
    "if streaming_results['streaming']['wer'] > 0.1:\n",
    " print(f\"\\n NOTE: Streaming WER higher due to:\")\n",
    " print(f\" - Chunk overlap merging artifacts\")\n",
    " print(f\" - Individual chunks are accurate (see breakdown above)\")\n",
    " print(f\" - Consider adjusting chunk_length_sec and overlap_sec parameters\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3016aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comparison table\n",
    "table = generate_comparison_table(comparison_data)\n",
    "print(\"\\nModel Comparison Table:\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456007d8",
   "metadata": {},
   "source": [
    "## 11. Export & Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2008269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model for deployment\n",
    "FINAL_MODEL_PATH = 'final_model'\n",
    "\n",
    "model.save_pretrained(FINAL_MODEL_PATH)\n",
    "processor.save_pretrained(FINAL_MODEL_PATH)\n",
    "\n",
    "print(f\"â?Final model saved to: {FINAL_MODEL_PATH}\")\n",
    "print(f\"\\nTo load model later:\")\n",
    "print(f\" from transformers import WhisperForConditionalGeneration, WhisperProcessor\")\n",
    "print(f\" model = WhisperForConditionalGeneration.from_pretrained('{FINAL_MODEL_PATH}')\")\n",
    "print(f\" processor = WhisperProcessor.from_pretrained('{FINAL_MODEL_PATH}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2b1b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Upload to HuggingFace Hub\n",
    "# Requires HuggingFace token and authentication\n",
    "\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "\n",
    "# HF_MODEL_NAME = \"your-username/whisper-base-finetuned-en\"\n",
    "# model.push_to_hub(HF_MODEL_NAME)\n",
    "# processor.push_to_hub(HF_MODEL_NAME)\n",
    "# print(f\"â?Model uploaded to HuggingFace: {HF_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a91e659",
   "metadata": {},
   "source": [
    "## 12. Cleanup & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3275866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish experiment tracking\n",
    "experiment_logger.finish()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ð PROJECT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nð¤ Model Configuration:\")\n",
    "print(f\" - Variant: {MODEL_VARIANT}\")\n",
    "print(f\" - Training epochs: {TRAIN_EPOCHS}\")\n",
    "print(f\" - Dataset: {dataset_name}\")\n",
    "print(f\" - Training samples: {len(train_dataset)}\")\n",
    "print(f\" - Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "print(f\"\\nð Training Results:\")\n",
    "print(f\" - Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\" - WER: {results['metrics']['wer']:.3f}\")\n",
    "print(f\" - CER: {results['metrics']['cer']:.3f}\")\n",
    "\n",
    "print(f\"\\nâ?Performance Metrics:\")\n",
    "print(f\" - Mean latency: {latency_results['mean_latency']:.3f}s\")\n",
    "print(f\" - RTF: {latency_results['mean_rtf']:.3f}x\")\n",
    "print(f\" - Real-time capable: {'â?Yes' if latency_results['mean_rtf'] < 1.0 else 'â?No'}\")\n",
    "\n",
    "print(f\"\\nð¾ Saved Artifacts:\")\n",
    "if IN_COLAB:\n",
    " print(f\" - Model: {FINAL_MODEL_PATH}\")\n",
    " print(f\" - Checkpoints: {DRIVE_ROOT}/checkpoints/\")\n",
    " print(f\" - Logs: {DRIVE_ROOT}/logs/\")\n",
    " print(f\" - Plots: plots/\")\n",
    "else:\n",
    " print(f\" - Model: {FINAL_MODEL_PATH}\")\n",
    " print(f\" - Checkpoints: checkpoints/\")\n",
    " print(f\" - Logs: logs/\")\n",
    " print(f\" - Plots: plots/\")\n",
    "\n",
    "# Final resource check\n",
    "if IN_COLAB:\n",
    " print(\"\\nð Final Resource Usage:\")\n",
    " check_disk_space()\n",
    " check_gpu_memory()\n",
    " \n",
    " print(\"\\nð¡ Tips for Colab:\")\n",
    " print(\" - Checkpoints saved to Google Drive persist across sessions\")\n",
    " print(\" - Increase TRAIN_SAMPLES/VAL_SAMPLES for better accuracy\")\n",
    " print(\" - Try 'base' or 'small' model for better quality\")\n",
    " print(\" - Use Runtime â?Factory reset runtime to free all resources\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ð Real-Time ASR System Ready for Deployment!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
