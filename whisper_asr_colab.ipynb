{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "995862c2",
   "metadata": {},
   "source": [
    "# Real-Time Multilingual ASR with Whisper\n",
    "\n",
    "This notebook implements a production-ready, real-time speech recognition system using OpenAI's Whisper models.\n",
    "\n",
    "**Features:**\n",
    "- Data preparation with augmentation\n",
    "- Model fine-tuning with multiple variants\n",
    "- Comprehensive evaluation (WER, CER, latency)\n",
    "- Real-time streaming inference\n",
    "- Full MLOps best practices (versioning, logging, reproducibility)\n",
    "\n",
    "**Author:** COMP3057 Project  \n",
    "**Version:** 1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c25953",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23841875",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Colab Configuration Guide\n",
    "\n",
    "**Resource Constraints:** A100 GPU (~40GB), 220GB Disk\n",
    "\n",
    "**Quick Start Options:**\n",
    "\n",
    "| Profile | Model | Samples | Epochs | Time | Disk | Quality |\n",
    "|---------|-------|---------|--------|------|------|---------|\n",
    "| **Fast Demo** | tiny | 50/10 | 1 | ~5min | ~2GB | Basic |\n",
    "| **Balanced** | base | 200/40 | 2 | ~20min | ~5GB | Good |\n",
    "| **Best Quality** | small | 500/100 | 3 | ~60min | ~10GB | Better |\n",
    "\n",
    "**Adjustable Parameters (in cells below):**\n",
    "- `TRAIN_SAMPLES` / `VAL_SAMPLES` - Dataset size\n",
    "- `MODEL_VARIANT` - Model quality (tiny/base/small)\n",
    "- `TRAIN_EPOCHS` - Training duration\n",
    "\n",
    "**Tips:**\n",
    "- Start with **Fast Demo** to verify everything works\n",
    "- Increase resources gradually if you have time\n",
    "- Checkpoints auto-save to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f24b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment and clone project\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Check if running in Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"üîß Running in Google Colab\")\n",
    "    \n",
    "    # Mount Google Drive for saving checkpoints and logs\n",
    "    from google.colab import drive\n",
    "    if not os.path.exists('/content/drive'):\n",
    "        print(\"üìÅ Mounting Google Drive...\")\n",
    "        drive.mount('/content/drive', force_remount=False)\n",
    "        print(\"‚úì Drive mounted at /content/drive\")\n",
    "    else:\n",
    "        print(\"‚úì Drive already mounted\")\n",
    "    \n",
    "    # --- Project Setup and Path Configuration ---\n",
    "    PROJECT_DIR = 'COMP3057_Project'\n",
    "    \n",
    "    # Clone repository if not exists\n",
    "    if not os.path.exists(PROJECT_DIR):\n",
    "        print(\"\\nüì¶ Cloning repository from GitHub...\")\n",
    "        !git clone https://github.com/jimmy00415/COMP3057_Project.git\n",
    "        print(\"‚úì Repository cloned\")\n",
    "    else:\n",
    "        print(\"‚úì Repository already exists\")\n",
    "    \n",
    "    # Change to project directory\n",
    "    os.chdir(PROJECT_DIR)\n",
    "    print(f\"‚úì Working directory: {os.getcwd()}\")\n",
    "\n",
    "    # Add project root to Python path\n",
    "    # This is the crucial step to solve ModuleNotFoundError\n",
    "    if os.getcwd() not in sys.path:\n",
    "        sys.path.insert(0, os.getcwd())\n",
    "        print(f\"‚úì Added '{os.getcwd()}' to Python path\")\n",
    "    \n",
    "    # Install dependencies IMMEDIATELY after cloning\n",
    "    print(\"\\nüì¶ Installing dependencies...\")\n",
    "    !pip install -q -r requirements.txt\n",
    "    !pip install -q sounddevice\n",
    "    print(\"‚úì Dependencies installed\")\n",
    "\n",
    "else:\n",
    "    print(\"üíª Running locally\")\n",
    "    print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "gpu_available = torch.cuda.is_available()\n",
    "if gpu_available:\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"\\nüéÆ GPU: {gpu_name}\")\n",
    "    print(f\"   Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: No GPU detected! Training will be very slow.\")\n",
    "    print(\"   Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU (T4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dcae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab Resource Optimization & Monitoring\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "# Re-check if in Colab (in case this cell is run independently)\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "def check_disk_space():\n",
    "    \"\"\"Check available disk space.\"\"\"\n",
    "    total, used, free = shutil.disk_usage(\"/\")\n",
    "    print(f\"üíæ Disk Space:\")\n",
    "    print(f\"   Total: {total // (2**30)} GB\")\n",
    "    print(f\"   Used: {used // (2**30)} GB\")\n",
    "    print(f\"   Free: {free // (2**30)} GB\")\n",
    "    return free // (2**30)\n",
    "\n",
    "def check_gpu_memory():\n",
    "    \"\"\"Check GPU memory usage.\"\"\"\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"\\nüéÆ GPU Memory:\")\n",
    "        print(f\"   Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"   Reserved: {reserved:.2f} GB\")\n",
    "        print(f\"   Total: {total:.2f} GB\")\n",
    "        print(f\"   Free: {total - reserved:.2f} GB\")\n",
    "\n",
    "def cleanup_cache():\n",
    "    \"\"\"Clear unnecessary cache to free disk space.\"\"\"\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    print(\"‚úì Cache cleared\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Setup Google Drive paths for checkpoints\n",
    "    DRIVE_ROOT = '/content/drive/MyDrive/COMP3057_ASR'\n",
    "    import os\n",
    "    os.makedirs(DRIVE_ROOT, exist_ok=True)\n",
    "    os.makedirs(f'{DRIVE_ROOT}/checkpoints', exist_ok=True)\n",
    "    os.makedirs(f'{DRIVE_ROOT}/logs', exist_ok=True)\n",
    "    print(f\"\\nüìÅ Google Drive storage: {DRIVE_ROOT}\")\n",
    "    \n",
    "    # Create symlinks to save to Drive instead of local disk\n",
    "    if os.path.exists('checkpoints') and not os.path.islink('checkpoints'):\n",
    "        shutil.rmtree('checkpoints')\n",
    "    if not os.path.exists('checkpoints'):\n",
    "        os.symlink(f'{DRIVE_ROOT}/checkpoints', 'checkpoints')\n",
    "        print(\"‚úì Checkpoints will be saved to Google Drive\")\n",
    "    \n",
    "    if os.path.exists('logs') and not os.path.islink('logs'):\n",
    "        shutil.rmtree('logs')\n",
    "    if not os.path.exists('logs'):\n",
    "        os.symlink(f'{DRIVE_ROOT}/logs', 'logs')\n",
    "        print(\"‚úì Logs will be saved to Google Drive\")\n",
    "    \n",
    "    # Check initial resources\n",
    "    print(\"\\nüìä Initial Resource Check:\")\n",
    "    free_disk = check_disk_space()\n",
    "    check_gpu_memory()\n",
    "    \n",
    "    if free_disk < 50:\n",
    "        print(\"\\n‚ö†Ô∏è  WARNING: Low disk space! Consider:\")\n",
    "        print(\"   1. Use smaller dataset (already optimized)\")\n",
    "        print(\"   2. Use 'tiny' or 'base' model variant\")\n",
    "        print(\"   3. Reduce save_steps to save fewer checkpoints\")\n",
    "else:\n",
    "    # Define cleanup_cache for local use\n",
    "    import torch\n",
    "    def cleanup_cache():\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        import gc\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce2e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Import project modules\n",
    "from src.utils import (\n",
    "    load_config,\n",
    "    set_seed,\n",
    "    setup_logging,\n",
    "    get_device,\n",
    "    ExperimentLogger,\n",
    "    DataVersionManager,\n",
    "    ModelRegistry\n",
    ")\n",
    "\n",
    "from src.data import (\n",
    "    AudioPreprocessor,\n",
    "    VoiceActivityDetector,\n",
    "    AudioAugmenter,\n",
    "    WhisperDataset,\n",
    "    prepare_datasets,\n",
    "    create_dataloaders\n",
    ")\n",
    "\n",
    "from src.models import (\n",
    "    WhisperModelManager,\n",
    "    compare_models\n",
    ")\n",
    "\n",
    "from src.training import WhisperTrainer\n",
    "\n",
    "from src.evaluation import (\n",
    "    ModelEvaluator,\n",
    "    LatencyBenchmark,\n",
    "    TrainingVisualizer,\n",
    "    EvaluationVisualizer,\n",
    "    generate_comparison_table\n",
    ")\n",
    "\n",
    "from src.inference import (\n",
    "    StreamingASR,\n",
    "    BatchInference\n",
    ")\n",
    "\n",
    "print(\"‚úì All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172c29b4",
   "metadata": {},
   "source": [
    "## 2. Configuration & Reproducibility Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55abd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config('config.yaml')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "set_seed(config['project']['seed'])\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_logging('INFO', 'logs/training.log')\n",
    "\n",
    "# Get device\n",
    "device = get_device(config['project']['device'])\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize experiment tracking (choose: wandb, mlflow, or tensorboard)\n",
    "experiment_logger = ExperimentLogger(\n",
    "    backend=config['mlops']['experiment_tracking']['backend'],\n",
    "    project_name=config['mlops']['experiment_tracking']['project_name'],\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Initialize versioning\n",
    "data_version_manager = DataVersionManager(\n",
    "    config['mlops']['versioning']['data_version_file']\n",
    ")\n",
    "model_registry = ModelRegistry(\n",
    "    config['mlops']['versioning']['model_registry']\n",
    ")\n",
    "\n",
    "print(f\"‚úì Configuration loaded\")\n",
    "print(f\"  - Seed: {config['project']['seed']}\")\n",
    "print(f\"  - Device: {device}\")\n",
    "print(f\"  - Tracking: {config['mlops']['experiment_tracking']['backend']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f62e685",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6286d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing utilities\n",
    "audio_preprocessor = AudioPreprocessor(\n",
    "    target_sr=config['data']['sampling_rate'],\n",
    "    normalize=True\n",
    ")\n",
    "\n",
    "vad = VoiceActivityDetector(\n",
    "    threshold=config['data']['vad_threshold']\n",
    ")\n",
    "\n",
    "augmenter = AudioAugmenter(\n",
    "    speed_perturbation=config['data']['augmentation']['speed_perturbation'],\n",
    "    pitch_shift_semitones=config['data']['augmentation']['pitch_shift_semitones'],\n",
    "    background_noise_prob=config['data']['augmentation']['background_noise_prob']\n",
    ") if config['data']['augmentation']['enabled'] else None\n",
    "\n",
    "print(\"‚úì Preprocessing utilities initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1acff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "# Using small subsets optimized for Colab's disk/memory constraints\n",
    "# For production with more resources, increase the sample counts\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Configuration for Colab\n",
    "TRAIN_SAMPLES = 50  # Small for quick demo, increase to 500-1000 for better results\n",
    "VAL_SAMPLES = 10    # Small for quick demo, increase to 100-200 for better results\n",
    "\n",
    "print(f\"Loading dataset with {TRAIN_SAMPLES} train + {VAL_SAMPLES} val samples...\")\n",
    "print(\"(Optimized for Colab - increase samples for production)\")\n",
    "\n",
    "try:\n",
    "    # Try minds14 first - smaller and faster to download\n",
    "    print(\"\\nTrying minds14 dataset (lightweight, ~50MB)...\")\n",
    "    dataset = load_dataset(\n",
    "        \"PolyAI/minds14\",\n",
    "        \"en-US\",\n",
    "        split=f\"train[:{TRAIN_SAMPLES + VAL_SAMPLES}]\"\n",
    "    )\n",
    "    \n",
    "    # Split into train/val\n",
    "    split_point = TRAIN_SAMPLES\n",
    "    train_dataset = dataset.select(range(split_point))\n",
    "    val_dataset = dataset.select(range(split_point, TRAIN_SAMPLES + VAL_SAMPLES))\n",
    "    \n",
    "    dataset_name = \"minds14_en\"\n",
    "    print(f\"‚úì minds14 loaded successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"minds14 failed: {e}\")\n",
    "    print(\"\\nTrying LibriSpeech (larger, ~300MB)...\")\n",
    "    \n",
    "    # Fallback to LibriSpeech\n",
    "    dataset = load_dataset(\n",
    "        \"openslr/librispeech_asr\",\n",
    "        \"clean\",\n",
    "        split=f\"test.clean[:{TRAIN_SAMPLES + VAL_SAMPLES}]\"\n",
    "    )\n",
    "    \n",
    "    # Split into train/val\n",
    "    split_point = TRAIN_SAMPLES\n",
    "    train_dataset = dataset.select(range(split_point))\n",
    "    val_dataset = dataset.select(range(split_point, TRAIN_SAMPLES + VAL_SAMPLES))\n",
    "    \n",
    "    dataset_name = \"librispeech_clean\"\n",
    "    print(f\"‚úì LibriSpeech loaded\")\n",
    "\n",
    "print(f\"\\n‚úì Dataset loaded: {dataset_name}\")\n",
    "print(f\"  - Training samples: {len(train_dataset)}\")\n",
    "print(f\"  - Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Inspect dataset structure\n",
    "print(f\"\\nüìä Dataset structure:\")\n",
    "print(f\"  - Columns: {train_dataset.column_names}\")\n",
    "if len(train_dataset) > 0:\n",
    "    print(f\"  - Sample keys: {list(train_dataset[0].keys())}\")\n",
    "    # Show audio info\n",
    "    audio_info = train_dataset[0]['audio']\n",
    "    if isinstance(audio_info, dict):\n",
    "        print(f\"  - Audio sampling rate: {audio_info.get('sampling_rate', 'N/A')} Hz\")\n",
    "\n",
    "# Log dataset version\n",
    "data_version_manager.log_dataset_version(\n",
    "    dataset_name=dataset_name,\n",
    "    version=f\"colab_demo_{TRAIN_SAMPLES}train_{VAL_SAMPLES}val\",\n",
    "    metadata={'train': len(train_dataset), 'val': len(val_dataset)}\n",
    ")\n",
    "\n",
    "# Check disk space after loading\n",
    "if IN_COLAB:\n",
    "    print(\"\\nüìä Disk space after dataset load:\")\n",
    "    check_disk_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc7d1fc",
   "metadata": {},
   "source": [
    "## 4. Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e251a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model manager\n",
    "model_manager = WhisperModelManager(config)\n",
    "\n",
    "# Choose model variant based on Colab resources\n",
    "# Recommendations for A100 (40GB):\n",
    "# - tiny: 39M params, ~500MB, fastest (RECOMMENDED for demo)\n",
    "# - base: 74M params, ~1GB, good balance\n",
    "# - small: 244M params, ~2GB, better accuracy\n",
    "# - medium: 769M params, ~6GB, best accuracy (may be slow)\n",
    "\n",
    "MODEL_VARIANT = 'tiny'  # Change to 'base' or 'small' if you have time/resources\n",
    "\n",
    "print(f\"ü§ñ Loading Whisper model: {MODEL_VARIANT}\")\n",
    "print(f\"   (Optimized for Colab - use tiny/base for best experience)\")\n",
    "\n",
    "# Load model and processor\n",
    "model, processor = model_manager.initialize_model(\n",
    "    variant=MODEL_VARIANT,\n",
    "    device=str(device)\n",
    ")\n",
    "\n",
    "# Get model info\n",
    "model_info = model_manager.get_model_info()\n",
    "print(f\"\\n‚úì Model initialized: {MODEL_VARIANT}\")\n",
    "print(f\"  - Total parameters: {model_info['total_parameters']:,}\")\n",
    "print(f\"  - Trainable parameters: {model_info['trainable_parameters']:,}\")\n",
    "\n",
    "# Check GPU memory after model load\n",
    "if IN_COLAB:\n",
    "    check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a09c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model variants\n",
    "variants_info = compare_models(config)\n",
    "\n",
    "print(\"\\nWhisper Model Variants Comparison:\")\n",
    "print(\"-\" * 60)\n",
    "for variant, info in variants_info.items():\n",
    "    print(f\"{variant:10s} | Params: {info['params']:8s} | Speed: {info['speed']:10s} | Accuracy: {info['accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c80b81f",
   "metadata": {},
   "source": [
    "## 5. Prepare Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04885ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets\n",
    "# Auto-detect column names from the dataset\n",
    "audio_column = \"audio\"  # Standard across most datasets\n",
    "# Text column varies: \"text\", \"sentence\", \"transcription\", etc.\n",
    "text_column = None\n",
    "for col in [\"text\", \"sentence\", \"transcription\", \"transcript\"]:\n",
    "    if col in train_dataset.column_names:\n",
    "        text_column = col\n",
    "        break\n",
    "\n",
    "if text_column is None:\n",
    "    raise ValueError(f\"Could not find text column. Available columns: {train_dataset.column_names}\")\n",
    "\n",
    "print(f\"Using columns: audio='{audio_column}', text='{text_column}'\")\n",
    "\n",
    "train_dataset_wrapper = WhisperDataset(\n",
    "    train_dataset,\n",
    "    processor,\n",
    "    audio_column=audio_column,\n",
    "    text_column=text_column,\n",
    "    max_audio_length_sec=config['data']['audio_max_length_sec'],\n",
    "    augmenter=augmenter  # Apply augmentation only to training\n",
    ")\n",
    "\n",
    "val_dataset_wrapper = WhisperDataset(\n",
    "    val_dataset,\n",
    "    processor,\n",
    "    audio_column=audio_column,\n",
    "    text_column=text_column,\n",
    "    max_audio_length_sec=config['data']['audio_max_length_sec'],\n",
    "    augmenter=None  # No augmentation for validation\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "from src.data import DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "collator = DataCollatorWithPadding(processor)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset_wrapper,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=True,\n",
    "    collate_fn=collator,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset_wrapper,\n",
    "    batch_size=config['training']['batch_size'],\n",
    "    shuffle=False,\n",
    "    collate_fn=collator,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Data loaders created\")\n",
    "print(f\"  - Training batches: {len(train_loader)}\")\n",
    "print(f\"  - Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49fde4e",
   "metadata": {},
   "source": [
    "## 6. Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27108b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for training\n",
    "model = model_manager.prepare_for_training()\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = WhisperTrainer(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    config=config,\n",
    "    device=str(device),\n",
    "    experiment_logger=experiment_logger\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e52984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "# Optimized for Colab: fewer epochs, memory-efficient settings\n",
    "\n",
    "TRAIN_EPOCHS = 1  # Set to 1 for quick demo, 2-3 for Colab session, 5+ for production\n",
    "\n",
    "print(f\"üöÄ Starting training for {TRAIN_EPOCHS} epoch(s)...\")\n",
    "print(f\"   Batch size: {config['training']['batch_size']}\")\n",
    "print(f\"   Gradient accumulation: {config['training']['gradient_accumulation_steps']}\")\n",
    "print(f\"   Effective batch size: {config['training']['batch_size'] * config['training']['gradient_accumulation_steps']}\")\n",
    "print(f\"   Mixed precision (FP16): {config['training']['fp16']}\")\n",
    "\n",
    "# Check resources before training\n",
    "if IN_COLAB:\n",
    "    print(\"\\nüìä Pre-training resources:\")\n",
    "    check_disk_space()\n",
    "    check_gpu_memory()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Training\n",
    "best_val_loss = trainer.train(num_epochs=TRAIN_EPOCHS)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n‚úì Training completed!\")\n",
    "print(f\"  - Best validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Cleanup after training\n",
    "if IN_COLAB:\n",
    "    print(\"\\nüßπ Cleaning up...\")\n",
    "    cleanup_cache()\n",
    "    print(\"\\nüìä Post-training resources:\")\n",
    "    check_disk_space()\n",
    "    check_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729c9491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register trained model\n",
    "from src.utils import get_git_revision\n",
    "\n",
    "model_id = model_registry.register_model(\n",
    "    model_id=f\"{MODEL_VARIANT}_finetuned_{TRAIN_EPOCHS}ep\",\n",
    "    model_path=\"checkpoints/best_model_hf\",\n",
    "    metrics={'val_loss': best_val_loss},\n",
    "    config=config,\n",
    "    git_revision=get_git_revision(),\n",
    "    dataset_version=\"common_voice_11_0_en_subset\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì Model registered: {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a168c3e1",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3607b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    device=str(device)\n",
    ")\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"Evaluating model...\\n\")\n",
    "results = evaluator.evaluate_with_samples(val_loader, num_samples=5)\n",
    "\n",
    "print(f\"\\n‚úì Evaluation Results:\")\n",
    "print(f\"  - WER: {results['metrics']['wer']:.3f}\")\n",
    "print(f\"  - CER: {results['metrics']['cer']:.3f}\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\nüìù Sample Predictions:\")\n",
    "print(\"-\" * 80)\n",
    "for i, sample in enumerate(results['samples'][:3], 1):\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(f\"  Reference:  {sample['reference']}\")\n",
    "    print(f\"  Prediction: {sample['prediction']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b35df1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark latency\n",
    "print(\"Benchmarking inference latency...\\n\")\n",
    "\n",
    "latency_bench = LatencyBenchmark(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    device=str(device)\n",
    ")\n",
    "\n",
    "# Generate test audio clips\n",
    "test_audios = []\n",
    "for i in range(10):  # Test on 10 samples\n",
    "    sample = val_dataset[i]\n",
    "    audio = torch.tensor(sample['audio']['array'])\n",
    "    test_audios.append(audio)\n",
    "\n",
    "latency_results = latency_bench.benchmark_batch(test_audios, sr=16000)\n",
    "\n",
    "print(f\"‚úì Latency Benchmark Results:\")\n",
    "print(f\"  - Mean latency: {latency_results['mean_latency']:.3f}s\")\n",
    "print(f\"  - Std latency: {latency_results['std_latency']:.3f}s\")\n",
    "print(f\"  - Mean RTF: {latency_results['mean_rtf']:.3f}x\")\n",
    "print(f\"\\n  RTF < 1.0 = Real-time capable ‚úì\" if latency_results['mean_rtf'] < 1.0 else \"  RTF >= 1.0 = Not real-time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47284343",
   "metadata": {},
   "source": [
    "## 8. Real-Time Streaming Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fdd7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize streaming ASR\n",
    "streaming_asr = StreamingASR(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    vad=vad,\n",
    "    chunk_length_sec=config['inference']['streaming']['buffer_size_sec'],\n",
    "    overlap_sec=config['inference']['streaming']['overlap_sec'],\n",
    "    device=str(device)\n",
    ")\n",
    "\n",
    "print(\"‚úì Streaming ASR initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a820abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test streaming on audio file\n",
    "# Use a sample from validation set\n",
    "\n",
    "# Get a test audio file\n",
    "test_sample = val_dataset[0]\n",
    "test_audio = test_sample['audio']['array']\n",
    "test_text = test_sample[text_column]  # Use the detected text column\n",
    "\n",
    "# Save to temporary file\n",
    "import torchaudio\n",
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp:\n",
    "    tmp_path = tmp.name\n",
    "    torchaudio.save(tmp_path, torch.tensor(test_audio).unsqueeze(0), 16000)\n",
    "\n",
    "print(f\"Test audio saved to: {tmp_path}\")\n",
    "print(f\"Reference text: {test_text}\\n\")\n",
    "\n",
    "# Stream from file\n",
    "print(\"Streaming transcription:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "transcriptions = []\n",
    "def callback(text):\n",
    "    print(f\"[CHUNK] {text}\")\n",
    "    transcriptions.append(text)\n",
    "\n",
    "streaming_asr.reset()\n",
    "streaming_asr.stream_from_file(tmp_path, chunk_duration_sec=0.5, callback=callback)\n",
    "\n",
    "# Get full transcription\n",
    "full_transcription = streaming_asr.get_full_transcription(merge=True)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(f\"\\nüìù Final Transcription: {full_transcription}\")\n",
    "print(f\"üìñ Reference:          {test_text}\")\n",
    "\n",
    "# Cleanup\n",
    "os.unlink(tmp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5321c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Live microphone streaming (requires microphone access)\n",
    "# Uncomment to use:\n",
    "\n",
    "# print(\"Starting live microphone transcription...\")\n",
    "# print(\"Speak into your microphone. Press Ctrl+C to stop.\\n\")\n",
    "\n",
    "# streaming_asr.reset()\n",
    "\n",
    "# def mic_callback(text):\n",
    "#     print(f\"üé§ {text}\")\n",
    "\n",
    "# streaming_asr.stream_from_microphone(\n",
    "#     duration_sec=30,  # Record for 30 seconds\n",
    "#     callback=mic_callback\n",
    "# )\n",
    "\n",
    "# full_transcription = streaming_asr.get_full_transcription(merge=True)\n",
    "# print(f\"\\nüìù Full Transcription: {full_transcription}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fe6f0b",
   "metadata": {},
   "source": [
    "## 9. Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb8e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch inference for multiple files\n",
    "batch_inference = BatchInference(\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    device=str(device),\n",
    "    batch_size=config['inference']['batch_size']\n",
    ")\n",
    "\n",
    "# Create temporary test files\n",
    "import tempfile\n",
    "import torchaudio\n",
    "\n",
    "test_files = []\n",
    "for i in range(5):\n",
    "    sample = val_dataset[i]\n",
    "    audio = torch.tensor(sample['audio']['array'])\n",
    "    \n",
    "    with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp:\n",
    "        torchaudio.save(tmp.name, audio.unsqueeze(0), 16000)\n",
    "        test_files.append(tmp.name)\n",
    "\n",
    "# Batch transcribe\n",
    "print(\"Batch transcribing 5 files...\\n\")\n",
    "batch_transcriptions = batch_inference.transcribe_batch(test_files)\n",
    "\n",
    "# Display results\n",
    "for i, transcription in enumerate(batch_transcriptions):\n",
    "    reference = val_dataset[i][text_column]  # Use the detected text column\n",
    "    print(f\"File {i+1}:\")\n",
    "    print(f\"  Prediction: {transcription}\")\n",
    "    print(f\"  Reference:  {reference}\")\n",
    "    print()\n",
    "\n",
    "# Cleanup\n",
    "for f in test_files:\n",
    "    os.unlink(f)\n",
    "\n",
    "print(\"‚úì Batch inference completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85b07c4",
   "metadata": {},
   "source": [
    "## 10. Visualization & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837705fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize evaluation results\n",
    "eval_viz = EvaluationVisualizer(save_dir='plots')\n",
    "\n",
    "# Model comparison data (example)\n",
    "comparison_data = {\n",
    "    'whisper-tiny': {\n",
    "        'params': '39M',\n",
    "        'wer_clean': 0.15,\n",
    "        'wer_accented': 0.20,\n",
    "        'latency': 0.1,\n",
    "        'rtf': 0.1\n",
    "    },\n",
    "    'whisper-base': {\n",
    "        'params': '74M',\n",
    "        'wer_clean': results['metrics']['wer'],\n",
    "        'wer_accented': 0.14,\n",
    "        'latency': latency_results['mean_latency'],\n",
    "        'rtf': latency_results['mean_rtf']\n",
    "    },\n",
    "    'whisper-small': {\n",
    "        'params': '244M',\n",
    "        'wer_clean': 0.08,\n",
    "        'wer_accented': 0.12,\n",
    "        'latency': 0.5,\n",
    "        'rtf': 0.5\n",
    "    }\n",
    "}\n",
    "\n",
    "# Plot comparison\n",
    "comparison_path = eval_viz.plot_model_comparison(comparison_data)\n",
    "print(f\"‚úì Model comparison plot saved: {comparison_path}\")\n",
    "\n",
    "# Display\n",
    "from IPython.display import Image\n",
    "display(Image(comparison_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3016aa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comparison table\n",
    "table = generate_comparison_table(comparison_data)\n",
    "print(\"\\nModel Comparison Table:\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456007d8",
   "metadata": {},
   "source": [
    "## 11. Export & Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2008269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model for deployment\n",
    "FINAL_MODEL_PATH = 'final_model'\n",
    "\n",
    "model.save_pretrained(FINAL_MODEL_PATH)\n",
    "processor.save_pretrained(FINAL_MODEL_PATH)\n",
    "\n",
    "print(f\"‚úì Final model saved to: {FINAL_MODEL_PATH}\")\n",
    "print(f\"\\nTo load model later:\")\n",
    "print(f\"  from transformers import WhisperForConditionalGeneration, WhisperProcessor\")\n",
    "print(f\"  model = WhisperForConditionalGeneration.from_pretrained('{FINAL_MODEL_PATH}')\")\n",
    "print(f\"  processor = WhisperProcessor.from_pretrained('{FINAL_MODEL_PATH}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2b1b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Upload to HuggingFace Hub\n",
    "# Requires HuggingFace token and authentication\n",
    "\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "\n",
    "# HF_MODEL_NAME = \"your-username/whisper-base-finetuned-en\"\n",
    "# model.push_to_hub(HF_MODEL_NAME)\n",
    "# processor.push_to_hub(HF_MODEL_NAME)\n",
    "# print(f\"‚úì Model uploaded to HuggingFace: {HF_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a91e659",
   "metadata": {},
   "source": [
    "## 12. Cleanup & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3275866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish experiment tracking\n",
    "experiment_logger.finish()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä PROJECT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nü§ñ Model Configuration:\")\n",
    "print(f\"  - Variant: {MODEL_VARIANT}\")\n",
    "print(f\"  - Training epochs: {TRAIN_EPOCHS}\")\n",
    "print(f\"  - Dataset: {dataset_name}\")\n",
    "print(f\"  - Training samples: {len(train_dataset)}\")\n",
    "print(f\"  - Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "print(f\"\\nüìà Training Results:\")\n",
    "print(f\"  - Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"  - WER: {results['metrics']['wer']:.3f}\")\n",
    "print(f\"  - CER: {results['metrics']['cer']:.3f}\")\n",
    "\n",
    "print(f\"\\n‚ö° Performance Metrics:\")\n",
    "print(f\"  - Mean latency: {latency_results['mean_latency']:.3f}s\")\n",
    "print(f\"  - RTF: {latency_results['mean_rtf']:.3f}x\")\n",
    "print(f\"  - Real-time capable: {'‚úì Yes' if latency_results['mean_rtf'] < 1.0 else '‚úó No'}\")\n",
    "\n",
    "print(f\"\\nüíæ Saved Artifacts:\")\n",
    "if IN_COLAB:\n",
    "    print(f\"  - Model: {FINAL_MODEL_PATH}\")\n",
    "    print(f\"  - Checkpoints: {DRIVE_ROOT}/checkpoints/\")\n",
    "    print(f\"  - Logs: {DRIVE_ROOT}/logs/\")\n",
    "    print(f\"  - Plots: plots/\")\n",
    "else:\n",
    "    print(f\"  - Model: {FINAL_MODEL_PATH}\")\n",
    "    print(f\"  - Checkpoints: checkpoints/\")\n",
    "    print(f\"  - Logs: logs/\")\n",
    "    print(f\"  - Plots: plots/\")\n",
    "\n",
    "# Final resource check\n",
    "if IN_COLAB:\n",
    "    print(\"\\nüìä Final Resource Usage:\")\n",
    "    check_disk_space()\n",
    "    check_gpu_memory()\n",
    "    \n",
    "    print(\"\\nüí° Tips for Colab:\")\n",
    "    print(\"  - Checkpoints saved to Google Drive persist across sessions\")\n",
    "    print(\"  - Increase TRAIN_SAMPLES/VAL_SAMPLES for better accuracy\")\n",
    "    print(\"  - Try 'base' or 'small' model for better quality\")\n",
    "    print(\"  - Use Runtime ‚Üí Factory reset runtime to free all resources\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ Real-Time ASR System Ready for Deployment!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
