# Real-Time Multilingual ASR Configuration

project:
  name: "multilingual-asr"
  version: "1.0.0"
  seed: 42
  device: "cuda"  # cuda or cpu

data:
  sampling_rate: 16000
  chunk_size_sec: 2.0
  chunk_overlap_sec: 0.2
  audio_max_length_sec: 30.0
  vad_threshold: 0.5
  datasets:
    - name: "peoples_speech"
      version: "v1"
      split: "train+validation"
    - name: "common_voice"
      language: "en"
      version: "11.0"
      split: "train+validation"
    - name: "speechocean762"
      version: "v1"
  augmentation:
    enabled: true
    speed_perturbation: [0.9, 1.0, 1.1]
    pitch_shift_semitones: [-2, 0, 2]
    background_noise_prob: 0.3

model:
  variants:
    tiny: "openai/whisper-tiny"
    base: "openai/whisper-base"
    small: "openai/whisper-small"
    medium: "openai/whisper-medium"
    distil: "distil-whisper/distil-large-v3"
  default_variant: "base"
  language: "english"
  task: "transcribe"
  freeze_encoder: false
  freeze_layers: 0

training:
  batch_size: 16
  gradient_accumulation_steps: 2
  num_epochs: 10
  learning_rate: 5.0e-5
  warmup_steps: 500
  weight_decay: 0.01
  max_grad_norm: 1.0
  eval_steps: 500
  save_steps: 1000
  early_stopping_patience: 3
  fp16: true

evaluation:
  metrics:
    - "wer"
    - "cer"
  test_sets:
    - "test_clean"
    - "test_accented"
  latency_benchmark_clips: 100

inference:
  streaming:
    enabled: true
    buffer_size_sec: 2.0
    overlap_sec: 0.2
    vad_enabled: true
  batch_size: 1
  num_beams: 1
  temperature: 0.0
  compression_ratio_threshold: 2.4
  logprob_threshold: -1.0
  no_speech_threshold: 0.6

mlops:
  experiment_tracking:
    backend: "wandb"  # wandb, mlflow, tensorboard
    project_name: "whisper-asr"
  versioning:
    data_version_file: "data_versions.txt"
    model_registry: "model_registry.json"
  logging:
    level: "INFO"
    save_logs: true
